C++ Concurrency In Action Anthony Williams 第二版

C++ Concurrency in Action 2ed 前五章介绍了 C++11 线程库 API 的基本用法，后六章从实践角度介绍了并发编程的设计思想，相比第一版多介绍了一些 C++17 特性，
如 std::scoped_lock、std::shared_mutex，并多出一章（第十章）介绍 C++17 标准库并行算法。

C++11线程库API

    线程管理（Managing thread）
    线程间共享数据（Sharing data between thread）
    同步并发操作（Synchronizing concurrent operation）
    C++ 内存模型和基于原子类型的操作（The C++ memory model and operations on atomic type）

并发编程实践

    基于锁的并发数据结构的设计（Designing lock-based concurrent data structure）
    无锁并发数据结构的设计（Designing lock-free concurrent data structure）
    并发代码的设计（Designing concurrent code）
    高级线程管理（Advanced thread management）
    并行算法（Parallel algorithm）
    多线程应用的测试与调试（Testing and debugging multithreaded application）

标准库相关头文件
头文件 			说明
<chrono> 		时钟
<condition_variable> 	条件变量
<atomic> 		原子类型和原子操作
<future> 		异步处理的结果
<mutex> 		锁
<shared_mutex>
<ratio> 			编译期有理数算数
<thread> 		线程
<execution> 		标准库算法执行策略

特性 			API
thread 			std::thread、std::jthread
mutex和shared_mutex 	std::mutex、std::recursive_mutex、std::timed_mutex、std::recursive_timed_mutex
                                            std::shared_mutex、std::shared_timed_mutex
			std::lock_guard、std::scoped_lock、std::unique_lock、std::shared_lock
condition variable 	std::condition_variable、std::condition_variable_any
atomic 			std::atomic、std::atomic_thread_fence
future 			std::future、std::shared_future
			std::promise、std::packaged_task、std::async
interruption 		无

01 线程管理

线程管理基础

    每个程序至少有一个执行main函数的线程，其他线程也有自己的入口函数，两者会同时运行

#include <thread>
#include <iostream>

void f()
{
  std::cout << "hello world";
}

int main()
{
  std::thread t(f);
  t.join();
}

    执行main函数的线程会在函数结束时完会退出，执行入口函数的线程也是同理
    将函数添加为std::thread的参数即可启动线程

std::thread t(f);

    std::thread的参数也可以是带operator()的对象实例或者lambda

#include <thread>
#include <iostream>

struct A {
  void operator()() const
  {
    std::cout << 1;
  }
};

int main()
{
  A a;
  std::thread t1(a); // 调用A::operator()
  // std::thread t(A()); // most vexing parse，A()被视为函数声明
  // 解决most vexing parse的方法
  std::thread t2{A()};
  std::thread t3((A()));
  std::thread t4{[] { std::cout << 1; }};
  t1.join();
  t2.join();
  t3.join();
  t4.join();
}

    启动线程后在线程销毁前要对其调用join或detach，否则std::thread的析构函数会调用std::terminate终止程序

struct A {
  int& i;
  A(int& x) : i(x) {}
  void operator()() const
  {
    for (int j = 0; j < 1000000; ++j)
    {
      doSomething(i); // 存在隐患：对象析构后i空悬
    }
  }
};

void f()
{
  int x = 0;
  A a(x);
  std::thread t(a);
  t.detach(); // 不等待t结束
} // 函数结束后t可能还在运行，调用doSomething(i)，i是x的引用，而x已经销毁

int main()
{
  std::thread t(f); // 导致空悬引用
  t.join();
}

    使用join即可保证局部变量在线程结束后才被销毁

void f()
{
  int x = 0;
  A a(x);
  std::thread t(a);
  t.join(); // 等待t结束
}

    调用join会在线程结束后清理std::thread，使其与完成的线程不再关联，因此join过的std::thread不能再次被join

void f()
{
  int x = 0;
  A a(x);
  std::thread t(a);
  t.join();
  t.join(); // 错误：t与f已经没有关联了，t.joinable()为false
}

    如果线程运行过程中发生异常，之后调用的join会被忽略，为此需要捕获异常并在处理异常时调用join

void f()
{
  int x = 0;
  A a(x);
  std::thread t(a);
  try
  {
    doSomethingHere();
  }
  catch(...)
  {
    t.join();
    throw;
  }
  t.join();
}

    更简洁的方法是使用RAII类来管理std::thread

class thread_guard {
  std::thread& t;
 public:
  explicit thread_guard(std::thread& x) : t(x) {}
  ~thread_guard() { if (t.joinable()) t.join(); }
  thread_guard(const thread_guard&) = delete;
  thread_guard& operator=(const thread_guard&) = delete;
};

struct A {
  int& i;
  A(int& x) : i(x) {}
  void operator()() const
  {
    for (int j = 0; j < 1000000; ++j)
    {
      doSomething(i); // 存在隐患：对象析构后i空悬
    }
  }
};

void f()
{
  int x = 0;
  A a(x);
  std::thread t(a);
  thread_guard g(t);
  doSomethingHere();
} // 局部对象逆序销毁，优先销毁thread_guard对象，从而调用t.join()
// 即使doSomethingHere抛出异常也不影响这个销毁

    使用detach分离线程会让线程在后台运行，线程分离后与主线程无法直接交互，也不能被join

std::thread t(f);
t.detach();
assert(!t.joinable()); // 因此joinable为true才能join或detach

    分离线程称为守护线程，即没有任何显式接口并运行在后台的线程，其特点是长时间运行。比如有一个文档处理应用，为了同时编辑多个文档，每次打开一个新文档则可以开一个分离线程

void edit_document(const std::string& filename)
{
  open_document_and_display_gui(filename);
  while (!done_editing())
  {
    user_command cmd=get_user_input();
    if (cmd.type == open_new_document)
    {
      const std::string new_name = get_filename_from_user();
      std::thread t(edit_document, new_name);
      t.detach();
    }
    else
    {
      process_user_input(cmd);
    }
  }
}

为线程函数传递参数

    有参数的函数也能传给std::thread，参数的默认实参会被忽略

void f(int i = 1) // 传递给std::thread时默认实参会被忽略
{
  std::cout << i;
}

int main()
{
  std::thread t(f, 42); // 第一个参数为函数名，其余参数为函数的参数
  t.join();
}

    std::thread会无视参数的引用类型，因此需要使用std::ref来生成一个引用包裹对象以传入引用类型

void f(int& n) { ++n; }

int main()
{
  int i = 1;
  std::thread t(f, std::ref(i));
  t.join();
  std::cout << i; // 2
}

    也可以传递类成员函数

class A {
 public:
  void f(int i) { std::cout << i; }
};

int main()
{
  A a;
  std::thread t(&A::f, &a, 42); // 第一个参数为成员函数指针，第二个参数为对象实例指针
  t.join();
}

    如果参数是move-only对象则需要使用std::move

void f(std::unique_ptr<int> p)
{
  std::cout << *p;
}

int main()
{
  std::unique_ptr<int> p(new int(42));
  std::thread t(f, std::move(p));
  t.join();
}

转移线程所有权

void f();
void g();

std::thread t1(f);
std::thread t2 = std::move(t1); // t1所有权给t2，t2关联执行f的线程
t1 = std::thread(g); // t1重新关联一个执行g的线程
std::thread t3;
t3 = std::move(t2); // t3关联t2的线程，t2无关联
t1 = std::move(t3); // t1已有关联g的线程，调用std::terminate终止程序

    线程所有权可以转移到函数外

void f(int i) { std::cout << i; }

std::thread g()
{
  return std::thread(f, 42);
}

int main()
{
  std::thread t{g()};
  t.join();
}

    同理std::thread也能作为参数

void f(std::thread t);

void g()
{
  f(std::thread(someFunction));
  std::thread t(someFunction);
  f(std::move(t));
}

    现在写一个可以直接用std::thread构造的RAII类

class scoped_thread {
  std::thread t;
 public:
  explicit scoped_thread(std::thread x) : t(std::move(x))
  {
    if (!t.joinable())
    {
      throw std::logic_error("no thread");
    }
  }
  ~scoped_thread() { t.join(); }
  scoped_thread(const scoped_thread&) = delete;
  scoped_thread& operator=(const scoped_thread&)=delete;
};

struct A {
  int& i;
  A(int& x) : i(x) {}
  void operator()() const
  {
    for (int j = 0; j < 1000000; ++j)
    {
      doSomething(i); // 存在隐患：对象析构后i空悬
    }
  }
};

void f()
{
  int x = 0;
  scoped_thread g(std::thread{A(x)}); // 直接将线程传到类中
  doSomethingHere();
} // scoped_thread对象销毁将自动调用join

    下面实现一个为std::thread添加了析构行为的joining_thread

class A {
  std::thread t;
 public:
  A() noexcept = default;
  
  template<typename T, typename... Ts>
  explicit A(T&& f, Ts&&... args) :
  t(std::forward<T>(f), std::forward<Ts>(args)...)
  {}
  
  explicit A(std::thread x) noexcept : t(std::move(x)) {}
  A(A&& rhs) noexcept : t(std::move(rhs.t)) {}
  
  A& operator=(A&& rhs) noexcept
  {
    if (joinable()) join();
    t = std::move(rhs.t);
    return *this;
  }
  
  A& operator=(std::thread rhs) noexcept
  {
    if (joinable()) join();
    t = std::move(rhs);
    return *this;
  }
  
  ~A() noexcept
  {
    if (joinable()) join();
  }
  
  void swap(A&& rhs) noexcept { t.swap(rhs.t); }
  std::thread::id get_id() const noexcept { return t.get_id(); }
  bool joinable() const noexcept { return t.joinable(); }
  void join() { t.join(); }
  void detach() { t.detach(); }
  std::thread& as_thread() noexcept { return t; }
  const std::thread& as_thread() const noexcept { return t; }
};

    移动操作同样适用于支持移动的容器

void f()
{
  std::vector<std::thread> v;
  for (int i = 0; i < 10; ++i)
  {
    v.emplace_back(someFunction);
  }
  std::for_each(std::begin(v), std::end(v), std::mem_fn(&std::thread::join));
}

运行期选择线程数量

    hardware_concurrency会返回支持的并发线程数

std::cout << std::thread::hardware_concurrency();

    并行版本的std::accumulate

template<typename Iterator, typename T>
struct accumulate_block {
  void operator()(Iterator first, Iterator last, T& res)
  {
    res = std::accumulate(first, last, res);
  }
};

template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init)
{
  const unsigned long len = std::distance(first, last);
  if (!len) return init;
  const unsigned long min_per_thread = 25;
  const unsigned long max_threads = (len + min_per_thread - 1) / min_per_thread;
  const unsigned long hardware_threads = std::thread::hardware_concurrency();
  const unsigned long num_threads = // 线程数量
    std::min(hardware_threads != 0 ? hardware_threads : 2, max_threads);
  const unsigned long block_size = len / num_threads; // 每个线程中的数据量
  std::vector<T> res(num_threads);
  std::vector<std::thread> threads(num_threads - 1); // 已有一个主线程，所以少一个线程
  Iterator block_start = first;
  for (unsigned long i = 0; i < num_threads - 1; ++i)
  {
    Iterator block_end = block_start;
    std::advance(block_end, block_size); // block_end指向当前块的尾部
    threads[i] = std::thread(accumulate_block<Iterator, T>{}, block_start, block_end, std::ref(res[i]));
    block_start = block_end;
  }
  accumulate_block<Iterator, T>()(block_start, last, res[num_threads - 1]);
  std::for_each(threads.begin(), threads.end(), std::mem_fn(&std::thread::join));
  return std::accumulate(res.begin(), res.end(), init);
}

线程标识

    可以通过对线程实例调用成员函数get_id或在当前线程中调用std::this_thread::get_id获取线程id
    线程id允许拷贝和比较，因此可以将其作为容器的键值。如果两个线程id相等，则两者是同一线程或都无线程

std::thread::id masterThread; // 主线程

void f()
{
  if (std::this_thread::get_id() == masterThread)
  { // 主线程要做一些额外工作，即可通过比较线程id来确认主线程
    doMasterThreadWork();
  }
  doCommonWork();
}

02 线程间共享数据

线程间共享数据存在的问题

    不变量（invariant）：关于一个特定数据结构总为true的语句，比如双向链表的两个相邻节点A和B，A的后指针一定指向B，B的前指针一定指向A。
    有时程序为了方便会暂时破坏不变量，这通常发生于更新复杂数据结构的过程中，比如删除双向链表中的一个节点N，
    要先让N的前一个节点指向N的后一个节点（不变量被破坏），再让N的后节点指向前节点，最后删除N（此时不变量重新恢复）
    线程修改共享数据时，就会发生破坏不变量的情况，此时如果有其他线程访问，就可能导致不变量被永久性破坏，这就是race condition
    如果线程执行顺序的先后对结果无影响，则为不需要关心的良性竞争。需要关心的是不变量被破坏时产生的race condition
    C++标准中定义了data race的概念，指代一种特定的race condition，即并发修改单个对象。data race会造成未定义行为
    race condition要求一个线程进行时，另一线程访问同一数据块，出现问题时很难复现，因此编程时需要使用大量复杂操作来避免race condition

用mutex保护共享数据

    使用mutex在访问共享数据前加锁，访问结束后解锁。一个线程用特定的mutex锁定后，其他线程必须等待该线程的mutex解锁才能访问共享数据
    C++提供了std::mutex来创建一个mutex，可通过std::mutex::lock加锁，通过std::unlock解锁，但一般不直接使用这两个函数
    std::lock_guard是一个用std::mutex构造的RAII模板类

lock_guard

Operation			Effect
lock_guard lg(m)			Creates a lock guard for the mutex m and locks it 
lock_guard lg(m,adopt_lock)	Creates a lock guard for the already locked mutex m
lg.~lock_guard()			Unlocks the mutex and destroys the lock guard

#include <list>
#include <mutex>
#include <algorithm>

std::list<int> v;
std::mutex m;

void f(int n)
{
  std::lock_guard<std::mutex> l(m); // C++17中引入了类模板实参推断，可简写为std::lock_guard l(m);
  v.emplace_back(n);
}

bool listContains(int n)
{
  std::lock_guard<std::mutex> l(m);
  return std::find(std::begin(v), std::end(v), n) != std::end(v);
}

    C++17提供了加强版的std::scoped_lock，它可以接受任意数量的std::mutex，可完全取代std::lock_guard

std::scoped_lock g(m1, m2);

    一般mutex和要保护的数据一起放在类中，定义为private数据成员，而非全局变量，这样能让代码更清晰。
    但如果某个成员函数返回指向数据成员的指针或引用，则通过这个指针的访问行为不会被mutex限制，因此需要谨慎设置接口，确保mutex能锁住数据

class A {
 private:
  int i;
 public:
  void doSomething();
};

class B {
 private:
  A data;
  std::mutex m;
 public:
  template<typename F>
  void processData(F f)
  {
    std::scoped_lock l(m);
    f(data);
  }
};

A* p;
void oops(A& a)
{
  p = &a;
}

B b;
void foo()
{
  b.processData(oops); // processData加了mutex，但传入该函数会获取指向数据成员的指针
  p->doSomething(); // 未锁定mutex的情况下访问数据
}

    即便在很简单的接口中，也可能遇到race condition

std::stack<int> s；
if (!s.empty())
{
  int n = s.top();
  s.pop();
}

    上述代码先检查非空再获取栈顶元素，在单线程中是安全的，但在多线程中，检查非空之后，如果其他线程先pop，就会导致当前线程top出错。
    这是一个经典的race condition，即使用mutex也不能阻止，这就是接口固有的问题，解决方法是改变接口的设计
    另一个潜在的竞争是，如果两个线程都还没pop，而是分别获取了top，虽然不会产生未定义行为，但这种对同一值处理了两次的行为更为严重，因为看起来没有任何错误，很难定位bug
    思考一个问题，既然如此，为什么不直接让pop返回栈顶元素。原因在于，假设有一个stack<vector<int>>，拷贝vector时需要在堆上分配内存，
    如果系统负载严重或资源有限（比如vector有大量元素），vector的拷贝构造函数就会抛出std::bad_alloc异常。
    如果pop可以返回栈顶元素值，返回一定是最后执行的语句，stack在返回前已经弹出了元素，但如果拷贝返回值时抛出异常，就会导致弹出的数据丢失（从栈上移除但拷贝失败）。
    因此std::stack的设计者将这个操作分解为top和pop两部分，但这样的分割却造成了race condition
    下面思考几种把top和pop合为一步的方法。第一种方法是传入一个参数获取结果值

std::vector<int> res;
s.pop(res);

    这种方式的明显缺点是，需要构造一个栈元素类型的实例，这是不现实的：为了获取结果而临时构造一个对象并不划算、
    元素类型可能不支持赋值（比如用户自定义某个类型）、构造函数还需要一些参数......
    第二种方案是为元素类型设置不抛异常的拷贝或移动构造函数，使用std::is_nothrow_copy_constructible和std::is_nothrow_move_constructible即可保证不抛异常，
    因为pop返回值时只担心该过程抛异常。但这种方式过于局限，抛异常的构造函数还是更常见的，这些类型也希望能存入stack
    第三种方案是返回指向弹出元素的指针，指针可以自由拷贝且不会抛异常。这需要管理对象的内存分配，使用std::shared_ptr是个不错的选择，但这个方案的开销太大，尤其是对于内置类型
    第四种方案是结合方案一二或者一三，比如结合方案一三实现一个线程安全的stack

#include <exception>
#include <memory>
#include <mutex>
#include <stack>

struct emptyStack : std::exception
{
  const char* what() const noexcept
  {
    return "empty stack!";
  }
};

template<typename T>
class A {
 private:
  std::stack<T> s;
  mutable std::mutex m;
 public:
  A() : s(std::stack<T>()) {}
  
  A(const A& rhs)
  {
    std::lock_guard<std::mutex> l(rhs.m);
    s = rhs.s;
  }
  
  A& operator=(const A&) = delete;
  
  void push(T n)
  {
    std::lock_guard<std::mutex> l(m);
    s.push(std::move(n));
  }
  
  std::shared_ptr<T> pop() // 返回一个指向栈顶元素的指针
  {
    std::lock_guard<std::mutex> l(m);
    if (s.empty()) throw emptyStack();
    const std::shared_ptr<T> res(std::make_shared<T>(std::move(s.top())));
    s.pop();
    return res;
  }
  
  void pop(T& n) // 传引用获取结果
  {
    std::lock_guard<std::mutex> l(m);
    if (s.empty()) throw emptyStack();
    n = std::move(s.top());
    s.pop();
  }
  
  bool empty() const
  {
    std::lock_guard<std::mutex> l(m);
    return s.empty();
  }
};

    之前锁的粒度（锁保护的数据量大小）太小，保护操作覆盖不周全，这里的粒度就较大，覆盖了大量操作。
    但并非粒度越大越好，如果锁粒度太大，过多线程请求竞争占用资源时，并发的性能提升就被抵消掉了
    如果给定操作需要多个mutex时，就会引入一个新的潜在问题，即死锁

死锁

    死锁的四个必要条件：互斥、占有且等待、不可抢占、循环等待
    避免死锁通常建议让两个mutex以相同顺序上锁，总是先锁A再锁B，但这并不适用所有情况
    std::lock解决了此问题，它可以一次性锁住多个mutex，并且没有死锁风险

class A {
 public:
  explicit A(int x) : i(x) {}
  int i;
  std::mutex m;
};

void f(A& from, A& to, int n)
{
  std::lock(from.m, to.m);
  // 下面按固定顺序加锁，看似不会有死锁的问题
  // 但如果没有std::lock同时上锁
  // 另一线程中执行f(to, from, n)
  // 两个锁的顺序就反了过来，从而可能导致死锁
  std::lock_guard<std::mutex> lock1(from.m, std::adopt_lock); // std::adopt_lock表示获取m的所有权
  std::lock_guard<std::mutex> lock2(to.m, std::adopt_lock);
  from.i -= n;
  to.i += n;
}

int main()
{
  A x(70);
  A y(30);
  
  std::thread t1(f, std::ref(x), std::ref(y), 5);
  std::thread t2(f, std::ref(y), std::ref(x), 10);
  
  t1.join();
  t2.join();
}

    std::lock可能抛异常，此时就不会上锁，因此要么都锁住，要么都不锁
    也可以使用std::unique_lock，它比std::lock_guard灵活，可以传入std::adopt_lock管理mutex，也可以传入std::defer_lock表示mutex应保持解锁状态，
    以使mutex能被std::unique_lock::lock获取，或可以把std::unique_lock传给std::lock

std::unique_lock<std::mutex> lock1(from.m, std::defer_lock);
std::unique_lock<std::mutex> lock2(to.m, std::defer_lock);
// std::defer_lock表示不获取m的所有权，因此m还未上锁
std::lock(lock1, lock2); // 此处上锁

    std::unique_lock比std::lock_guard占用的空间多，会稍慢一点，如果不需要更灵活的锁，依然可以使用std::lock_guard。
    上面展示了延迟锁的情况，另一种要求灵活性的情况是转移锁的所有权到另一个作用域

unique_lock

Operation			Effect
unique_lock l			Default constructor; creates a lock not associated with a mutex
unique_lock l(m)			Creates a lock guard for the mutex m and locks it
unique_lock l(m,adopt_lock)	Creates a lock guard for the already locked mutex m
unique_lock l(m,defer_lock)	Creates a lock guard for the mutex m without locking it
unique_lock l(m,try_lock)		Creates a lock guard for the mutex m and tries to lock it
unique_lock l(m,dur)		Creates a lock guard for the mutex m and tries to lock it for duration dur
unique_lock l(m,tp)		Creates a lock guard for the mutex m and tries to lock it until timepoint tp
unique_lock l(rv)			Move constructor; moves lock state from rv to l (rv has no associated mutex anymore)
l.~unique_lock()			Unlocks the mutex, if any locked, and destroys the lock guard
unique_lock l = rv		Move assignment; moves the lock state from rv to l (rv has no associated mutex anymore)
swap(l1,l2)			Swaps locks
l1.swap(l2)			Swaps locks
l.release()			Returns a pointer to the associated mutex and releases it
l.owns_lock()			Returns true if an associated mutex is locked
if (l)				Checks whether an associated mutex is locked
l.mutex()			Returns a pointer to the associated mutex
l.lock()				Locks the associated mutex
l.try_lock()			Tries to lock the associated mutex (returns true if lock successful)
l.try_lock_for(dur)		Tries to lock the associated mutex for duration dur (returns true if lock successful)
l.try_lock_until(tp)		Tries to lock the associated mutex until timepoint tp (returns true if lock successful)
l.unlock()			Unlocks the associated mutex

std::unique_lock<std::mutex> getLock()
{
  extern std::mutex m;
  std::unique_lock<std::mutex> l(m);
  prepareData();
  return l; // 不需要std::move（编译器负责调用移动构造函数）
}

void f()
{
  std::unique_lock<std::mutex> l(getLock());
  doSomething();
}

    对一些费时的操作（如文件IO）上锁可能造成很多操作被阻塞，可以在面对这些操作时先解锁

void f()
{
  std::unique_lock<std::mutex> l(m);
  auto data = getData();
  l.unlock(); // 费时操作没有必要持有锁，先解锁
  auto res = process(data);
  l.lock(); // 为了写入数据再次上锁
  writeResult(data, res);
}

    如果支持C++17，最易最优的同时上锁方法是使用std::scoped_lock

std::scoped_lock l(from.m, to.m);

    解决死锁并不简单，std::lock和std::scoped_lock无法获取其中的锁，此时解决死锁更依赖于开发者的能力
    第一个避免死锁的建议是，一个线程已经获取一个锁时就不要获取第二个。如果每个线程只有一个锁，锁上就不会产生死锁
  （但除了互斥锁，其他方面也可能造成死锁，比如即使无锁，线程间相互等待也可能造成死锁）
    第二个建议是，持有锁时避免调用用户提供的代码。用户提供的代码可能做任何时，包括获取锁，如果持有锁时调用用户代码获取锁，就会违反第一个建议，并造成死锁。
    但有时调用用户代码是无法避免的
    第三个建议是，按固定顺序获取锁。如果必须获取多个锁且不能用std::lock同时获取，最好在每个线程上用固定顺序获取。
    上面的例子虽然是按固定顺序获取锁，但如果不同时加锁就会出现死锁，对于这种情况的建议是额外规定固定的调用顺序
    第四个建议是使用层次锁，如果一个锁被低层持有，就不允许再上锁

// 设定值来表示层级
hierarchical_mutex high(10000);
hierarchical_mutex mid(6000);
hierarchical_mutex low(5000);

void lf() // 最低层函数
{
  std::scoped_lock l(low);
}

void hf()
{
  std::scoped_lock l(high);
  lf(); // 可以调用低层函数
}

void mf()
{
  std::scoped_lock l(mid);
  hf(); // 中层调用了高层函数，违反了层次结构
}

    下面实现hierarchical_mutex

class hierarchical_mutex {
  std::mutex internal_mutex;
  const unsigned long hierarchy_value; // 当前层级值
  unsigned long previous_hierarchy_value; // 前一线程的层级值
  // 所在线程的层级值，thread_local表示值存在于线程存储期
  static thread_local unsigned long this_thread_hierarchy_value;
  void check_for_hierarchy_violation() // 检查是否违反层级结构
  {
    if (this_thread_hierarchy_value <= hierarchy_value)
    {
      throw std::logic_error("mutex hierarchy violated");
    }
  }
  void update_hierarchy_value()
  {
    // 先存储当前线程的层级值（用于解锁时恢复）
    previous_hierarchy_value = this_thread_hierarchy_value;
    // 再把其设为锁的层级值
    this_thread_hierarchy_value = hierarchy_value;
  }
 public:
  explicit hierarchical_mutex(unsigned long value) :
    hierarchy_value(value), previous_hierarchy_value(0)
  {}
  void lock()
  {
    check_for_hierarchy_violation(); // 要求线程层级值大于锁的层级值
    internal_mutex.lock(); // 内部锁被锁住
    update_hierarchy_value(); // 更新层级值
  }
  void unlock()
  {
    if (this_thread_hierarchy_value != hierarchy_value)
    {
      throw std::logic_error("mutex hierarchy violated");
    }
    // 恢复前一线程的层级值
    this_thread_hierarchy_value = previous_hierarchy_value;
    internal_mutex.unlock();
  }
  bool try_lock()
  {
    check_for_hierarchy_violation();
    if (!internal_mutex.try_lock()) return false;
    update_hierarchy_value();
    return true;
  }
};

thread_local unsigned long // 初始化为ULONG_MAX以使构造锁时能通过检查
hierarchical_mutex::this_thread_hierarchy_value(ULONG_MAX);

    看似复杂其实逻辑很简单，简化一下很容易理解

class A {
  std::mutex m; // 内部锁
  int val; // 当前层级值
  int pre; // 用于保存前一线程层级值
  static thread_local int tVal; // tVal存活于一个线程周期
 public:
  explicit A(int x) : val(x), pre(0) {}
  void lock()
  {
    if (tVal > val)
    { // 存储线程层级值tVal到pre后将其更新为锁的层级值val
      m.lock();
      pre = tVal;
      tVal = val;
    }
    else
    {
      throw std::logic_error("mutex hierarchy violated");
    }
  }
  void unlock()
  { // 恢复线程层级值为pre
    if (tVal != val)
    {
      throw std::logic_error("mutex hierarchy violated");
    }
    tVal = pre;
    m.unlock();
  }
  bool try_lock()
  {
    if (tVal > val)
    {
      if (!m.try_lock()) return false;
      pre = tVal;
      tVal = val;
      return true;
    }
    else
    {
      throw std::logic_error("mutex hierarchy violated");
    }
  }
};

thread_local int A::tVal(INT_MAX); // 保证初始构造std::scoped_lock正常

    解释之前的例子

hierarchical_mutex high(10000);
hierarchical_mutex mid(6000);
hierarchical_mutex low(5000); // 构造一个层级锁
// 初始化时锁的层级值hierarchy_value为5000
// previous_hierarchy_value为0

void lf()
{
  std::scoped_lock l(low);
  // 用层级锁构造std::scoped_lock时会调用low.lock
  // lock先检查，this_thread_hierarchy_value初始化为ULONG_MAX
  // 因此this_thread_hierarchy_value大于hierarchy_value
  // 通过检查，内部锁上锁
  // 更新值，把previous_hierarchy_value更新为线程层级值ULONG_MAX
  // 把this_thread_hierarchy_value更新为low的层级值5000
} // 调用low.unlock，检查this_thread_hierarchy_value，值为5000
// 与hierarchy_value相等，通过检查
// 接着把this_thread_hierarchy_value恢复为pre保存的ULONG_MAX
// 最后解锁

void hf()
{
  std::scoped_lock l(high);
  // this_thread_hierarchy_value更新为high的层级值10000
  lf(); // 调用lf时，lf里的this_thread_hierarchy_value值为10000
  // 过程只是把lf中的注释里this_thread_hierarchy_value初始值改为10000
  // 同样能通过检查，其余过程一致，最后解锁lf会恢复到这里的线程层级值10000
}

void mf()
{
  std::scoped_lock l(mid);
  // this_thread_hierarchy_value更新为mid的层级值6000
  hf(); // 调用hf时，hf里的this_thread_hierarchy_value值为6000
  // 构造hf里的l时，调用high.lock
  // 检查this_thread_hierarchy_value，小于high.hierarchy_value
  // 于是throw std::logic_error("mutex hierarchy violated")
}

其他保护共享数据的可选方式
保护共享数据的初始化

    一个极端但常见的情况是，共享数据在并发访问和初始化都需要保护，但之后要隐式同步。数据初始化后上锁只是为了保护初始化过程，但这会不必要地影响性能
    延迟初始化在单线程中很常见

std::shared_ptr<A> P;
void f()
{
  if (!p)
  {
    p.reset(new A); // 在多线程中这里需要保护
  }
  p->doSomething();
}

    但在多线程直接上锁会导致不必要的线程资源阻塞

std::shared_ptr<A> P;
std::mutex m;

void f()
{
  std::unique_lock<std::mutex> l(m); // 所有线程会在此处阻塞
  if (!p)
  {
    p.reset(new A);
  }
  l.unlock();
  p->doSomething();
}

    很多人能想到一个更好的方法是双重检查锁模式

void f()
{
  if (!p) // 这里没被锁保护，会与其他线程中被锁保护的reset竞争
  {
    std::scoped_lock l(m);
    if (!p)
    {
      p.reset(new A);
    }
  }
  p->doSomething();
}

    但这个方案也存在潜在的race condition，第一次的检查没上锁，可能与其他线程中被保护的reset操作产生竞争。
    如果当前线程看见其他线程写入了指针，但没看到新创建的对象实例，调用doSomething就会出错

p.reset(new A);
// 1. 为A对象分配一片内存
// 2. 在分配的内存上调用A的构造函数，构造一个A对象
// 3. 返回该内存的指针，让p指向该内存
// 编译器不一定按23顺序执行，可能32

    为了处理race condition，C++标准库提供了std::once_flag和std::call_once

template< class Callable, class... Args >
void call_once( std::once_flag& flag, Callable&& f, Args&&... args );
Executes the Callable object f exactly once, even if called concurrently, from several threads. 

#include <iostream>
#include <thread>
#include <mutex>

std::once_flag flag;

void f()
{
  std::call_once(flag, [] { std::cout << 1; });
  std::cout << 2;
}

int main()
{
  std::thread t1(f);
  std::thread t2(f);
  std::thread t3(f);
  t1.join();
  t2.join();
  t3.join();
}

// output
1222

    每个线程只要使用std::call_once，在std::call_once结束时就能安全地知道指针已被其他线程初始化，而且这比使用mutex的开销更小

std::shared_ptr<A> p;
std::once_flag flag;

void init()
{
  p.reset(new A);
}

void f()
{
  std::call_once(flag, init);
  p->doSomething();
}

    std::call_once也可以用在类中

class A {
 private:
  std::once_flag flag;
  void init() { ... }
 public:
  void f()
  {
    std::call_once(flag, &A::init, this);
    ...
  }
};

    static变量的初始化存在潜在的race condition：变量声明为static时，声明后就完成了初始化，一个线程完成了初始化，其他线程仍会抢着定义这个变量。
    为此，C++11规定static变量的初始化只完全发生在一个线程中，直到初始化完成前其他线程都不会做处理，从而避免了race condition。
    只有一个全局实例时可以不使用std::call_once而直接用static

class A {
 public:
  static A& getInstance();
  A(const A&) = delete;
  A& operator(const A&) = delete;
 private:
  A() = default;
  ~A() = default;
};

A& A::getInstance()
{
  static A instance; // 线程安全的初始化
  return instance;
}

保护不常更新的数据结构

    有些数据（比如缓存中存放的DNS入口表）需要经常访问但更新频率很低，如果用std::mutex保护数据有些过度（大量读的操作也会因锁而影响性能），
    这就需要用上读写锁（reader-writer mutex），它允许多个线程并发读但仅一个线程写
    C++17提供了std::shared_mutex和std::shared_timed_mutex（C++14），后者比前者提供了更多操作，但前者性能更高。C++11没有提供读写锁，为此可使用boost::shared_mutex
    读写锁并不是万能的，其性能与处理器数量及读写线程的负载有关
    C++14提供了std::shared_lock，用法和std::unique_lock相同，此外std::shared_lock还允许多线程同时获取共享锁，因此一般用std::shared_lock锁定读，std::unique_lock锁定写

class A {
 private:
  mutable std::shared_mutex m;
  int n = 0;
 public:
  int read()
  {
    std::shared_lock<std::shared_mutex> l(m);
    return n;
  }
  void write()
  {
    std::unique_lock<std::shared_mutex> l(m);
    ++n;
  }
};

递归锁

    一个线程已经获取std::mutex（即已上锁）后再次上锁就会产生未定义行为

std::mutex m;

void f()
{
  m.lock();
  m.unlock();
}

void g()
{
  m.lock();
  f();
  m.unlock();
}

int main()
{
  std::thread t(g);
  t.join(); // 产生未定义行为
}

    为了允许这种情况，C++提供了std::recursive_mutex，它可以在一个线程上多次获取锁，但在其他线程获取锁之前必须释放所有的锁
    多数情况下，如果需要递归锁，说明代码设计存在问题。比如一个类的每个成员函数都会上锁，一个成员函数调用另一个成员函数，就可能多次上锁，
    这种情况用递归锁就可以避免产生未定义行为。但显然这个设计本身是有问题的，更好的办法是提取其中一个函数作为private成员并且不上锁，其他成员先上锁再调用该函数

03 同步并发操作

等待一个事件或其他条件

    一个线程要等待另一个线程完成任务，确定完成任务的方法有几种。第一种是持续检查mutex，这种方法显然很浪费资源。第二种是每隔一段时间进行一次检查

bool flag;
std::mutex m;

void f()
{
  std::unique_lock<std::mutex> l(m);
  while (!flag)
  {
    l.unlock();
    std::this_thread::sleep_for(std::chrono::milliseconds(100)); // 休眠100ms
    // 休眠期间其他线程就有机会获取mutex并设置flag
    l.lock();
  }
}

    但很难确定适当的休眠时间，过长（会直接影响程序行为，很少见）过短（相当于没有，一样浪费资源）都不好
    第三种方案是使用条件变量（condition variable），标准库对条件变量提供了两种实现：std::condition_variable和std::condition_variable_any，前者仅限和std::mutex工作，
    而后者可以与任何满足最低标准的mutex工作（因此加上_any的后缀），更通用也意味着更大的开销，因此一般首选使用前者

#include <iostream>
#include <string>
#include <thread>
#include <mutex>
#include <condition_variable>

std::mutex m;
std::condition_variable cv;
std::string data;
bool ready = false;
bool processed = false;

void f()
{
  std::unique_lock<std::mutex> l(m); // 传给wait的只能是std::unique_lock
  cv.wait(l, [] { return ready; }); // 第二个参数为false时解锁mutex阻塞线程
  // 当收到其他线程notify_one时wait会被唤醒，重新检查条件
  data += " after processing";
  processed = true;
  l.unlock();
  cv.notify_one();
}

int main()
{
  std::thread t(f);
  data = "data";
  {
    std::lock_guard<std::mutex> l(m);
    data += " ready";
    ready = true;
    cv.notify_one(); // 唤醒cv.wait，重新检查ready == true
  }
  {
    std::unique_lock<std::mutex> l(m);
    cv.wait(l, [] { return processed; });
  }
  std::cout << data; // data ready after processing
  t.join();
}

用条件变量实现线程安全的queue

    std::queue的接口如下

template<class T, class Container = std::deque<T>>
class queue {
 public:
  explicit queue(const Container&);
  explicit queue(Container&&);
  template<class Alloc> explicit queue(const Alloc&);
  template<class Alloc> explicit queue(const  Container&, const Alloc&);
  template<class Alloc> explicit queue(Container&&, const Alloc&);
  template<class Alloc> explicit queue(const queue&, const Alloc&);
  template<class Alloc> explicit queue(queue&&, const Alloc&);
  
  T& front();
  const T& front() const;
  T& back();
  const T& back() const;
  
  bool empty() const;
  size_type size() const;
  
  void swap(queue&);
  void push(const T&);
  void push(T&&);
  void pop();
  template <class... Args> void emplace(Args&&... args);
};

    和std::stack一样，std::queue的接口设计存在固有竞争，因此需要将front和pop合并成一个函数（就像合并std::stack的top和pop）。
    这里提供了pop的两个变种，try_pop总会直接返回（即使没有可弹出的值），wait_and_pop等待有值可检索才返回。用之前实现stack的方式实现queue，接口就会像下面这样

template<typename T>
class A {
 public:
  A();
  A(const A&);
  A& operator=(const A&) = delete;
  
  void push(T);
  bool try_pop(T&); // 没有可检索的值则返回false
  std::shared_ptr<T> try_pop(); // 直接返回检索值，没有则返回空指针
  
  void wait_and_pop(T&);
  std::shared_ptr<T> wait_and_pop();
  bool empty() const;
};

    使用条件变量完整实现线程安全的queue

#include <memory>
#include <mutex>
#include <condition_variable>
#include <queue>

template<typename T>
class A {
  mutable std::mutex m; // 必须可变
  std::queue<T> q;
  std::condition_variable cv;
 public:
  A() {}
  A(const A& rhs)
  {
    std::lock_guard<std::mutex> l(rhs.m);
    q = rhs.q;
  }
  
  void push(T x)
  {
    std::lock_guard<std::mutex> l(m);
    q.push(std::move(x));
    cv.notify_one();
  }
  
  void wait_and_pop(T& x)
  {
    std::unique_lock<std::mutex> l(m);
    cv.wait(l, [this] { return !q.empty(); });
    x = std::move(q.front());
    q.pop();
  }
  
  std::shared_ptr<T> wait_and_pop()
  {
    std::unique_lock<std::mutex> l(m);
    cv.wait(l, [this] { return !q.empty(); });
    std::shared_ptr<T> res(std::make_shared<T>(std::move(q.front())));
    q.pop();
    return res;
  }
  
  bool try_pop(T& x)
  {
    std::lock_guard<std::mutex> l(m);
    if (q.empty()) return false;
    x = std::move(q.front());
    q.pop();
    return true;
  }
  
  std::shared_ptr<T> try_pop()
  {
    std::lock_guard<std::mutex> l(m);
    if (q.empty()) return std::shared_ptr<T>();
    std::shared_ptr<T> res(std::make_shared<T>(std::move(q.front())));
    q.pop();
    return res;
  }
  
  bool empty() const
  {
    std::lock_guard<std::mutex> l(m);
    // 其他线程可能有此对象（拷贝构造）所以要上锁
    return q.empty();
  }
};

使用期值等待一次性事件

    标准库提供了只能关联一个事件的唯一期值std::future和能关联多个事件的共享期值std::shared_future，并发TS中扩展了这两个类，
    分别为std::experimental::future和std::experimental::shared_future
    最简单的一次性事件就是运行在后台的计算结果，而std::thread不能获取返回值

future

future f			Default constructor; creates a future with an invalid state
future f(rv)		Move constructor; creates a new future, which gets the state of rv, and invalidates the state of rv
f.~future()		Destroys the state and destroys *this
f = rv			Move assignment; destroys the old state of f, gets the state of rv, and invalidates the state of rv
f.valid()			Yields true if f has a valid state, so you can call the following member functions
f.get()			Blocks until the background operation is done 
			(forcing a deferred associated functionality to start synchronously), yields the result (if any) or raises any 
			exception that occurred, and invalidates its state
f.wait()			Blocks until the background operation is done 
			(forcing a deferred associated functionality to start synchronously)
f.wait_for(dur)		Blocks for duration dur or until the background operation is done 
			(a deferred thread is not forced to start)
f.wait_until(tp)		Blocks until timepoint tp or until the background operation is done 
			(a deferred thread is not forced to start)
f.share()			Yields a shared_future with the current state and invalidates the state of f

Note that the return value of get() depends on the type future<> is specialized with:
If it is void, get() also has type void and returns nothing.
If the future is parametrized with a reference type, get() returns a reference to the return value.
Otherwise, get() returns a copy or move assigns the return value, depending on whether the return type supports move assignment semantics.
Note that you can call get() only once, because get() invalidates the future’s state.
For a future that has an invalid state, calling anything else but the destructor, the move assignment operator, or valid() results in undefined behavior. 
For this case, the standard recommends throwing an exception of type future_error with the code std::future_errc::no_state, but this is not required.
Note that neither a copy constructor nor a copy assignment operator is provided, ensuring that no two objects can share the state of a background operation. 
You can move the state to another future object only by calling the move constructor or the move assignment operator. 
However, the state of background tasks can be shared in multiple objects by using a shared_future object, which share() yields.
If the destructor is called for a future that is the last owner of a shared state and the associated task has started but not finished yet, the destructor blocks until the end of the task.

shared_future

Class shared_future provides the same semantics and interface as class future with the following differences:
Multiple calls of get() are allowed. Thus, get() does not invalidate its state.
Copy semantics (copy constructor, copy assignment operator) are supported.
get() is a constant member function returning a const reference to the value stored in the shared state 
(which means that you have to ensure that the lifetime of the returned reference is shorter than the shared state). 
For class std::future, get() is a nonconstant member function returning a move-assigned copy (or a copy if that’s not supported), unless the class is specialized by a reference type.
Member share() is not provided.
The fact that the return value of get() is not copied creates some risks. Besides lifetime issues, data races are possible. 

int f()
{
  return 1;
}

int main()
{
  std::thread t(f); // 如何读取f的返回值？
  t.join();
}

std::async

    先用std::async启动一个异步任务，它返回一个持有计算结果的std::future，通过std::future::get即可阻塞线程，直到期值的状态为ready并返回该结果

template< class Function, class... Args>
[[nodiscard]] 
std::future<std::invoke_result_t<std::decay_t<Function>,  std::decay_t<Args>...>>
    async( Function&& f, Args&&... args );

template< class Function, class... Args >
[[nodiscard]] 
std::future<std::invoke_result_t<std::decay_t<Function>, std::decay_t<Args>...>>
    async( std::launch policy, Function&& f, Args&&... args );

int f()
{
  return 1;
}

int main()
{
  std::future<int> ft = std::async(f);
  std::cout << ft.get(); // 1
}

    std::async和std::thread一样支持额外的函数参数

// 函数
int f(int);
auto ft = std::async(f, 42);

// 成员函数
struct A {
  int f(int);
};

A a;
auto ft1 = std::async(&A::f, &a, 42); // 调用p->f(42)，p是指向a的指针
auto ft2 = std::async(&A::f, a, 42); // 调用tmpa.f(42)，tmpa是a的副本

// 函数对象
struct A {
  int operator()(int);
};
A a;
auto ft1 = std::async(A(), 42); // 调用tmpa(42)，tmpa由A的移动构造函数获得
auto ft2 = std::async(std::ref(a), 42); // 调用a(42)

    std::async还可以设置第一个参数为线程的创建策略

int f();
// 函数必须异步执行，即运行在不同的线程上
auto ft1 = std::async(std::launch::async, f);
// 函数只在返回的期值调用get或wait时运行
auto ft2 = std::async(std::launch::deferred, f);
// 不指定时的默认启动策略是对两者进行或运算的结果
// auto ft3 = std::async(f)等价于
auto ft3 = std::async(std::launch::async | std::launch::deferred, f);

std::packaged_task

    除了std::async，还可以用std::packaged_task让std::future与任务关联

packaged_task

Operation			Effect
packaged_task pt		Default constructor; creates a packaged task with no shared state and no stored task
packaged_task pt(f)		Creates an object for the task f
packaged_task pt(alloc,f)		Creates an object for the task f using allocator alloc
packaged_task pt(rv)		Move constructor; moves the packaged task rv (task and state) to pt 
				(rv has no shared state afterward)
pt.~packaged_task()		Destroys *this (might make shared state ready)
pt = rv				Move assignment; move assigns the packaged task rv (task and state) to pt 
				(rv has no shared state afterward)
swap(pt1,pt2)			Swaps packaged tasks
pt1.swap(pt2)			Swaps packaged tasks
pt.valid()			Yields true if pt has a shared state
pt.get_future()			Yields a future object to retrieve the shared state (outcome of the task)
pt(args)				Calls the task (with optional arguments) and makes the shared state ready
pt.make_ready_at_thread_exit(args)	Calls the task (with optional arguments) and at thread exit makes the shared state ready
pt.reset()			Creates a new shared state for pt (might make the old shared state ready)

int f();

std::packaged_task<int()> pt(f);
auto ft = pt.get_future();
pt(); // 调用std::packaged_task对象，将std::future设为就绪
std::cout << ft.get();

    很多GUI架构要求用指定线程更新GUI，如果另一个线程要更新GUI，就需要发送信消息给指定线程。使用std::packaged_task即可实现此功能

std::mutex m;
std::deque<std::packaged_task<void()>> d;

void gui_thread() // 更新GUI的指定线程
{
  while (!gui_shutdown_message_received()) // 未收到终止消息则一直轮询
  {
    process_gui_message(); // 处理收到的消息
    std::packaged_task<void()> pt;
    {
      std::lock_guard<std::mutex> l(m);
      if (d.empty()) continue; // 进入下一次循环
      pt = std::move(d.front());
      d.pop_front();
    }
    pt();
  }
}

std::thread t(gui_thread);

template<typename F>
std::future<void> postTask(F f)
{
  std::packaged_task<void()> pt(f);
  std::future<void> res = pt.get_future();
  std::lock_guard<std::mutex> l(m);
  d.push_back(std::move(pt));
  return res;
}

std::promise

    std::promise可以显式设置值

promise

Operation			Effect
promise p			Default constructor; creates a promise with shared state
promise p(allocator_arg,alloc)	Creates a promise with shared state, which uses alloc as allocator
promise p(rv)			Move constructor; creates a new promise object, 
				which gets the state of rv and removes the shared state from rv
p.~promise()			Releases the shared state and if it is not ready (no value or exception), 
				stores a std::future_error exception with condition broken_promise
p = rv				Move assignment; move assigns the state of rv to p and if p was not ready, 
				stores a std::future_error exception with condition broken_promise there
swap(p1,p2)			Swaps states of p1 and p2
p1.swap(p2)			Swaps states of p1 and p2
p.get_future()			Yields a future object to retrieve the shared state (outcome of a thread)
p.set_value(val)			Sets val as (return) value and makes the state ready (or throws std::future_error)
p.set_value_at_thread_exit(val)	Sets val as (return) value and makes the state ready at the end of the current thread 
				(or throws std::future_error)
p.set_exception(e)		Sets e as exception and makes the state ready (or throws std::future_error)
p.set_exception_at_thread_exit(e)	Sets e as exception and makes the state ready at the end of the current thread 
				(or throws std::future_error)

std::promise<int> ps;
std::future<int> ft = ps.get_future();
ps.set_value(42); // set_value还会将状态设置为就绪
std::cout << ft.get(); // 42

    在线程间对状态发送信号

void f(std::promise<void> ps)
{
  std::this_thread::sleep_for(std::chrono::seconds(1));
  ps.set_value();
}

int main()
{
  std::promise<void> ps;
  std::future<void> ft = ps.get_future();
  std::thread t(f, std::move(ps));
  ft.wait(); // 阻塞直到set_value，相当于没有返回值的get
  t.join();
}

    一个std::promise只能关联一个std::future，关联多次时将抛出std::future_error异常

std::promise<int> ps;
auto ft1 = ps.get_future();
auto ft2 = ps.get_future(); // 抛出std::future_error异常

将异常存储于期值中

int f(int x)
{
  if (x < 0)
  {
    throw std::out_of_range("x < 0");
  }
  return 1;
}

int main()
{
  auto ft = std::async(f, -1); // ft将存储异常
  int x = ft.get(); // 抛出已存储的异常
}

    std::promise也支持此功能

int main()
{
  std::promise<int> ps;
  auto ft = ps.get_future();
  std::thread t([&ps]
  {
    try
    {
      ps.set_value(f(-1)); // 此时还没有存储异常
    }
    catch(...)
    {
      ps.set_exception(std::current_exception()); // 存储异常
    }
  });
  t.join();
  ft.get(); // 抛出异常
}

    如果std::packaged_task和std::promise直到析构都未设置值，std::future::get也会抛出std::future_error异常

int f();

int main()
{
  std::future<int> ft;
  {
    std::packaged_task<int()> pt(f);
    ft = pt.get_future();
    // std::promise<int> ps;
    // ft = ps.get_future();
  }
  ft.get(); // 抛出异常
}

std::shared_future

    std::future调用get后就无法再次get，也就是说只能获取一次数据，此外还会导致所在线程与其他线程数据不同步。std::shared_future就可以解决此问题

std::promise<int> ps;
std::future<int> ft(ps.get_future());
assert(ft.valid());
std::shared_future<int> sf(std::move(ft));
assert(!ft.valid());
assert(sf.valid());

    也可以直接构造

std::promise<int> ps;
// std::future隐式转换为std::shared_future
std::shared_future<int> sf(ps.get_future());

    用std::future::share可以直接生成std::shared_future，这样就可以直接用auto简化声明std::shared_future

std::promise<int> ps;
auto sf = ps.get_future().share();

    每一个std::shared_future对象上返回的结果不同步，为了避免多线程访问同一std::shared_future对象时的数据竞争就必须加锁保护。
    更好的方法是给每个线程拷贝一个std::shared_future对象，这样就可以安全访问而无需加锁

限定等待时间

    阻塞调用的时间不确定，在一些情况下需要限制等待时间。指定超时的方式有两种，一是指定一段延迟的时间（duration），另一种是指定一个时间点

时钟（clock）

    对于标准库来说，时钟就是时间信息源。具体来说，时钟是提供了四种信息的类
        当前时间：如std::chrono::system_clock::now()
        表示时间值的类型：std::chrono::time_point
        时钟节拍（一个嘀嗒的周期）：一般一秒有25个节拍，一个周期则为std::ratio<1, 25>
        通过时钟节拍确定时钟是否稳定（steady，匀速）：std::chrono::steady_clock::is_steady（稳定时钟，代表系统时钟的真实时间）、
        std::chrono::system_clock::is_steady（一般因为时钟可调节而不稳定，即使这是为了考虑本地时钟偏差的自动调节）、high_resolution_clock::is_steady（最小节拍最高精度的时钟）
        打印当前系统时间（如果出现localtime不安全的警告，则在配置属性 - C/C++ - 预处理器 - 预处理器定义添加_CRT_SECURE_NO_WARNINGS）

std::chrono::system_clock::time_point now = std::chrono::system_clock::now();
std::time_t now_c = std::chrono::system_clock::to_time_t(now); // 转为整数
std::cout << std::put_time(std::localtime(&now_c), "%F %T"); // %F即%Y-%m-%d，%T即%H:%M:%S

std::chrono::duration

    标准库提供了表示时间间隔类型的std::chrono::duration

// 比如将表示秒的类型定义为
std::duration<int> // 即std::chrono::seconds
// 则表示分的类型可定义为
std::duration<int, std::ratio<60>> // 即std::chrono::minutes
// 表示毫秒的类型可定义为
std::duration<int, std::ratio<1, 1000>> // 即std::chrono::milliseconds

    C++14的std::chrono_literals提供了表示时间的后缀

using namespace std::chrono_literals;
auto x = 45min; // 等价于std::chrono::minutes(45)
std::cout << x.count(); // 45
auto y = std::chrono::duration_cast<std::chrono::seconds>(x);
std::cout << y.count(); // 2700
auto z = std::chrono::duration_cast<std::chrono::hours>(x);
std::cout << z.count(); // 0（转换会截断）

    标准库通过字面值运算符模板实现此后缀功能

constexpr std::chrono::minutes operator ""min(unsigned long long m)
{
  return std::chrono::minutes(m);
}

    duration支持四则运算

using namespace std::chrono_literals;
auto x = 1h;
auto y = 15min;
auto z = x - 2 * y;
std::cout << z.count(); // 30

    使用duration即可设置等待时间

int f();
auto ft = std::async(f);

using namespace std::chrono_literals;
if (ft.wait_for(1s) == std::future_status::ready)
{
  std::cout << ft.get();
}

std::chrono::time_point

    time_point是表示时间的类型，值为从某个时间点（比如unix时间戳：1970年1月1日0时0分）开始计时的时间长度

// 第一个模板参数为开始时间点的时钟类型，第二个为时间单位
std::chrono::time_point<std::chrono::system_clock, std::chrono::seconds>

    time_point可以加减dutation

using namespace std::chrono_literals;
auto x = std::chrono::high_resolution_clock::now();
auto y = x + 1s;
std::cout << std::chrono::duration_cast<std::chrono::milliseconds>(y - x).count();

    两个time_point也能相减

auto start = std::chrono::high_resolution_clock::now();
doSomething();
auto stop = std::chrono::high_resolution_clock::now();
std::cout << std::chrono::duration_cast<std::chrono::milliseconds>(stop - start).count();

    使用绝对的时间点来设置等待时间

std::condition_variable cv;
bool done;
std::mutex m;

bool wait_loop()
{
  const auto timeout = std::chrono::steady_clock::now() + std::chrono::milliseconds(500);
  std::unique_lock<std::mutex> l(m);
  while (!done)
  {
    if (cv.wait_until(l, timeout) == std::cv_status::timeout) break;
  }
  return done;
}

接受timeout的函数

    timeout可以用于休眠，比如std::this_thread::sleep_for和std::this_thread::sleep_until，此外timeout还能配合条件变量、期值甚至mutex使用。
    std::mutex和std::recursive_mutex不支持timeout，而std::timed_mutex和std::recursive_timed_mutex支持，它们提供了try_lock_for和try_lock_until
    支持timeout的函数有
        std::this_thread::sleep_for
        std::this_thread::sleep_until
        std::condition_variable::wait_for
        std::condition_variable::wait_until
        std::condition_variable_any::wait_for
        std::condition_variable_any::wait_until
        std::timed_mutex::try_lock_for
        std::timed_mutex::try_lock_until
        std::recursive_timed_mutex::try_lock_for
        std::recursive_timed_mutex::try_lock_until
        std::unique_lock::try_lock_for
        std::unique_lock::try_lock_until
        std::future::wait_for
        std::future::wait_until
        std::shared_future::wait_for
        std::shared_future::wait_until


std::future<...>    f(std::async(func));	// try to call func asynchronously
...
f.wait();	// wait for func to be done (might start background task)

std::future<...>    f(std::async(func));	// try to call func asynchronously
...
f.wait_for(std::chrono::seconds(10));	// wait at most 10 seconds for func

std::future<...>    f(std::async(func));	// try to call func asynchronously
...
f.wait_until(std::system_clock::now()+std::chrono::minutes(1));
Both wait_for() and wait_until() return one of the following:
std::future_status::deferred if async() deferred the operation and no calls to wait() or get() have yet forced it to start 
(both function return immediately in this case)
std::future_status::timeout if the operation was started asynchronously but hasn’t finished yet 
(if the waiting expired due to the passed timeout)
std::future_status::ready if the operation has finished
Using wait_for() or wait_until() especially allows to program so-called speculative execution. 
For example, consider a scenario where we must have a usable result of a computation within a certain time, and it would be nice to have an accurate answer:
int quickComputation(); // process result ‘‘quick and dirty’’
int accurateComputation(); // process result ‘‘accurate but slow’’
std::future<int>  f;	// outside declared because lifetime of accurateComputation()
                                           // might exceed lifetime of bestResultInTime()
int bestResultInTime()
{
    // define time slot to get the answer:
    auto tp = std::chrono::system_clock::now() + std::chrono::minutes(1);

    // start both a quick and an accurate computation:
    f = std::async (std::launch::async,  accurateComputation);
    int guess = quickComputation();

    // give accurate computation the rest of the time slot:
    std::future_status s = f.wait_until(tp);

    // return the best computation result we have:
    if (s == std::future_status::ready)
    {
        return f.get();
    }
    else
    {
        return guess;	// accurateComputation() continues
    }
}
Note that the future f can’t be a local object declared inside bestResultInTime() because when the time was too short to finish accurateComputation() 
the destructor of the future would block until that asynchronous task has finished.
By passing a zero duration or a timepoint that has passed, you can simply “poll” to see whether a background task has started and/or is (still) running:
future<...>   f(async(task));	// try to call task asynchronously
...
// do something while task has not finished (might never happen!)
while (f.wait_for(chrono::seconds(0)) != future_status::ready) {
...
}
Note, however, that such a loop might never end, because, for example, on single-threaded environments, the call will be deferred until get() is called. 
So you either should call async() with the std::launch::async launch policy passed as first argument or check explicitly whether wait_for() returns std::future_status::deferred:
future<...>   f(async(task));	// try to call task asynchronously
...
// check whether task was deferred:
if (f.wait_for(chrono::seconds(0)) != future_status::deferred)
{
    // do something while task has not ?nished
    while (f.wait_for(chrono::seconds(0)) != future_status::ready)
    {
        ...
    }
}
...
auto r = f.get(); // force execution of task and wait for result (or exception)
Another reason for an endless loop here might be that the thread executing the loop has the processor and the other threads are not getting any time to make the future ready. 
This can reduce the speed of programs dramatically. The quickest fix is to call yield() (see Section 18.3.7, page 981) inside the loop:
std::this_thread::yield(); // hint to reschedule to the next thread
and/or sleep for a short period of time.

使用同步操作简化代码
使用期值进行函数式编程（functional programming）

    FP不会改变外部状态，不修改共享数据就不存在race condition，因此也就没有必要使用锁
    以快速排序为例

    快速排序的顺序实现（虽然接口是函数式，但考虑到FP实现需要大量拷贝操作，所以内部使用命令式）

template<typename T>
std::list<T> f(std::list<T> v)
{
  if (v.empty()) return v;
  std::list<T> res;
  // std::list::splice用于转移另一个list中的元素到目标list
  res.splice(res.begin(), v, v.begin()); // 将v的首元素移到res中
  const T& firstVal = *res.begin();
  // std::partition按条件在原容器上划分为两部分
  // 并返回划分点（第一个不满足条件元素）的迭代器
  auto it = std::partition(v.begin(), v.end(), [&](const T& x) { return x < firstVal; });
  std::list<T> low;
  low.splice(low.end(), v, v.begin(), it); // 转移左半部分到low
  auto l(f(std::move(low))); // 对左半部分递归排序
  auto r(f(std::move(v))); // 对右半部分递归排序
  res.splice(res.end(), r);
  res.splice(res.begin(), l);
  return res;
}

    使用期值实现并行的快速排序

template<typename T>
std::list<T> f(std::list<T> v)
{
  if (v.empty()) return v;
  std::list<T> res;
  res.splice(res.begin(), v, v.begin());
  const T& firstVal = *res.begin();
  auto it = std::partition(v.begin(), v.end(), [&](const T& x) { return x < firstVal; });
  std::list<T> low;
  low.splice(low.end(), v, v.begin(), it);
  // 用另一个线程对左半部分排序
  std::future<std::list<T>> l(std::async(&f<T>, std::move(low)));
  // 其他不变
  auto r(f(std::move(v)));
  res.splice(res.end(), r);
  res.splice(res.begin(), l.get()); // 获取future中的值
  return res;
}

    FP不仅是并发编程的典范，还是CSP（Communicating Sequential Processer）的典范。CSP中的线程理论上是分开的，没有共享数据，
    但communication channel允许消息在不同线程间传递，这被Erlang所采用，并在MPI（Message Passing Interface）上常用来做C和C++的高性能计算

使用消息传递进行同步操作

    CSP的思路很简单，如果没有共享数据，每个线程可以完全独立地思考，其行为取决于收到的消息。
    因此每个线程实际上是一个状态机，收到一条消息时就以某种方式更新状态，并且还可能发送消息给其他线程
    真正的CSP没有共享数据，所有通信通过消息队列传递，但由于C++线程共享地址空间，因此无法强制实现这个要求。
    所以这就需要引入一些约定，作为应用或者库的作者，必须确保在线程间不会共享数据（当然为了通信，必须共享消息队列）
    考虑实现一个ATM应用，它需要处理取钱时和银行的交互，并控制物理机器对银行卡的反应。一个处理方法是分三个线程，分别处理物理机器、ATM逻辑、与银行的交互，
    线程间通过消息通讯而非共享数据，比如插卡时机器线程发送消息给逻辑线程，逻辑线程返回一条消息通知机器线程可以给多少钱。
    一个简单的ATM逻辑的状态机建模如下

    这个状态机可以用一个类实现，类中有一个表示状态的成员函数指针

struct card_inserted
{
  std::string account;
};

class atm
{
  messaging::receiver incoming;
  messaging::sender bank;
  messaging::sender interface_hardware;
  void (atm::*state)();
  std::string account;
  std::string pin;
  
  void waiting_for_card()
  {
    interface_hardware.send(display_enter_card());
    incoming.wait()
    .handle<card_inserted>([&](const card_inserted& msg)
    {
      account = msg.account;
      pin = "";
      interface_hardware.send(display_enter_pin());
      state = &atm::getting_pin;
    });
  }
  
  void getting_pin();
public:
  void run()
  {
    state = &atm::waiting_for_card;
    try
    {
      for (;;) (this->*state)();
    }
    catch(const messaging::close_queue&) {}
  }
};

    getting_pin的实现比较繁琐，它要处理三种不同的消息

void atm::getting_pin()
{
  incoming.wait()
  .handle<digit_pressed>([&](const digit_pressed& msg)
  {
    const unsigned pin_length = 4;
    pin += msg.digit;
    if (pin.length() == pin_length)
    {
      bank.send(verify_pin(account, pin, incoming));
      state = &atm::verifying_pin;
    }
  })
  .handle<clear_last_pressed>([&](const clear_last_pressed& msg)
  {
    if (!pin.empty()) pin.resize(pin.length() - 1);
  })
  .handle<cancel_pressed>([&](const cancel_pressed& msg)
  {
    state = &atm::done_processing;
  });
}

    这里不需要考虑同步和并发的问题，只要考虑在某个点接受和发送的消息。这个ATM逻辑的状态机与系统的其他部分各自运行在独立的线程上，这种设计方式称为actor model，
    系统中有多个独立的actor，actor之间可以互相发送消息但不会共享状态，这种方式可以极大简化并发系统的设计

std::experimental::future

    并发TS中提供了std::experimental::promise和std::experimental::packaged_task，与标准库唯一不同的是，它们返回std::experimental::future，这个期值提供了持续性。
    持续性带来的好处是数据就绪就（then）进行处理，调用std::experimental::future::then即可添加持续性

    std::async只能返回std::future，如果想返回std::experimental::future则需要手动实现一个新的async

template<typename F>
std::experimental::future<decltype(std::declval<F>()())>
new_async(F&& func)
{
  std::experimental::promise<decltype(std::declval<F>()())> p;
  auto ft = p.get_future();
  std::thread t(
    [p = std::move(p), f = std::decay_t<F>(func)]() mutable {
      try {
        p.set_value_at_thread_exit(f());
      } catch(...) {
        p.set_exception_at_thread_exit(std::current_exception());
      }
    }
  );
  t.detach();
  return ft;
}

    假如要实现一个登录逻辑，将用户名和密码发送给后台验证，取得用户信息后更新到显示界面，串行实现如下

void process_login(const std::string& username, const std::string& password)
{
  try {
    const user_id id = backend.authenticate_user(username, password);
    const user_data info_to_display = backend.request_current_info(id);
    update_display(info_to_display);
  } catch(std::exception& e) {
    display_error(e);
  }
}

首先修改为如下

std::future<void> process_login(const std::string& username, const std::string& password)
{
  return std::async(std::launch::async, [=] () {
    try {
      const user_id id = backend.authenticate_user(username, password);
      const user_data info_to_display = backend.request_current_info(id);
      update_display(info_to_display);
    } catch(std::exception& e) {
      display_error(e);
    }
  });
}

再修改为如下

std::experimental::future<void> process_login(const std::string& username, const std::string& password)
{
  return new_async([=] () {
    return backend.authenticate_user(username, password);
  })
  .then([] (std::experimental::future<user_id> id) {
    return backend.request_current_info(id.get());
  })
  .then([] (std::experimental::future<user_data> info_to_display) {
    try {
      update_display(info_to_display.get());
    } catch(std::exception& e) {
      display_error(e);
    }
  });
}

假设backend.async_authenticate_user返回std::experimental::future<user_id>
再次修改为如下

std::experimental::future<void> process_login(const std::string& username, const std::string& password)
{
  return backend.async_authenticate_user(username, password)
  .then([] (std::experimental::future<user_id> id) {
    return backend.async_request_current_info(id.get());
  })
  .then([] (std::experimental::future<user_data> info_to_display) {
    try {
      update_display(info_to_display.get());
    } catch(std::exception& e) {
      display_error(e);
    }
  });
}

最后这里还可以用泛型lambda来简化代码

std::experimental::future<void> process_login(const std::string& username, const std::string& password)
{
  return backend.async_authenticate_user(username, password)
  .then([] (auto id) {
    return backend.async_request_current_info(id.get());
  })
  .then([] (auto info_to_display) {
    try {
      update_display(info_to_display.get());
    } catch(std::exception& e) {
      display_error(e);
    }
  });
}

    除了std::experimental::future，支持持续性的还有 std::experimental::shared_future

auto ft1 = new_async(some_function).share();
auto ft2 = ft1.then([] (std::experimental::shared_future<some_data> data) {
  do_stuff(data);
});
auto ft3 = ft1.then([] (std::experimental::shared_future<some_data> data) {
  return do_other_stuff(data);
});

std::experimental::when_all

    使用std::async从多个期值中获取结果

std::future<FinalResult> process_data(std::vector<MyData>& vec)
{
  const size_t chunk_size = whatever;
  std::vector<std::future<ChunkResult>> res;
  for (auto begin = vec.begin(), end = vec.end(); beg! = end;)
  {
    const size_t remaining_size = end - begin;
    const size_t this_chunk_size = std::min(remaining_size, chunk_size);
    res.push_back(std::async(process_chunk, begin, begin + this_chunk_size));
    begin += this_chunk_size;
  }
  return std::async([all_results = std::move(res)] () {
    std::vector<ChunkResult> v;
    v.reserve(all_results.size());
    for (auto& f: all_results)
    {
      v.push_back(f.get()); // 这里会导致反复唤醒，增加了很多开销
    }
    return gather_results(v);
  });
}

    使用std::experimental::when_all可以避免反复唤醒导致的开销，为其传入一组需要等待的期值，将返回一个新的期值。当传入的所有期值都就绪时，则返回的期值就绪

std::experimental::future<FinalResult> process_data(std::vector<MyData>& vec)
{
  const size_t chunk_size = whatever;
  std::vector<std::experimental::future<ChunkResult>> res;
  for (auto begin = vec.begin(), end = vec.end(); beg! = end;)
  {
    const size_t remaining_size = end - begin;
    const size_t this_chunk_size = std::min(remaining_size, chunk_size);
    res.push_back(new_async(process_chunk, begin, begin + this_chunk_size));
    begin += this_chunk_size;
  }
  return std::experimental::when_all(res.begin(), res.end())
  .then([] (std::future<std::vector<std::experimental::future<ChunkResult>>> ready_results)
  {
    std::vector<std::experimental::future<ChunkResult>>
    all_results = ready_results.get();
    std::vector<ChunkResult> v;
    v.reserve(all_results.size());
    for (auto& f: all_results)
    {
      v.push_back(f.get());
    }
    return gather_results(v);
  });
}

std::experimental::when_any

    在传入的期值中有一个就绪时，则std::experimental::when_any返回的期值就绪

std::experimental::future<FinalResult>
find_and_process_value(std::vector<MyData>& data)
{
  const unsigned concurrency = std::thread::hardware_concurrency();
  const unsigned num_tasks = (concurrency > 0) ? concurrency : 2;
  std::vector<std::experimental::future<MyData*>> res;
  const auto chunk_size = (data.size() + num_tasks - 1) / num_tasks;
  auto chunk_begin = data.begin();
  std::shared_ptr<std::atomic<bool>> done_flag = std::make_shared<std::atomic<bool>>(false);
  for (unsigned i = 0; i < num_tasks; ++i)
  { // 产生num_tasks个异步任务到res中
    auto chunk_end = (i < (num_tasks - 1)) ? chunk_begin + chunk_size : data.end();
    res.push_back(new_async([=] {
      for (auto entry = chunk_begin; !*done_flag && (entry != chunk_end); ++entry)
      {
        if (matches_find_criteria(*entry))
        {
          *done_flag = true;
          return &(*entry);
        }
      }
      return (MyData*)nullptr;
    }));
    chunk_begin = chunk_end;
  }
  std::shared_ptr<std::experimental::promise<FinalResult>> final_result =
    std::make_shared<std::experimental::promise<FinalResult>>();

  struct DoneCheck {
    std::shared_ptr<std::experimental::promise<FinalResult>> final_result;
    
    DoneCheck(std::shared_ptr<std::experimental::promise<FinalResult>> final_result_)
    : final_result(std::move(final_result_)) {}
    
    void operator()(
      std::experimental::future<std::experimental::when_any_result<
        std::vector<std::experimental::future<MyData*>>>> res_param)
    {
      auto res = res_param.get();
      MyData* const ready_result = res.futures[res.index].get(); // 从就绪的期值中获取值
      // 找到符合条件的值则处理结果并set_value
      if (ready_result)
      {
        final_result->set_value(process_found_value(*ready_result));
      }
      else
      {
        res.futures.erase(res.futures.begin() + res.index); // 否则丢弃值
        if (!res.futures.empty())
        { // 如果还有需要检查的值则再次调用when_any
          std::experimental::when_any(res.futures.begin(), res.futures.end())
          .then(std::move(*this));
        }
        else
        { // 如果没有其他期值则在promise中设置一个异常
          final_result->set_exception(std::make_exception_ptr(std::runtime_error("Not found")));
        }
      }
    }
  };
  std::experimental::when_any(res.begin(), res.end()).then(DoneCheck(final_result));
  return final_result->get_future();
}

    when_all和when_any除了可以接收一对迭代器，也可以直接接受期值

std::experimental::future<int> ft1 = new_async(f1);
std::experimental::future<std::string> ft2 = new_async(f2);
std::experimental::future<double> ft3 = new_async(f3);
std::experimental::future<
  std::tuple<
    std::experimental::future<int>,
    std::experimental::future<std::string>,
    std::experimental::future<double>>> res =
  std::experimental::when_all(std::move(ft1), std::move(ft2), std::move(ft3));

std::experimental::latch

    std::experimental::latch用一个计数器值构造，等待事件发生时就调用std::experimental::latch::count_down将计数器值减一，
    计数器值为0时则std::experimental::latch::is_ready返回true，如果想让计数器减一并阻塞至0则可以调用std::experimental::latch::count_down_and_wait
    用std::experimental::latch等待事件

void f()
{
  const unsigned thread_count = ...;
  std::experimental::latch done(thread_count); // 用计数器值构造latch
  my_data data[thread_count];
  std::vector<std::future<void>> threads;
  for (unsigned i = 0; i < thread_count; ++i)
  {
    threads.push_back(std::async(std::launch::async, [&, i] {
      data[i] = make_data(i);
      done.count_down(); // 在进行下一步之前先递减计数器
      do_more_stuff();
    }));
  }
  done.wait(); // 等待至计数器为0
  process_data(data, thread_count);
}

std::experimental::barrier

    一组处理数据的线程，处理过程中独立，无需同步，但在处理下一项数据前，必须要求所有线程完成任务。std::experimental::barrier就可以用于处理这种情况，
    它用线程的数量构造，调用std::experimental::barrier::arrive_and_wait会阻塞至所有线程完成任务，当最后一个线程完成任务时，所有线程被释放，barrier被重置。
    如果想从线程集中移除线程，可以让该线程在barrier上调用std::experimental::barrier::arrive_and_drop

result_chunk process(data_chunk);
std::vector<data_chunk> divide_into_chunks(data_block data, unsigned num_threads);

void process_data(data_source& source, data_sink& sink) // 源数据和输出数据
{
  const unsigned concurrency = std::thread::hardware_concurrency();
  const unsigned num_threads = (concurrency > 0) ? concurrency : 2;
  std::experimental::barrier b(num_threads); // 构造barrier
  std::vector<joining_thread> threads(num_threads);
  std::vector<data_chunk> chunks;
  result_block res;
  for (unsigned i = 0; i < num_threads; ++i)
  {
    threads[i] = joining_thread([&, i] {
      while (!source.done()) // 循环至处理完所有任务
      {
        if (i == 0)
        { // 线程0拆分数据
          data_block current_block = source.get_next_data_block();
          chunks = divide_into_chunks(current_block, num_threads);
        }
        b.arrive_and_wait(); // 这里阻塞是因为其他线程必须等待线程0初始化
        res.set_chunk(i, num_threads, process(chunks[i])); // 更新结果到res
        b.arrive_and_wait(); // 这里阻塞是因为写入前线程0必须等待其他线程完成
        if (i == 0) sink.write_data(std::move(res)); // 只有线程0可以输出结果
      }
    });
  }
}

std::experimental::flex_barrier

    std::experimental::flex_barrier比std::experimental::barrier，但开销更大，此外std::experimental::flex_barrier还要多接受一个函数作为参数，
    当所有线程到达阻塞点时，由其中一个线程运行该函数
    下面用std::experimental::flex_barrier简化上面的例子

void process_data(data_source& source, data_sink& sink)
{
  const unsigned concurrency = std::thread::hardware_concurrency();
  const unsigned num_threads = (concurrency > 0) ? concurrency : 2;
  std::vector<data_chunk> chunks;
  auto split_source = [&] {
    if (!source.done())
    {
      data_block current_block = source.get_next_data_block();
      chunks = divide_into_chunks(current_block, num_threads);
    }
  };
  split_source(); // 先调用上面的lambda拆分数据
  result_block res;
  std::experimental::flex_barrier sync(num_threads, [&] {
    sink.write_data(std::move(res)); // 每次迭代完成输出一次结果
    split_source(); // 并为下一次迭代拆分数据
    return -1; // 必须返回不小于-1的值且不抛出异常，-1表示线程数不变
    // 返回其他值则表示下一周期的线程数
  });
  std::vector<joining_thread> threads(num_threads);
  for (unsigned i = 0; i < num_threads; ++i)
  {
    threads[i] = joining_thread([&, i] {
      while (!chunks.empty())   // 如果还有数据要处理
      {
        res.set_chunk(i, num_threads, process(chunks[i])); // 更新结果到res
        sync.arrive_and_wait(); // 只需要一个阻塞点
      }
    });
  }
}

04 C++内存模型和基于原子类型的操作

内存模型基础

    为了避免race condition，线程就要规定执行顺序。一种方式是使用mutex，后一线程必须等待前一线程解锁。第二种方式是使用原子操作来避免竞争访问同一内存位置
    原子操作是不可分割的操作，要么做了要么没做，不存在做一半的状态。如果读取对象值的加载操作是原子的，那么对象上的所有修改操作也是原子的，
    读取的要么是初始值，要么是某个修改完成后的存储值。因此，原子操作不存在修改过程中值被其他线程看到的情况，也就避免了竞争风险
    每个对象从初始化开始都有一个修改顺序，这个顺序由来自所有线程对该对象的写操作组成。通常这个顺序在运行时会变动，但在任何给定的程序执行中，系统中所有线程都必须遵循此顺序
    如果对象不是原子类型，就要通过同步来保证线程遵循每个变量的修改顺序。如果一个变量对于不同线程表现出不同的值序列，就会导致数据竞争和未定义行为。
    使用原子操作就可以把同步的责任抛给编译器

原子操作和原子类型
标准原子类型

    标准原子类型定义在<atomic>中。也可以用mutex模拟原子操作，实际上标准原子类型可能就是这样实现的，它们都有一个is_lock_free函数，返回true说明该原子类型操作是无锁的，
    用的是原子指令，返回false则是用锁

struct A { int a[100]; };
struct B { int x, y; };

std::cout << std::boolalpha
  << std::atomic<A>{}.is_lock_free() // false
  << std::atomic<B>{}.is_lock_free(); // true

    原子操作的主要用处是替代mutex实现同步。如果原子操作内部是用mutex实现的，就不会有期望的性能提升，还不如直接用mutex来同步。
    C++17中每个原子类型都有一个is_always_lock_free成员变量，为true时表示该原子类型在此平台上lock-free

std::cout << std::atomic<int>{}.is_always_lock_free; // 1

    C++17之前可以用标准库为各个原子类型定义的ATOMIC_xxx_LOCK_FREE宏来判断该类型是否无锁，值为0表示原子类型是有锁的，为2表示无锁，为1表示运行时才能确定

// VS2019中的定义，位于头文件<atomic>

// LOCK-FREE PROPERTY
#define ATOMIC_BOOL_LOCK_FREE 2
#define ATOMIC_CHAR_LOCK_FREE 2
#ifdef __cpp_lib_char8_t
#define ATOMIC_CHAR8_T_LOCK_FREE 2
#endif // __cpp_lib_char8_t
#define ATOMIC_CHAR16_T_LOCK_FREE 2
#define ATOMIC_CHAR32_T_LOCK_FREE 2
#define ATOMIC_WCHAR_T_LOCK_FREE  2
#define ATOMIC_SHORT_LOCK_FREE  2
#define ATOMIC_INT_LOCK_FREE    2
#define ATOMIC_LONG_LOCK_FREE   2
#define ATOMIC_LLONG_LOCK_FREE  2
#define ATOMIC_POINTER_LOCK_FREE  2

    只有std::atomic_flag未提供is_lock_free，该类型是一个简单的布尔标志，所有操作都保证lock-free。基于std::atomic_flag就能实现一个简单的锁，并实现其他基础原子类型。
    其余原子类型可以通过特化std::atomic来实现，且可以有更完整的功能，但不保证lock-free
    标准库中为std::atomic对内置类型的特化定义了类型别名，比如

namespace std {
  using atomic_bool = atomic<bool>;
  using atomic_char = std::atomic<char>;
}

    通常类型std::atomic<T>的别名就是atomic_T，只有以下几种例外：signed缩写为s，unsigned缩写为u，long long缩写为llong

namespace std {
  using atomic_schar = std::atomic<signed char>;
  using atomic_uchar = std::atomic<unsigned char>;
  using atomic_uint = std::atomic<unsigned>;
  using atomic_ushort = std::atomic<unsigned short>;
  using atomic_ulong = std::atomic<unsigned long>;
  using atomic_llong = std::atomic<long long>;
  using atomic_ullong = std::atomic<unsigned long long>;
}

原子类型 	相关特化类
atomic_bool 	std::atomic<bool>
atomic_char 	std::atomic<char>
atomic_schar 	std::atomic<signed char>
atomic_uchar 	std::atomic<unsigned char>
atomic_int 	std::atomic<int>
atomic_uint 	std::atomic<unsigned>
atomic_short 	std::atomic<short>
atomic_ushort 	std::atomic<unsigned short>
atomic_long 	std::atomic<long>
atomic_ulong 	std::atomic<unsigned long>
atomic_llong 	std::atomic<long long>
atomic_ullong 	std::atomic<unsigned long long>
atomic_char16_t 	std::atomic<char16_t>
atomic_char32_t 	std::atomic<char32_t>
atomic_wchar_t 	std::atomic<wchar_t>

原子类型定义 		标准库中相关类型定义
atomic_int_least8_t 	int_least8_t
atomic_uint_least8_t 	uint_least8_t
atomic_int_least16_t 	int_least16_t
atomic_uint_least16_t 	uint_least16_t
atomic_int_least32_t 	int_least32_t
atomic_uint_least32_t 	uint_least32_t
atomic_int_least64_t 	int_least64_t
atomic_uint_least64_t 	uint_least64_t
atomic_int_fast8_t 	int_fast8_t
atomic_uint_fast8_t 	uint_fast8_t
atomic_int_fast16_t 	int_fast16_t
atomic_uint_fast16_t 	uint_fast16_t
atomic_int_fast32_t 	int_fast32_t
atomic_uint_fast32_t 	uint_fast32_t
atomic_int_fast64_t 	int_fast64_t
atomic_uint_fast64_t 	uint_fast64_t
atomic_intptr_t 		intptr_t
atomic_uintptr_t 		uintptr_t
atomic_size_t 		size_t
atomic_ptrdiff_t 		ptrdiff_t
atomic_intmax_t	 	intmax_t
atomic_uintmax_t 	uintmax_t

    原子类型不允许由另一个原子类型拷贝赋值，因为拷贝赋值调用了两个对象，破坏了操作的原子性。但可以用对应的内置类型赋值

T operator=(T desired) noexcept;
T operator=(T desired) volatile noexcept;
atomic& operator=(const atomic&) = delete;
atomic& operator=(const atomic&) volatile = delete;

    此外std::atomic为支持赋值提供了成员函数

std::atomic<T>::store // 替换当前值
std::atomic<T>::load // 返回当前值
std::atomic<T>::exchange // 替换值，并返回被替换前的值

// 与期望值比较，不等则将期望值设为原子值并返回false
// 相等则将原子值设为目标值并返回true
// 在缺少CAS（compare-and-exchange）指令的机器上，weak版本在相等时可能替换失败并返回false
// 因此weak版本通常要求循环，而strong版本返回false就能确保不相等
std::atomic<T>::compare_exchange_weak
std::atomic<T>::compare_exchange_strong

std::atomic<T>::fetch_add // 原子加法，返回相加前的值
std::atomic<T>::fetch_sub
std::atomic<T>::fetch_and
std::atomic<T>::fetch_or
std::atomic<T>::fetch_xor
std::atomic<T>::operator++ // 前自增等价于fetch_add(1)+1
std::atomic<T>::operator++(int) // 后自增等价于fetch_add(1)
std::atomic<T>::operator-- // fetch_sub(1)-1
std::atomic<T>::operator--(int) // fetch_sub(1)
std::atomic<T>::operator+= // fetch_add(x)+x
std::atomic<T>::operator-= // fetch_sub(x)-x
std::atomic<T>::operator&= // fetch_and(x)&x
std::atomic<T>::operator|= // fetch_or(x)|x
std::atomic<T>::operator^= // fetch_xor(x)^x

fetch_add	atomically adds the argument to the value stored in the atomic object and obtains the value held previously 
fetch_sub	atomically subtracts the argument from the value stored in the atomic object and obtains the value held previously 
fetch_and	atomically performs bitwise AND between the argument and the value of the atomic object and obtains the value held previously 
fetch_or		atomically performs bitwise OR between the argument and the value of the atomic object and obtains the value held previously 
fetch_xor	atomically performs bitwise XOR between the argument and the value of the atomic object and obtains the value held previously 

Operation				triv	int type	ptr type	Effect
atomic a=val				Yes	Yes	Yes	Initializes a with val (not an atomic operation)
atomic a; atomic_init(&a,val)		Yes	Yes	Yes	Ditto (without atomic_init(), a is not initialized)
a.is_lock_free()				Yes	Yes	Yes	true if type internally does not use locks
a.store(val)				Yes	Yes	Yes	Assigns val (returns void)
a.load()					Yes	Yes	Yes	Returns copy of the value of a
a.exchange(val)				Yes	Yes	Yes	Assigns val and returns copy of old value of a
a.compare_exchange_strong(exp,des)	Yes	Yes	Yes	CAS operation
a.compare_exchange_weak(exp,des)	Yes	Yes	Yes	Weak CAS operation
a = val					Yes	Yes	Yes	Assigns and returns copy of val
a.operator atomic()			Yes	Yes	Yes	Returns copy of the value of a
a.fetch_add(val)					Yes	Yes	
a.fetch_sub(val)					Yes	Yes	
a += val						Yes	Yes	
a  -= val						Yes	Yes	
++a, a++					Yes	Yes	
--a, a--						Yes	Yes	
a.fetch_and(val)					Yes		
a.fetch_or(val)					Yes		
a.fetch_xor(val)					Yes		
a  &= val					Yes		
a  |= val						Yes		
a  ^= val					Yes		

Operation					triv	int type	ptr type
a.store(val,mo)					Yes	Yes	Yes
a.load(mo)					Yes	Yes	Yes
a.exchange(val,mo)				Yes	Yes	Yes
a.compare_exchange_strong(exp,des,mo)		Yes	Yes	Yes
a.compare_exchange_strong(exp,des,mo1,mo2)	Yes	Yes	Yes
a.compare_exchange_weak(exp,des,mo)		Yes	Yes	Yes
a.compare_exchange_weak(exp,des,mo1,mo2)	Yes	Yes	Yes
a.fetch_add(val,mo)					Yes	Yes
a.fetch_sub(val,mo)					Yes	Yes
a.fetch_and(val,mo)					Yes	
a.fetch_or(val,mo)					Yes	
a.fetch_xor(val,mo)					Yes	

    这些成员函数有一个用来指定内存序的参数std::memory_order，后续会解释内存序的含义

enum class memory_order : /*unspecified*/ {
    relaxed, consume, acquire, release, acq_rel, seq_cst
};
inline constexpr memory_order memory_order_relaxed = memory_order::relaxed;
inline constexpr memory_order memory_order_consume = memory_order::consume;
inline constexpr memory_order memory_order_acquire = memory_order::acquire;
inline constexpr memory_order memory_order_release = memory_order::release;
inline constexpr memory_order memory_order_acq_rel = memory_order::acq_rel;
inline constexpr memory_order memory_order_seq_cst = memory_order::seq_cst;

void store(T desired, std::memory_order order = std::memory_order_seq_cst);
// store的顺序参数只能是
// memory_order_relaxed、memory_order_release、memory_order_seq_cst
T load(std::memory_order order = std::memory_order_seq_cst);
// load的顺序参数只能是
// memory_order_relaxed、memory_order_consume、memory_order_acquire、memory_order_seq_cst

std::atomic_flag

    std::atomic_flag是一个原子的布尔类型，也是唯一保证lock-free的原子类型。它只能在set和clear两个状态之间切换，并且初始化时只能为clear，且必须用ATOMIC_FLAG_INIT初始化

std::atomic_flag x = ATOMIC_FLAG_INIT;

x.clear(std::memory_order_release); // 将状态设为clear（false）
// 不能为读操作语义：memory_order_consume、memory_order_acquire、memory_order_acq_rel

bool y = x.test_and_set(); // 将状态设为set（true）且返回之前的值

    用std::atomic_flag实现自旋锁

class spinlock_mutex {
  std::atomic_flag flag = ATOMIC_FLAG_INIT; // 注意不能在构造函数中初始化
 public:
  void lock()
  {
    while (flag.test_and_set(std::memory_order_acquire));
  }
  void unlock()
  {
    flag.clear(std::memory_order_release);
  }
};

spinlock_mutex m;

void f(int n)
{
  for (int i = 0; i < 100; ++i)
  {
    m.lock();
    std::cout << "Output from thread " << n << '\n';
    m.unlock();
  }
}

int main()
{
  std::vector<std::thread> v;
  for (int i = 0; i < 10; ++i) v.emplace_back(f, i);
  for (auto& x : v) x.join();
}

其他原子类型

    std::atomic_flag功能过于局限，甚至无法像布尔类型一样使用，相比之下，std::atomic<bool>更易用。
    std::atomic<bool>不保证lock-free，可以用is_lock_free检验在当前平台上是否lock-free

std::atomic<bool> x(true);
x = false;
bool y = x.load(std::memory_order_acquire); // 读取x值返回给y
x.store(true); // x写为true
y = x.exchange(false, std::memory_order_acq_rel); // x用false替换，并返回旧值true给y
bool expected = false; // 期望值
// 不等则将期望值设为x并返回false，相等则将x设为目标值true并返回true
// weak版本在相等时也可能替换失败而返回false，因此一般用于循环
while (!x.compare_exchange_weak(expected, true) && !expected);
// 当然，对于只有两种值的std::atomic<bool>来说这显得有些繁琐
// 但对其他原子类型来说，这个影响就大了

    指针原子类型std::atomic<T*>也支持is_lock_free、load、store、exchange、compare_exchange_weak和compare_exchange_strong，
    与std::atomic<bool>语义相同，只不过读取和返回的类型是T*而非bool。此外指针原子类型还支持运算操作：fetch_add、fetch_sub、++、--、+=、-=

class A {};
A a[5];
std::atomic<A*> p(a); // p为&a[0]
A* x = p.fetch_add(2); // p为&a[2]，并返回原始值&a[0]
assert(x == a);
assert(p.load() == &a[2]);
x = (p -= 1); // p为&a[1]，并返回给x，相当于x = p.fetch_sub(1) - 1
assert(x == &a[1]);
assert(p.load() == &a[1]);

    整型原子类型（如std::atomic<int>）在上述操作之外还支持fetch_or、fetch_and、fetch_xor、|=、&=、^=

std::atomic<int> i(5);
int j = i.fetch_and(3); // 101 & 011 = 001，i为1，j为5

    如果原子类型是自定义类型，该自定义类型必须可平凡复制（trivially copyable），也就意味着该类型不能有虚函数或虚基类。这可以用is_trivially_copyable检验

class A {
 public:
  virtual void f() {}
};

std::cout << std::is_trivially_copyable_v<A>; // 0
std::atomic<A> a; // 错误：A不满足trivially copyable
std::adtomic<std::vector<int>> v; // 错误
std::atomic<std::string> s; // 错误

    自定义类型的原子类型不允许运算操作，只允许is_lock_free、load、store、exchange、compare_exchange_weak和compare_exchange_strong，以及赋值操作和向自定义类型转换的操作
    除了每个类型各自的成员函数，原子操作库还提供了通用的自由函数，只不过函数名多了一个atomic_前缀，参数变为指针类型

std::atomic<int> i(42);
int j = std::atomic_load(&i); // 等价于i.load()

    除std::atomic_is_lock_free外，每个自由函数有一个_explicit后缀版本，_explicit自由函数额外接受一个std::memory_order参数

std::atomic<int> i(42);
std::atomic_load_explicit(&i, std::memory_order_acquire); // i.load(std::memory_order_acquire)

    自由函数的设计主要考虑的是C语言没有引用而只能使用指针，compare_exchange_weak和compare_exchange_strong的第一个参数是引用，
    因此std::atomic_compare_exchange_weak、std::atomic_compare_exchange_strong的参数用的是指针

bool compare_exchange_weak(T& expected, T desired,
  std::memory_order success, 
  std::memory_order failure);

template<class T>
bool atomic_compare_exchange_weak(std::atomic<T>* obj, 
  typename std::atomic<T>::value_type* expected,
  typename std::atomic<T>::value_type desired);

template<class T>
bool atomic_compare_exchange_weak_explicit(std::atomic<T>* obj,
  typename std::atomic<T>::value_type* expected, 
  typename std::atomic<T>::value_type desired,
  std::memory_order succ, 
  std::memory_order fail);

    std::atomic_flag对应的自由函数的前缀不是atomic_而是atomic_flag_，但接受std::memory_order参数的版本一样是_explicit后缀

std::atomic_flag x =  ATOMIC_FLAG_INIT;
bool y = std::atomic_flag_test_and_set_explicit(&x, std::memory_order_acquire);
std::atomic_flag_clear_explicit(&x, std::memory_order_release);

    自由函数不仅可用于原子类型，还为std::shared_ptr提供了特化版本

std::shared_ptr<int> p(new int(42));
std::shared_ptr<int> x = std::atomic_load(&p);
std::shared_ptr<int> q;
std::atomic_store(&q, p);

    这些特化将在C++20中弃用，C++20直接允许std::atomic的模板参数为std::shared_ptr

std::atomic<std::shared_ptr<int>> x; // C++20

同步操作和强制排序（enforced ordering）

    两个线程分别读写数据，为了避免竞争，设置一个标记

std::vector<int> data;
std::atomic<bool> data_ready(false);

void read_thread()
{
  while (!data_ready.load()) // 1：happens-before 2
  {
    std::this_thread::sleep_for(std::chrono::milliseconds(1));
  }
  std::cout << data[0]; // 2
}

void write_thread()
{
  data.push_back(42); // 3：happens-before 4
  data_ready = true; // 4：inter-thread happens-before 1
}

    std::atomic<bool>上的操作要求强制排序，该顺序由内存模型关系happens-before和synchronizes-with提供
    happens-before保证了1在2之前发生，3在4之前发生，而1要求4，所以4在1之前发生，最终顺序确定为3412

    如果没有强制排序，CPU可能会调整指令顺序，如果顺序是4123，读操作就会因为越界而出错

synchronizes-with

    synchronizes-with关系只存在于原子类型操作上，如果一个数据结构包含原子类型，这个数据结构上的操作（比如加锁）也可能提供synchronizes-with关系
    变量x上，标记了内存序的原子写操作W，和标记了内存序的原子读操作，如果两者存在synchronizes-with关系，
    表示读操作读取的是：W写入的值，或W之后同一线程上原子写操作写入x的值，或任意线程上对x的一系列原子读改写操作（比如fetch_add、compare_exchange_weak）的值
    简单来说，如果线程A写入一个值，线程B读取该值，则A synchronizes-with B

happens-before

    happens-before和strongly-happens-before关系是程序操作顺序的基本构建块，它指定某个操作可以看到其他操作的结果。
    对单线程来说很简单，如果一个操作在另一个之前，就可以说前一个操作happens-before（且strongly-happens-before）后一个操作
    如果操作发生在同一语句中，一般不存在happens-before关系，因为它们是无序的

void f(int x, int y)
{
  std::cout << x << y;
}

int g()
{
  static int i = 0;
  return ++i;
}

int main()
{
  f(g(), g()); // 无序调用g，可能是21也可能是12
  // 一般C++默认使用__cdecl调用模式，参数从右往左入栈，这里就是21
}

    前一条语句中的所有操作都happens-before下一条语句中的所有操作

inter-thread happens-before

    如果一个线程中的操作A happens-before另一个线程中的操作B，则A inter-thread happens-before B
    A inter-thread happens-before B包括以下情况
        A synchronizes-with B
        A dependency-ordered-before B
        A inter-thread happens-before X，X inter-thread happens-before B
        A sequenced-before X，X inter-thread happens-before B
        A synchronizes-with X，X sequenced-before B

strongly-happens-before

    strongly-happens-before关系大多数情况下和happens-before一样，A strongly-happens-before B包括以下情况
        A synchronizes-with B
        A sequenced-before X，X inter-thread happens-before B
        A strongly-happens-before X，X strongly-happens-before B
    略微不同的是，inter-thread happens-before关系可以用memory_order_consume标记，而strongly-happens-before不行。
    但大多数代码不应该使用memory_order_consume，所以这点实际上影响不大

std::memory_order

typedef enum memory_order {
  memory_order_relaxed, // 无同步或顺序限制，只保证当前操作原子性
  memory_order_consume, // 标记读操作，依赖于该值的读写不能重排到此操作前
  memory_order_acquire, // 标记读操作，之后的读写不能重排到此操作前
  memory_order_release, // 标记写操作，之前的读写不能重排到此操作后
  memory_order_acq_rel, // 仅标记读改写操作，读操作相当于acquire，写操作相当于release
  memory_order_seq_cst // sequential consistency：顺序一致性，不允许重排，所有原子操作的默认选项
} memory_order;

Relaxed ordering

    标记为memory_order_relaxed的原子操作不是同步操作，不强制要求并发内存的访问顺序，只保证原子性和修改顺序一致性

std::atomic<int> x = 0;
std::atomic<int> y = 0;

// 线程1
i = y.load(std::memory_order_relaxed); // 1
x.store(i, std::memory_order_relaxed); // 2

// 线程2
j = x.load(std::memory_order_relaxed); // 3
y.store(42, std::memory_order_relaxed); // 4

// 可能执行顺序为4123，结果i == 42, j == 42

    Relaxed ordering不允许循环依赖

std::atomic<int> x = 0;
std::atomic<int> y = 0;

// 线程1
i = y.load(std::memory_order_relaxed); // 1
if (i == 42) x.store(i, std::memory_order_relaxed); // 2

// 线程2
j = x.load(std::memory_order_relaxed); // 3
if (j == 42) y.store(42, std::memory_order_relaxed); // 4

// 结果不允许为i == 42, j == 42，因为要产生这个结果，1依赖4，4依赖3，3依赖2，2依赖1

    典型使用场景是自增计数器，比如std::shared_ptr的引用计数器，它只要求原子性，不要求顺序和同步

std::atomic<int> x = 0;

void f()
{
  for (int i = 0; i < 1000; ++i)
  {
    x.fetch_add(1, std::memory_order_relaxed);
  }
}

int main()
{
  std::vector<std::thread> v;
  for (int i = 0; i < 10; ++i) v.emplace_back(f);
  for (auto& x : v) x.join();
  std::cout << x; // 10000
}

Release-Consume ordering

    对于标记为memory_order_consume原子变量x的读操作R，当前线程中依赖于x的读写不允许重排到R之前，其他线程中对依赖于x的变量写操作对当前线程可见
    如果线程A对一个原子变量x的写操作为memory_order_release，线程B对同一原子变量的读操作为memory_order_consume，带来的副作用是，
    线程A中所有dependency-ordered-before该写操作的其他写操作（non-atomic和relaxed atomic），在线程B的其他依赖于该变量的读操作中可见
    典型使用场景是访问很少进行写操作的数据结构（比如路由表），以及以指针为中介的publisher-subscriber场景，即生产者发布一个指针给消费者访问信息，
    但生产者写入内存的其他内容不需要对消费者可见，这个场景的一个例子是RCU（Read-Copy Update）。该顺序的规范正在修订中，并且暂时不鼓励使用memory_order_consume

std::atomic<int*> x;
int i;

void producer()
{
  int* p = new int(42);
  i = 42;
  x.store(p, std::memory_order_release);
}

void consumer()
{
  int* q;
  while (!(q = x.load(std::memory_order_consume)));
  assert(*q == 42); // 一定不出错：*q带有x的依赖
  assert(i == 42); // 可能出错也可能不出错：i不依赖于x
}

int main()
{
  std::thread t1(producer);
  std::thread t2(consumer);
  t1.join();
  t2.join();
}

Release-Acquire ordering

    对于标记为memory_order_acquire的读操作R，当前线程的其他读写操作不允许重排到R之前，其他线程中在同一原子变量上所有的写操作在当前线程可见
    如果线程A对一个原子变量的写操作W为memory_order_release，线程B对同一原子变量的读操作为memory_order_acquire，带来的副作用是，
    线程A中所有happens-before W的写操作（non-atomic和relaxed atomic）都在线程B中可见
    典型使用场景是互斥锁，线程A的释放后被线程B获取，则A中释放锁之前发生在critical section的所有内容都在B中可见

std::atomic<int*> x;
int i;

void producer()
{
  int* p = new int(42);
  i = 42;
  x.store(p, std::memory_order_release);
}

void consumer()
{
  int* q;
  while (!(q = x.load(std::memory_order_acquire)));
  assert(*q == 42); // 一定不出错
  assert(i == 42); // 一定不出错
}

int main()
{
  std::thread t1(producer);
  std::thread t2(consumer);
  t1.join();
  t2.join();
}

    对于标记为memory_order_release的写操作W，当前线程中的其他读写操作不允许重排到W之后，
    若其他线程acquire该原子变量，则当前线程所有happens-before的写操作在其他线程中可见，
    若其他线程consume该原子变量，则当前线程所有dependency-ordered-before W的其他写操作在其他线程中可见
    对于标记为memory_order_acq_rel的读改写（read-modify-write）操作，相当于写操作是memory_order_release，读操作是memory_order_acquire，
    当前线程的读写不允许重排到这个写操作之前或之后，其他线程中release该原子变量的写操作在修改前可见，并且此修改对其他acquire该原子变量的线程可见
    Release-Acquire ordering并不表示total ordering

std::atomic<bool> x = false;
std::atomic<bool> y = false;
std::atomic<int> z = 0;

void write_x()
{
  x.store(true, std::memory_order_release); // 1：happens-before 3（由于3的循环）
}

void write_y()
{
  y.store(true, std::memory_order_release); // 2 happens-before 5（由于5的循环）
}

void read_x_then_y()
{
  while (!x.load(std::memory_order_acquire)); // 3 happens-before 4
  if (y.load(std::memory_order_acquire)) ++z; // 4
}

void read_y_then_x()
{
  while (!y.load(std::memory_order_acquire)); // 5 happens-before 6
  if (x.load(std::memory_order_acquire)) ++z; // 6
}

int main()
{
  std::thread t1(write_x);
  std::thread t2(write_y);
  std::thread t3(read_x_then_y);
  std::thread t4(read_y_then_x);
  t1.join();
  t2.join();
  t3.join();
  t4.join();
  assert(z.load() != 0); // z可能为0：134y为false 256x为false，但12之间没有关系
}

    为了使两个写操作有序，将其放到一个线程里

std::atomic<bool> x = false;
std::atomic<bool> y = false;
std::atomic<int> z = 0;

void write_x_then_y()
{
  x.store(true, std::memory_order_relaxed); // 1：happens-before 2
  y.store(true, std::memory_order_release); // 2：happens-before 3（由于3的循环）
}

void read_y_then_x()
{
  while (!y.load(std::memory_order_acquire)); // 3：happens-before 4
  if (x.load(std::memory_order_relaxed)) ++z; // 4
}

int main()
{
  std::thread t1(write_x_then_y);
  std::thread t2(read_y_then_x);
  t1.join();
  t2.join();
  assert(z.load() != 0); // z一定不为0：顺序一定为1234
}

    利用Release-Acquire ordering可以传递同步

std::atomic<bool> x = false;
std::atomic<bool> y = false;
std::atomic<int> v[2];

void f()
{
  // v[0]、v[1]的设置没有先后顺序，但都happens-before 1
  v[0].store(1, std::memory_order_relaxed);
  v[1].store(2, std::memory_order_relaxed);
  x.store(true, std::memory_order_release); // 1：happens-before 2（由于2的循环）
}

void g()
{
  while (!x.load(std::memory_order_acquire)); // 2：happens-before 3
  y.store(true, std::memory_order_release); // 3：happens-before 4（由于4的循环）
}

void h()
{
  while (!y.load(std::memory_order_acquire)); // 4 happens-before v[0]、v[1]的读取
  assert(v[0].load(std::memory_order_relaxed) == 1);
  assert(v[1].load(std::memory_order_relaxed) == 2);
}

    使用读改写操作可以将上面的两个标记合并为一个

std::atomic<int> x = 0;
std::atomic<int> v[2];

void f()
{
  v[0].store(1, std::memory_order_relaxed);
  v[1].store(2, std::memory_order_relaxed);
  x.store(1, std::memory_order_release); // 1：happens-before 2（由于2的循环）
}

void g()
{
  int i = 1;
  while (!x.compare_exchange_strong(i, 2, std::memory_order_acq_rel)) // 2：happens-before 3（由于3的循环）
  { // x为1时，将x替换为2，返回true；x为0时，将i替换为x，返回false
    i = 1; // 返回false时，x未被替换，i被替换为0，因此将i重新设为1
  }
}

void h()
{
  while (x.load(std::memory_order_acquire) < 2); // 3
  assert(v[0].load(std::memory_order_relaxed) == 1);
  assert(v[1].load(std::memory_order_relaxed) == 2);
}

Sequentially-consistent ordering

    memory_order_seq_cst是所有原子操作的默认选项（因此可以省略不写），对于标记为memory_order_seq_cst的操作，
    读操作相当于memory_order_acquire，写操作相当于memory_order_release，读改写操作相当于memory_order_acq_rel，
    此外还附加一个单独的total ordering，即所有线程对同一操作看到的顺序也是相同的。这是最简单直观的顺序，但由于要求全局的线程同步，因此也是开销最大的

std::atomic<bool> x = false;
std::atomic<bool> y = false;
std::atomic<int> z = 0;

// 要么1 happens-before 2，要么2 happens-before 1
void write_x()
{
  x.store(true, std::memory_order_seq_cst); // 1：happens-before 3（由于3的循环）
}

void write_y()
{
  y.store(true, std::memory_order_seq_cst); // 2：happens-before 5（由于5的循环）
}

void read_x_then_y()
{
  while (!x.load(std::memory_order_seq_cst)); // 3：happens-before 4
  if (y.load(std::memory_order_seq_cst)) ++z; // 4：如果返回false则一定是1 happens-before 2
}

void read_y_then_x()
{
  while (!y.load(std::memory_order_seq_cst)); // 5：happens-before 6
  if (x.load(std::memory_order_seq_cst)) ++z; // 6：如果返回false则一定是2 happens-before 1
}

int main()
{
  std::thread t1(write_x);
  std::thread t2(write_y);
  std::thread t3(read_x_then_y);
  std::thread t4(read_y_then_x);
  t1.join();
  t2.join();
  t3.join();
  t4.join();
  assert(z.load() != 0); // z可能为1、2，一定不为0：12之间必定存在happens-before关系
}

std::atomic_thread_fence

std::atomic<bool> x, y;
std::atomic<int> z;

void f()
{
  x.store(true, std::memory_order_relaxed); // 1：happens-before 2
  std::atomic_thread_fence(std::memory_order_release); // 2：synchronizes-with 3
  y.store(true, std::memory_order_relaxed);
}

void g()
{
  while (!y.load(std::memory_order_relaxed));
  std::atomic_thread_fence(std::memory_order_acquire); // 3：happens-before 4
  if (x.load(std::memory_order_relaxed)) ++z; // 4
}

int main()
{
  x = false;
  y = false;
  z = 0;
  std::thread t1(f);
  std::thread t2(g);
  t1.join();
  t2.join();
  assert(z.load() != 0); // 1 happens-before 4
}

    将x替换为非原子bool类型，行为也一样

bool x = false;
std::atomic<bool> y;
std::atomic<int> z;

void f()
{
  x = true; // 1：happens-before 2
  std::atomic_thread_fence(std::memory_order_release); // 2：synchronizes-with 3
  y.store(true, std::memory_order_relaxed);
}

void g()
{
  while (!y.load(std::memory_order_relaxed));
  std::atomic_thread_fence(std::memory_order_acquire); // 3：happens-before 4
  if (x) ++z; // 4
}

int main()
{
  x = false;
  y = false;
  z = 0;
  std::thread t1(f);
  std::thread t2(g);
  t1.join();
  t2.join();
  assert(z.load() != 0); // 1 happens-before 4
}

05 基于锁的并发数据结构的设计

设计并发数据结构的建议

    设计并发数据结构要考虑两点，一是确保访问thread-safe，二是允许真正的并发访问。thread-safe基本要求如下
        数据结构的不变量（invariant）被一个线程破坏时，确保不被线程看到此状态
        提供操作完整的函数来避免数据结构接口中固有的race condition
        注意数据结构出现异常时的行为，以确保不变量不被破坏
        限制锁的范围，避免可能的嵌套锁，最小化死锁的概率
    关于真正的并发访问没有太多建议，但作为数据结构的设计者需要考虑以下问题
        部分操作是否可以在锁的范围外执行
        数据结构的不同部分是否被不同的mutex保护
        是否所有操作需要同级别的保护
        在不影响操作语义的前提下，能否对数据结构做一个简单的修改提高并发访问的概率
    这些问题可以总结为一点，即如何最小化线程对被mutex保护的数据的轮流访问，以及最大化真实的并发量

使用锁实现thread-safe stack

    pop直接返回弹出值来避免pop和top的潜在竞争

#include <exception>
#include <memory>
#include <mutex>
#include <stack>

struct emptyStack : std::exception
{
  const char* what() const noexcept
  {
    return "empty stack!";
  }
};

template<typename T>
class A {
  std::stack<T> s;
  mutable std::mutex m;
 public:
  A() : s(std::stack<T>()) {}
  
  A(const A& rhs)
  {
    std::lock_guard<std::mutex> l(rhs.m);
    s = rhs.s;
  }
  
  A& operator=(const A&) = delete;
  
  void push(T n)
  {
    std::lock_guard<std::mutex> l(m);
    s.push(std::move(n));
  }
  
  std::shared_ptr<T> pop() // 返回一个指向栈顶元素的指针
  {
    std::lock_guard<std::mutex> l(m);
    if (s.empty()) throw emptyStack();
    const std::shared_ptr<T> res(std::make_shared<T>(std::move(s.top())));
    s.pop();
    return res;
  }
  
  void pop(T& n) // 传引用获取结果
  {
    std::lock_guard<std::mutex> l(m);
    if (s.empty()) throw emptyStack();
    n = std::move(s.top());
    s.pop();
  }
  
  bool empty() const
  {
    std::lock_guard<std::mutex> l(m);
    return s.empty();
  }
};

使用锁和条件变量实现thread-safe queue

    合并front和pop

#include <memory>
#include <mutex>
#include <condition_variable>
#include <queue>

template<typename T>
class A {
  mutable std::mutex m;
  std::queue<T> q;
  std::condition_variable cv;
 public:
  A() {}
  A(const A& rhs)
  {
    std::lock_guard<std::mutex> l(rhs.m);
    q = rhs.q;
  }
  
  void push(T x)
  {
    std::lock_guard<std::mutex> l(m);
    q.push(std::move(x));
    cv.notify_one();
  }
  
  void wait_and_pop(T& x)
  {
    std::unique_lock<std::mutex> l(m);
    // 在queue中有元素时才返回，而不必持续调用empty
    cv.wait(l, [this] { return !q.empty(); });
    x = std::move(q.front());
    q.pop();
  }
  
  std::shared_ptr<T> wait_and_pop()
  {
    std::unique_lock<std::mutex> l(m);
    cv.wait(l, [this] { return !q.empty(); });
    std::shared_ptr<T> res(std::make_shared<T>(std::move(q.front())));
    q.pop();
    return res;
  }
  
  bool try_pop(T& x)
  {
    std::lock_guard<std::mutex> l(m);
    if (q.empty()) return false;
    x = std::move(q.front());
    q.pop();
    return true;
  }
  
  std::shared_ptr<T> try_pop()
  {
    std::lock_guard<std::mutex> l(m);
    if (q.empty()) return std::shared_ptr<T>();
    std::shared_ptr<T> res(std::make_shared<T>(std::move(q.front())));
    q.pop();
    return res;
  }
  
  bool empty() const
  {
    std::lock_guard<std::mutex> l(m);
    // 其他线程可能有此对象（拷贝构造）所以要上锁
    return q.empty();
  }
};

    这引入了一个异常安全问题，多个线程处于等待时，notify_one只会唤醒一个线程，
    如果这个线程在wait_and_pop中（比如构造std::shared_ptr对象时就可能）抛出异常，其余线程将永远不会唤醒
    如果不希望出现这种情况，应该把notify_one改为notify_all，这样就会唤醒所有线程，代价是大部分线程发现queue仍为empty时，又会继续休眠
    第二种方案是抛出异常时让wait_and_pop调用notify_one，从而唤醒另一个线程
    第三种方案是将std::shared_ptr的初始化移到push中，并且内部的std::queue不直接存储值，而是存储管理值的std::shared_ptr

#include <memory>
#include <mutex>
#include <condition_variable>
#include <queue>

template<typename T>
class A {
  mutable std::mutex m;
  std::queue<std::shared_ptr<T>> q; // 之前为std::queue<T> q
  std::condition_variable cv;
 public:
  A() {}
  A(const A& rhs)
  {
    std::lock_guard<std::mutex> l(rhs.m);
    q = rhs.q;
  }
  
  void push(T x)
  {
    std::shared_ptr<T> data(std::make_shared<T>(std::move(x)));
    // 上面的构造在锁外完成，之前只能在pop中且持有锁时完成
    // 内存分配操作开销很大，这种做法减少了mutex的持有时间，提升了性能
    std::lock_guard<std::mutex> l(m);
    q.push(data); // 之前为q.push(std::move(x))
    cv.notify_one();
  }
  
  void wait_and_pop(T& x)
  {
    std::unique_lock<std::mutex> l(m);
    cv.wait(l, [this] { return !q.empty(); });
    x = std::move(*q.front()); // 之前为std::move(q.front())
    q.pop();
  }
  
  std::shared_ptr<T> wait_and_pop()
  {
    std::unique_lock<std::mutex> l(m);
    cv.wait(l, [this] { return !q.empty(); });
    std::shared_ptr<T> res = q.front();
    // 之前为std::make_shared<T>(std::move(q.front()))
    // 现在构造转移到了push中
    q.pop();
    return res;
  }
  
  bool try_pop(T& x)
  {
    std::lock_guard<std::mutex> l(m);
    if (q.empty()) return false;
    x = std::move(*q.front()); // 之前为std::move(q.front())
    q.pop();
    return true;
  }
  
  std::shared_ptr<T> try_pop()
  {
    std::lock_guard<std::mutex> l(m);
    if (q.empty()) return std::shared_ptr<T>();
    std::shared_ptr<T> res = q.front();
    // 之前为std::make_shared<T>(std::move(q.front()))
    q.pop();
    return res;
  }
  
  bool empty() const
  {
    std::lock_guard<std::mutex> l(m);
    return q.empty();
  }
};

使用细粒度（fine-grained）锁和条件变量实现thread-safe queue

    和thread-safe stack一样，使用mutex保护了整个数据结构，但限制了对并发的支持。多线程在各种成员函数中被阻塞，而只有一个线程能在同一时间做任何事。
    不过这种限制主要是因为内部实现使用的是std::queue，为了支持更高级别的并发就需要提供更细粒度的锁，为了提供更细粒度的锁就要换一种实现方式
    最简单的queue实现方式是包含头尾指针的单链表

#include <memory>

template<typename T>
class A {
  struct node {
    T val;
    std::unique_ptr<node> next;
    node(T x) : val(std::move(x)) {}
  };
  std::unique_ptr<node> head;
  node* tail { nullptr };
 public:
  A() {}
  A(const A&) = delete;
  A& operator=(const A&) = delete;
  void push(T x)
  {
    std::unique_ptr<node> p(new node(std::move(x))); // 新建一个值为x的节点
    node* const oldHead = p.get(); // 获取新节点的原始指针
    if (tail)
    { // 2：如果尾节点不为空则next设为新节点
      tail->next = std::move(p);
    }
    else
    { // 如果尾节点为空（说明无元素）则头节点设为新节点
      head = std::move(p);
    }
    tail = oldHead; // tail设为新节点的原始指针
  }
  
  std::shared_ptr<T> try_pop()
  {
    if (!head) return std::shared_ptr<T>(); // 头节点为空则返回空指针
    std::shared_ptr<T> res(std::make_shared<T>(std::move(head->val))); // 保存头节点值
    std::unique_ptr<node> oldHead = std::move(head); // 获取头节点
    head = std::move(oldHead->next); // 1：头节点指向下一个节点
    return res; // 返回之前保存的头节点的值
  }
};

    这是一个单线程下没问题的queue，但在多线程下就有明显问题，即使用两个mutex分别保护头尾指针。push可以同时修改头尾指针，会对两个mutex上锁，
    更严重的是push和try_pop都能访问next节点（见注释1和2处），仅有一个元素时头尾指针相等，两处的next也是同一对象，
    于是try_pop读next与push写next就产生了竞争，锁的也是同一个mutex
    这个问题可以通过在queue的最后预设一个dummy node解决

#include <memory>

template<typename T>
class A {
  struct node {
    std::shared_ptr<T> val; // 之前为T val
    std::unique_ptr<node> next;
  };
  std::unique_ptr<node> head;
  node* tail;
 public:
  A() : head(new node), tail(head.get()) {} // 预设头尾节点指向一处（未存储任何值）
  A(const A&) = delete;
  A& operator=(const A&) = delete;
  void push(T x)
  { // 现在push只访问tail而不访问head，意味着不再会与try_pop操作同一节点而争夺锁
    std::shared_ptr<T> newVal(std::make_shared<T>(std::move(x))); // 新建一个保存新值的指针
    std::unique_ptr<node> p(new node); // 新建一个不存储值的新节点（用作新的尾节点）
    node* const newTail = p.get(); // 获取新节点的原始指针
    tail->val = newVal; // 当前尾节点的值设为新值（原本不存储值）
    tail->next = std::move(p); // tail->next设为新节点
    tail = newTail; // 最后令tail指向新节点
  }
  
  std::shared_ptr<T> try_pop()
  { // 同时访问head和tail只在最初的比较上，锁是短暂的
    // 之前的判断条件为if (!head)，现在为头节点与尾节点指向一处
    if (head.get() == tail) return std::shared_ptr<T>(); // 返回空指针
    std::shared_ptr<T> res(head->val); // 保存头节点的值，现在为std::shared_ptr类型
    std::unique_ptr<node> oldHead = std::move(head); // 获取旧的头节点
    head = std::move(oldHead->next); // 头节点指向下一个节点
    return res; // 返回之前保存的头节点的值
  }
};

    接着加上锁，锁的范围应该尽可能小。对于push来说很简单，对tail的访问上锁即可。对于try_pop来说，在弹出head（即将head设为next）之前都应该对head加锁，在最后返回时则不需要锁，
    因此可以把加锁的部分提取到一个新函数中。在比较head与tail时，对tail也存在短暂的访问，因此对tail的访问也需要加锁

#include <memory>
#include <mutex>

template<typename T>
class A {
  struct node {
    std::shared_ptr<T> val;
    std::unique_ptr<node> next;
  };
  std::unique_ptr<node> head;
  node* tail;
  std::mutex hm; // head mutex
  std::mutex tm; // tail mutex
  
  node* get_tail()
  {
    std::lock_guard<std::mutex> l(tm);
    return tail;
  }
  
  std::unique_ptr<node> pop_head()
  {
    std::lock_guard<std::mutex> l(hm);
    if (head.get() == get_tail()) return nullptr;
    std::unique_ptr<node> oldHead = std::move(head);
    head = std::move(oldHead->next);
    return oldHead;
  }
 public:
  A() : head(new node), tail(head.get()) {}
  A(const A&) = delete;
  A& operator=(const A&) = delete;
  // push有两种可能产生异常的情况：一是给mutex上锁，但数据在上锁成功后才会修改
  // 二是构建智能指针对象时可能抛出异常，但智能指针本身是异常安全的，异常时会释放
  void push(T x) // 因此push是异常安全的
  {
    std::shared_ptr<T> newVal(std::make_shared<T>(std::move(x)));
    std::unique_ptr<node> p(new node);
    node* const newTail = p.get();
    std::lock_guard<std::mutex> l(tm); // 此处加锁
    tail->val = newVal;
    tail->next = std::move(p);
    tail = newTail;
  }
  // 同理try_pop也是异常安全的
  std::shared_ptr<T> try_pop()
  {
    std::unique_ptr<node> oldHead = pop_head();
    return oldHead ? oldHead->val : std::shared_ptr<T>();
  }
};

    注意在try_pop中，tail mutex要在head mutex之中加锁

std::unique_ptr<node> pop_head()
{
  node* const oldTail = get_tail(); // 把tail mutex移到head mutex之外是有问题的
  // 这两行代码之间，head和tail都可能被改变
  std::lock_guard<std::mutex> l(hm);
  if (head.get() == oldTail) // 比较的是最新的head和最老的tail
  ...
}

    这个实现在并发度上也比最初版本的queue要好。由于使用了细粒度锁，push中创建新值和新节点都没上锁，多线程并发创建新值和新节点就不是问题。
    同一时间内，只有一个线程能添加新节点，但这只需要一个指针赋值操作，持有锁的时间很短
    try_pop中对tail mutex的持有时间也很短，只是用来做一次比较，因此try_pop和push几乎可以同时调用。
    try_pop中持有head mutex所做的也只是指针赋值操作，开销较大的析构操作在锁外进行（智能指针的析构特性）。
    这意味着同一时间内，虽然只有一个线程能调用pop_head，但允许多个线程删除节点并安全返回数据，这就提升了try_pop的并发调用数量
    接着还需要实现wait_and_pop，下面是最终版本的thread-safe queue

#include <memory>
#include <mutex>
#include <condition_variable>

template<typename T>
class A {
  struct node {
    std::shared_ptr<T> val;
    std::unique_ptr<node> next;
  };
  std::unique_ptr<node> head;
  node* tail;
  std::mutex hm; // head mutex
  std::mutex tm; // tail mutex
  std::condition_variable cv;
  
  node* get_tail()
  {
    std::lock_guard<std::mutex> l(tm);
    return tail;
  }
  
  std::unique_ptr<node> pop_head()
  {
    std::unique_ptr<node> oldHead = std::move(head);
    head = std::move(oldHead->next);
    return oldHead;
  }
  
  std::unique_lock<std::mutex> wait_for_data()
  {
    std::unique_lock<std::mutex> l(hm);
    cv.wait(l, [&] { return head.get() != get_tail(); });
    return std::move(l);
  }
  
  std::unique_ptr<node> wait_pop_head()
  {
    std::unique_lock<std::mutex> l(wait_for_data());
    return pop_head();
  }
  
  std::unique_ptr<node> wait_pop_head(T& x)
  {
    std::unique_lock<std::mutex> l(wait_for_data());
    x = std::move(*head->val); // 即*(head->val)，->优先级高于*
    return pop_head();
  }
  
  std::unique_ptr<node> try_pop_head()
  {
    std::lock_guard<std::mutex> l(hm);
    if (head.get() == get_tail()) return std::unique_ptr<node>();
    return pop_head();
  }
  
  std::unique_ptr<node> try_pop_head(T& x)
  {
    std::lock_guard<std::mutex> l(hm);
    if (head.get() == get_tail()) return std::unique_ptr<node>();
    x = std::move(*head->val);
    return pop_head();
  }
 public:
  A() : head(new node), tail(head.get()) {}
  A(const A&) = delete;
  A& operator=(const A&) = delete;
  void push(T x)
  {
    std::shared_ptr<T> newVal(std::make_shared<T>(std::move(x)));
    std::unique_ptr<node> p(new node);
    {
      std::lock_guard<std::mutex> l(tm);
      tail->val = newVal;
      node* const newTail = p.get();
      tail->next = std::move(p);
      tail = newTail;
    }
    cv.notify_one();
  }
  
  std::shared_ptr<T> wait_and_pop()
  {
    std::unique_ptr<node> oldHead = wait_pop_head();
    return oldHead->val;
  }
  
  void wait_and_pop(T& x)
  {
    std::unique_ptr<node> oldHead = wait_pop_head(x);
  }
  
  std::shared_ptr<T> try_pop()
  {
    std::unique_ptr<node> oldHead = try_pop_head();
    return oldHead ? oldHead->val : std::shared_ptr<T>();
  }
  
  bool try_pop(T& x)
  {
    std::unique_ptr<node> oldHead = try_pop_head(x);
    return oldHead;
  }
  
  bool empty()
  {
    std::lock_guard<std::mutex> l(hm);
    return head.get() == get_tail();
  }
};

    这个实现是之后实现lock-free queue的基础。它是一个unbounded queue，只要内存足够，线程就能持续push新值。
    而bounded queue会限定元素数量的最大值，当queue填满时，push会失败或者阻塞至pop一个元素，这对分配线程工作是有用的。
    将unbounded queue扩展成bounded queue不是难事，这里不再赘述

使用锁实现thread-safe lookup table

    lookup table就是通过key查询value的数据结构，比如std::map，但标准库关联容器的接口同样不适合并发访问，最大的问题在于迭代器，
    其他线程删除元素时导致迭代器失效，因此lookup table的接口设计就要跳过迭代器
    为了使用细粒度锁，就不应该使用标准库容器。可选的关联容器数据结构有三种，一是二叉树（如红黑树），但每次查找修改都要从访问根节点开始，也就表示根节点需要上锁，
    尽管沿着树向下访问节点时会解锁，但这个比起覆盖整个数据结构的单个锁好不了多少
    第二种方式是有序数组，这比二叉树还差，因为无法提前得知一个给定的值应该放在哪，于是同样需要一个覆盖整个数组的锁
    第三种方式是哈希表。假如有一个固定数量的桶，一个key属于哪个桶就取决于key的属性和哈希函数，这意味着可以安全地分开锁住每个桶。
    如果复用一个支持多个读单个写的mutex，就能将并发度提高相当于桶数量的倍数

#include <memory>
#include <mutex>
#include <shared_mutex>
#include <map>
#include <list>
#include <utility> // for std::pair
#include <algorithm> // for std::find_if
#include <functional> // for std::hash

template<typename K, typename V, typename Hash = std::hash<K>>
class A {
  class Bucket {
   public:
    std::list<std::pair<K, V>> data;
    mutable std::shared_mutex m; // 每个桶都用这个锁保护

    V value_for(const K& k, const V& v) const // 如果未找到则返回v
    { // 没有修改任何值，异常安全
      std::shared_lock<std::shared_mutex> l(m); // 只读锁，可共享
      auto it = std::find_if(data.begin(), data.end(), [&](auto& x) { return x.first == k; });
      return it == data.end() ? v : it->second;
    }
    
    void add_or_update_mapping(const K& k, const V& v) // 找到则修改，未找到则添加
    {
      std::unique_lock<std::shared_mutex> l(m); // 写，单独占用
      auto it = std::find_if(data.begin(), data.end(), [&](auto& x) { return x.first == k; });
      if (it == data.end())
      {
        data.emplace_back(k, v); // emplace_back异常安全
      }
      else
      {
        it->second = v; // 赋值可能抛异常，但值是用户提供的，可放心让用户处理
      }
    }
    
    void remove_mapping(const K& k)
    { // std::list::erase不会抛异常，因此异常安全
      std::unique_lock<std::shared_mutex> l(m); // 写，单独占用
      auto it = std::find_if(data.begin(), data.end(), [&](auto& x) { return x.first == k; });
      if (it != data.end()) data.erase(it);
    }
  };
  
  std::vector<std::unique_ptr<Bucket>> buckets;
  Hash hasher;
  Bucket& get_bucket(const K& k) const
  { // 桶数固定因此可以无锁调用
    return *buckets[hasher(k) % buckets.size()];
  }
 public:
  // 桶的默认数量为19（一般用x%桶数决定放置x的桶的索引，桶数为质数可以使得桶分布均匀）
  A(unsigned n = 19, const Hash& h = Hash{}) : buckets(n), hasher(h)
  {
    for (auto& x : buckets) x.reset(new Bucket);
  }
  A(const A&) = delete;
  A& operator=(const A&) = delete;
  V value_for(const K& k, const V& v = V{}) const
  {
    return get_bucket(k).value_for(k, v);
  }
  
  void add_or_update_mapping(const K& k, const V& v)
  {
    get_bucket(k).add_or_update_mapping(k, v);
  }
  
  void remove_mapping(const K& k)
  {
    get_bucket(k).remove_mapping(k);
  }
  // 为了方便使用，提供一个到std::map的映射
  std::map<K, V> get_map() const
  {
    std::vector<std::unique_lock<std::shared_mutex>> l;
    for (auto& x : buckets)
    {
      l.push_back(std::unique_lock<std::shared_mutex>(x->m));
    }
    std::map<K, V> res;
    for (auto& x : buckets)
    {
      for (auto& y : x->data) res.insert(y);
    }
    return res;
  }
};

使用锁实现thread-safe list

#include <memory>
#include <mutex>

template<typename T>
class A {
  struct node {
    std::mutex m;
    std::shared_ptr<T> data;
    std::unique_ptr<node> next;
    node() : next() {}
    node(const T& x) : data(std::make_shared<T>(x)) {}
  };
  node head;
 public:
  A() {}
  ~A() { remove_if([](const node&) { return true; }); }
  A(const A&) = delete;
  A& operator=(const A&) = delete;
  void push_front(const T& x)
  {
    std::unique_ptr<node> newNode(new node(x));
    std::lock_guard<std::mutex> l(head.m);
    newNode->next = std::move(head.next);
    head.next = std::move(newNode);
  }
  
  template<typename F>
  void for_each(F f)
  {
    std::unique_lock<std::mutex> l(head.m);
    node* cur = &head;
    while (node* const next = cur->next.get())
    {
      std::unique_lock<std::mutex> nextLock(next->m);
      l.unlock(); // 锁住了下一节点，因此可以释放上一节点的锁
      f(*next);
      cur = next; // 当前节点指向下一节点
      l = std::move(nextLock); // 转交下一节点锁的所有权，循环上述过程
    }
  }
  
  template<typename F>
  std::shared_ptr<T> find_first_if(F f)
  {
    std::unique_lock<std::mutex> l(head.m);
    node* cur = &head;
    while (node* const next = cur->next.get())
    {
      std::unique_lock<std::mutex> nextLock(next->m);
      l.unlock();
      if (f(*next)) return next->data; // f返回true时则返回目标值，无需继续查找
      cur = next;
      l = std::move(nextLock);
    }
    return std::shared_ptr<T>();
  }
  
  template<typename F>
  void remove_if(F f)
  {
    std::unique_lock<std::mutex> l(head.m);
    node* cur = &head;
    while (node* const next = cur->next.get())
    {
      std::unique_lock<std::mutex> nextLock(next->m);
      if (f(*next))
      { // f为true时则移除下一节点
        std::unique_ptr<node> oldNext = std::move(cur->next);
        cur->next = std::move(next->next); // 下一节点设为下下节点
        nextLock.unlock();
      }
      else
      { // 否则继续转至下一节点
        l.unlock();
        cur = next;
        l = std::move(nextLock);
      }
    }
  }
};

06 无锁并发数据结构的设计

非阻塞数据结构

    阻塞的算法和数据结构使用mutex、条件变量、期值来同步数据，但非阻塞不等价于lock-free，比如自旋锁

class spinlock_mutex {
  std::atomic_flag flag = ATOMIC_FLAG_INIT;
 public:
  void lock()
  {
    while (flag.test_and_set(std::memory_order_acquire));
  }
  void unlock()
  {
    flag.clear(std::memory_order_release);
  }
};

spinlock_mutex m;

void f(int n)
{
  for (int i = 0; i < 100; ++i)
  {
    m.lock();
    std::cout << "Output from thread " << n << '\n';
    m.unlock();
  }
}

int main()
{
  std::vector<std::thread> v;
  for (int i = 0; i < 10; ++i) v.emplace_back(f, i);
  for (auto& x : v) x.join();
}

    这里没有使用任何阻塞函数的调用，因此使用这个自旋锁的代码是非阻塞的，但并非lock-free
    非阻塞数据结构由松到严可分为三个等级：obstruction-free、lock-free、wait-free
        obstruction-free（无障碍）：如果其他线程都暂停了，任何一个给定的线程都会在有限步数内完成操作。上例就是这种情况，但这种情况很少见，
        所以满足这个条件只能算一个失败的lock-free实现
        lock-free（无锁）：如果多线程在同一个数据结构上操作，其中一个将在有限步数内完成操作。满足lock-free必定满足obstruction-free
        wait-free（无等待）：如果多线程在同一个数据结构上操作，每个线程都会在有限步数内完成操作。
        满足wait-free必定满足lock-free，但wait-free很难实现，因为要保证有限步数内完成操作，就要保证操作一次通过，并且执行到某一步不能导致其他线程操作失败
    lock-free数据结构必须允许多线程并发访问，但它们不能做相同操作，比如一个lock-free的queue允许一个线程push、另一个线程pop，但不允许两个线程同时push。
    此外，如果一个访问lock-free数据结构的线程被中途挂起，其他线程必须能完成操作而不需要等待挂起的线程
    使用lock-free数据结构主要是为了最大化并发访问，不需要阻塞。第二个原因是鲁棒性，如果线程在持有锁时死掉就会导致数据结构被永久破坏，而对lock-free数据结构来说，
    除了死掉的线程里的数据，其他的数据都不会丢失。lock-free没有任何锁，所以一定不会出现死锁
    但lock-free可能造成更大开销，用于lock-free的原子操作比非原子操作慢得多，且lock-free数据结构中的原子操作一般比lock-based中的多，
    此外，硬件必须访问同一个原子变量以在线程间同步数据。无论lock-free还是lock-based，性能方面的检查（最坏情况等待时间、平均等待时间、总体执行时间或其他方面）都是非常重要的

lock-free thread-safe stack

    最简单的stack实现方式是包含头节点指针的链表。push的过程很简单，创建一个新节点，然后让新节点的next指针指向当前head，最后head设为新节点
    这里的race condition在于，如果两个线程同时push，让各自的新节点的next指针指向当前head，这样必然导致head最终设为二者之一的新节点，而另一个被丢弃
    解决方法是，在最后设置head时先进行判断，只有当前head与新节点的next相等，才将head设为新节点，如果不等则让next指向当前head并重新判断。
    而这个操作必须是原子的，因此就需要使用compare_exchange_weak，不需要使用compare_exchange_strong，
    因为compare_exchange_weak在相等时可能替换失败，但替换失败也会返回false，放在循环里带来的效果是一样的，
    而compare_exchange_weak在一些机器架构上可以产生比compare_exchange_strong更优化的代码

template<typename T>
class A {
  struct node {
    T val;
    node* next;
    node(const T& x): val(x) {}
  };
  std::atomic<node*> head;
 public:
  void push(const T& x)
  {
    const auto newNode = new node(x);
    newNode->next = head.load();
    while (!head.compare_exchange_weak(newNode->next, newNode));
  }
};

    pop的过程也很简单，先用一个指针存储当前head，再将head设为haed->next，最后返回存储的值并删除指针
    这里的race condition在于，如果两个线程同时pop，一个已经删除了原来的head，另一个线程读取head->next时，head已经是空悬指针。
    因此，这里绕开删除指针这一步，先考虑前几步的实现

template<typename T>
class A {
 public:
  void pop(T& n) // 传引用获取结果
  {
    node* oldHead = head.load(); // 还需要考虑head为空指针的情况
    while (!head.compare_exchange_weak(oldHead, oldHead->next));
    n = oldHead->val;
  }
};

    这里还有两个问题要考虑，一是链表为空时head为空指针，二是异常安全问题。
    如果直接返回值，弹出元素一定在返回之前，如果拷贝返回值时抛出异常就会导致元素丢失（移除成功但拷贝失败），因此这里传入一个引用来保存结果。
    但实际上传引用也不行，如果其他线程移除了节点，就无法解引用被删除的节点，当前线程就不能安全地拷贝数据
    如果想安全地返回值，应该返回一个指向数据值的智能指针，如果没有返回值则可以通过返回nullptr来表示

template<typename T>
class A {
  struct node {
    std::shared_ptr<T> val;
    node* next { nullptr };
    node(const T& x) : val(std::make_shared<T>(x)) {}
  };
  std::atomic<node*> head;
 public:
  void push(const T& x)
  {
    const auto newNode = new node(x);
    newNode->next = head.load();
    while (!head.compare_exchange_weak(newNode->next, newNode));
  }
  std::shared_ptr<T> pop() // 还未考虑释放原来的头节点指针
  {
    node* oldHead = head.load();
    while (oldHead && !head.compare_exchange_weak(oldHead, oldHead->next));
    return oldHead ? oldHead->val : std::shared_ptr<T>();
  }
};

处理内存泄漏

    释放被移除的head的难点在于，一个线程在释放内存时，无法得知其他线程是否持有要释放的指针。当没有其他线程调用pop时，就可以任意释放了。
    这意味着可以用一个计数器来记录调用pop的线程数，当计数为1时就可以安全释放了，否则就把要释放的节点添加到一个待删除节点的列表

template<typename T>
class A {
  struct node {
    std::shared_ptr<T> val;
    node* next { nullptr };
    node(const T& x) : val(std::make_shared<T>(x)) {}
  };
  std::atomic<node*> head { nullptr };
  std::atomic<unsigned> cnt { 0 }; // 调用pop的线程数
  std::atomic<node*> toDel { nullptr }; // 待删除节点的列表的头节点
 public:
  void push(const T& x)
  {
    const auto newNode = new node(x);
    newNode->next = head.load();
    while (!head.compare_exchange_weak(newNode->next, newNode));
  }
  std::shared_ptr<T> pop()
  {
    ++cnt; // 调用pop的线程数加一，表示oldHead正被持有，保证可以被解引用
    node* oldHead = head.load();
    while (oldHead && !head.compare_exchange_weak(oldHead, oldHead->next));
    std::shared_ptr<T> res;
    if (oldHead) res.swap(oldHead->val); // oldHead一定可以解引用，oldHead->val设为nullptr
    try_reclaim(oldHead); // 如果计数器为1则释放oldHead，否则添加到待删除列表中
    return res; // res保存了oldHead->val
  }
 private:
  static void deleteNodes(node* n) // 释放n及之后的所有节点
  {
    while (n)
    {
      node* tmp = n->next;
      delete n;
      n = tmp;
    }
  }
  void try_reclaim(node* oldHead)
  {
    if (cnt == 1) // 调用pop的线程数为1则可以进行释放
    {
      // exchange返回toDel值，即待删除列表的头节点，再将toDel设为nullptr
      node* n = toDel.exchange(nullptr); // 获取待删除列表的头节点
      if (--cnt == 0)
      { // 没有其他线程，则释放待删除列表中所有节点
        deleteNodes(n);
      }
      else if (n)
      { // 如果多于一个线程则继续保存到待删除列表
        addToDel(n);
      }
      delete oldHead; // 删除传入的节点
    }
    else // 调用pop的线程数超过1，添加当前节点到待删除列表
    {
      addToDel(oldHead, oldHead);
      --cnt;
    }
  }
  void addToDel(node* n) // 把n及之后的节点置于待删除列表之前
  {
    node* last = n;
    while (const auto tmp = last->next) last = tmp; // last指向尾部
    addToDel(n, last); // 添加从n至last的所有节点到待删除列表
  }
  void addToDel(node* first, node* last)
  {
    last->next = toDel; // 链接到已有的待删除列表之前
    // 确保最后last->next为toDel，再将toDel设为first，first即新的头节点
    while (!toDel.compare_exchange_weak(last->next, first));
  }
};

    但如果要释放所有节点，必须有一个时刻计数器的值为0。在高负载的情况下，往往不会存在这样的时刻，从而导致待删除节点的列表无限增长

Hazard Pointer

    另一个释放的思路是，在线程访问节点时，设置一个hazard pointer，其中保存了线程ID和该节点。所有线程的hazard pointer存储在一个全局数组中，释放时检查该数组，
    如果其他线程的hazard pointer都不包含此节点，则可以直接释放，否则将节点添加到待删除列表中

std::shared_ptr<T> pop()
{
  std::atomic<void*>& hp = get_HazardPointer_for_current_thread();
  node* oldHead = head.load();
  do { // 外循环确保oldHead为最新的head，循环结束后将head设为head->next
    node* tmp;
    do { // 循环至hp设为当前最新的head
      tmp = oldHead;
      hp.store(oldHead);
      oldHead = head.load(); // 获取最新的head
    } while (oldHead != tmp);
  } while (oldHead && !head.compare_exchange_strong(oldHead, oldHead->next));
  hp.store(nullptr); // 清空hp
  std::shared_ptr<T> res;
  if (oldHead)
  {
    res.swap(oldHead->val);
    if (outstanding_hazard_pointers_for(oldHead))
    { // 如果hp数组中仍存在内部指针与head相等的hp
      reclaim_later(oldHead); // 将head添加到待删除节点的列表中
    }
    else
    { // 否则释放该节点
      delete oldHead;
    }
    delete_nodes_with_no_hazards(); // 释放待删除节点列表中可删除的节点
  }
  return res;
}

    hazard pointer的实现

constexpr int maxSize = 100;

struct HazardPointer {
  std::atomic<std::thread::id> id;
  std::atomic<void*> p;
};

HazardPointer a[maxSize];

class HP {
  HazardPointer* hp;
 public:
  HP(const HP&) = delete;
  HP operator=(const HP&) = delete;
  HP(): hp(nullptr)
  {
    for (int i = 0; i < maxSize; ++i)
    {
      std::thread::id oldId;
      if (a[i].id.compare_exchange_strong(oldId, std::this_thread::get_id()))
      { // a[i].id == oldId说明a[i]未被设置过，将其设为当前线程ID
        hp = &a[i]; // 将a[i]分配给该线程的hp
        break;
      }
    }
    if (!hp)
    { // 遍历数组都已经被设置过，说明没有新的位置可以分配给当前线程
      throw std::runtime_error("No hazard pointers available");
    }
  }

  std::atomic<void*>& getPointer()
  {
    return hp->p;
  }

  ~HP()
  {
    hp->p.store(nullptr);
    hp->id.store(std::thread::id());
  }
};

std::atomic<void*>& get_HazardPointer_for_current_thread()
{
  thread_local static HP hp; // 每个线程都有各自的hazard pointer
  return hp.getPointer();
}

bool outstanding_hazard_pointers_for(void* x)
{
  for (int i = 0; i < maxSize; ++i)
  {
    if (a[i].p.load() == x) return true;
  }
  return false;
}

    待删除节点的列表的实现

template<typename T>
void f(void* p) // 删除器
{
  delete static_cast<T*>(p);
}

struct DataToReclaim {
  void* data;
  std::function<void(void*)> deleter;
  DataToReclaim* next;

  template<typename T>
  DataToReclaim(T* p) : data(p), deleter(&f<T>), next(nullptr) {}

  ~DataToReclaim()
  {
    deleter(data);
  }
};

std::atomic<DataToReclaim*> toDel; // 待删除节点的列表的头节点

void addToDel(DataToReclaim* n)
{
  n->next = toDel.load();
  while (!toDel.compare_exchange_weak(n->next, n));
}

template<typename T> // 这里用模板来实现
void reclaim_later(T* data) // 因为DataToReclaim的构造函数是模板
{
  addToDel(new DataToReclaim(data));
}

void delete_nodes_with_no_hazards() // 释放待删除节点列表中可删除的节点
{
  DataToReclaim* cur = toDel.exchange(nullptr);
  while (cur)
  {
    DataToReclaim* const tmp = cur->next;
    if (!outstanding_hazard_pointers_for(cur->data))
    {
      delete cur;
    }
    else
    {
      addToDel(cur);
    }
    cur = tmp;
  }
}

    hazard pointer实现简单，并达到了安全释放的目的，但增加了很多开销，
    因为每次调用pop时，删除当前节点前后都要遍历数组进行检查，并且检查时需要原子访问hazard pointer的内部指针，原子操作的开销也较高
    hazard pointer包含在IBM提交的专利申请中，实际上无锁内存回收技术领域十分活跃，大公司都会申请自己的专利。不过在GPL协议下仍然允许免费使用这个专利

引用计数

    hazard pointer将使用中的节点存储在一个链表中解决了问题，而引用计数存储的是访问每个节点的线程数量。
    首先会想到使用自带引用计数的std::shared_ptr，并且它的操作也是原子的，但它不保证lock-free，可以用如下方法检查所在平台的std::shared_ptr是否lock-free

std::shared_ptr<int> p(new int(42));
assert(std::atomic_is_lock_free(&p)); // 如果std::shared_ptr在该平台lock-free

    如果是，则可以用它来实现lock-free stack

template<typename T>
class A {
  struct node {
    std::shared_ptr<T> val;
    std::shared_ptr<node> next;
    node(const T& x): val(std::make_shared<T>(x)) {}
  };
  std::shared_ptr<node> head;
 public:
  void push(const T& x)
  {
    const auto newNode = std::make_shared<node>(x);
    newNode->next = std::atomic_load(&head);
    while (!std::atomic_compare_exchange_weak(&head, &newNode->next, newNode));
  }
  std::shared_ptr<T> pop()
  {
    std::shared_ptr<node> oldHead = std::atomic_load(&head);
    while (oldHead && !std::atomic_compare_exchange_weak(&head, &oldHead, oldHead->next));
    if (oldHead)
    {
      std::atomic_store(&oldHead->next, std::shared_ptr<node>());
      return oldHead->val;
    }
    return std::shared_ptr<T>();
  }
  ~A()
  {
    while (pop());
  }
};

    实现上很难为std::shared_ptr提供原子操作成员函数，C++20将支持std::atomic<std::shared_ptr>并对std::shared_ptr弃用上面的原子操作。
    并发TS提供了std::experimental::atomic_shared_ptr，它与std::atomic<std::shared_ptr>等价，它也不保证lock-free。下面是对上例的重写

class A {
  struct node {
    std::shared_ptr<T> val;
    std::experimental::atomic_shared_ptr<node> next;
    node(const T& x): val(std::make_shared<T>(x)) {}
  };
  std::experimental::atomic_shared_ptr<node> head;
 public:
  void push(const T& x)
  {
    const auto newNode = std::make_shared<node>(x);
    newNode->next = head.load();
    while (!head.compare_exchange_weak(newNode->next, newNode));
  }
  std::shared_ptr<T> pop()
  {
    std::shared_ptr<node> oldHead = head.load();
    while (oldHead && !head.atomic_compare_exchange_weak(oldHead, oldHead->next.load()));
    if (oldHead)
    {
      oldHead->next = std::shared_ptr<node>();
      return oldHead->val;
    }
    return std::shared_ptr<T>();
  }
  ~A()
  {
    while (pop());
  }
};

    如果std::shared_ptr在平台上并非lock-free，就不能用来管理lock-free数据结构的引用计数了，这时只能手动管理
    一种方法是，为每个节点使用内部和外部两个引用计数，两者之和就是节点的引用计数。每当有线程读取节点时，外部计数递增，读取结束时内部计数递减。
    当没有线程再读取节点时，就不再需要外部计数，每当外部计数递减或被丢失，内部计数就递增。当内部计数为零时，就表示节点可以安全删除了

template<typename T>
class A {
  struct node;
  struct cntPtr {
    int exCnt { 0 }; // 外部计数
    node* p { nullptr };
  };
  struct node {
    std::shared_ptr<T> val;
    std::atomic<int> inCnt; // 内部计数
    cntPtr next;
    node(const T& x): val(std::make_shared<T>(x)), inCnt(0) {}
  };
  std::atomic<cntPtr> head;
  
  void increaseHeadCount(cntPtr& oldCnt)
  {
    cntPtr newCnt;
    do {
      newCnt = oldCnt;
      ++newCnt.exCnt; // 访问head时递增外部计数，表示该节点正被使用
    } while (!head.compare_exchange_strong(oldCnt, newCnt));
    oldCnt.exCnt = newCnt.exCnt;
  }
 public:
  void push(const T& x)
  {
    cntPtr newNode;
    newNode.p = new node(x);
    newNode.exCnt = 1;
    newNode.p->next = head.load();
    while (!head.compare_exchange_weak(newNode.p->next, newNode));
  }
  std::shared_ptr<T> pop()
  {
    cntPtr oldHead = head.load();
    for (;;)
    {
      increaseHeadCount(oldHead); // 外部计数递增表示该节点正被使用
      node* const p = oldHead.p; // 因此可以安全地访问
      if (!p) return std::shared_ptr<T>();
      if (head.compare_exchange_strong(oldHead, p->next))
      {
        std::shared_ptr<T> res;
        res.swap(p->val);
        // 再将外部计数减2加到内部计数，减2是因为，
        // 节点被删除减1，该线程无法再次访问此节点再减1
        const int increaseCount = oldHead.exCnt - 2;
        if (p->inCnt.fetch_add(increaseCount) == -increaseCount)
        { // 如果内部计数加上increaseCount为0（相加前为-increaseCount）
          delete p;
        }
        return res;
      }
      else if (p->inCnt.fetch_sub(1) == 1)
      {
        delete p;
      }
    }
  }
  ~A()
  {
    while (pop());
  }
};

内存序

    以上使用的都是默认内存序std::memory_order_seq_cst，但这是开销最大的内存序，下面根据操作间的依赖关系选择最小内存序

template<typename T>
class A {
  struct node;
  struct cntPtr {
    int exCnt;
    node* p;
  };
  struct node {
    std::shared_ptr<T> val;
    std::atomic<int> inCnt;
    cntPtr next;
    node(const T& x): val(std::make_shared<T>(x)), inCnt(0) {}
  };
  std::atomic<cntPtr> head;

  void increaseHeadCount(cntPtr& oldCnt)
  {
    cntPtr newCnt;
    do { // 比较失败不改变当前值，并可以继续循环，因此可以选择relaxed
      newCnt = oldCnt;
      ++newCnt.exCnt;
    } while (!head.compare_exchange_strong(oldCnt, newCnt, std::memory_order_acquire, std::memory_order_relaxed));
    oldCnt.exCnt = newCnt.exCnt;
  }
 public:
  void push(const T& x)
  {
    cntPtr newNode;
    newNode.p = new node(x);
    newNode.exCnt = 1;
    // 下面比较中release保证之前的语句都先执行，因此load可以使用relaxed
    newNode.p->next = head.load(std::memory_order_relaxed);
    // 比较失败不改变当前值，并可以继续循环，因此可以选择relaxed
    while (!head.compare_exchange_weak(newNode.p->next, newNode, std::memory_order_release, std::memory_order_relaxed));
  }
  std::shared_ptr<T> pop()
  {
    cntPtr oldHead = head.load(std::memory_order_relaxed);
    for (;;)
    {
      increaseHeadCount(oldHead); // acquire
      node* const p = oldHead.p;
      if (!p) return std::shared_ptr<T>();
      if (head.compare_exchange_strong(oldHead, p->next, std::memory_order_relaxed))
      {
        std::shared_ptr<T> res;
        res.swap(p->val);
        const int increaseCount = oldHead.exCnt - 2;
        // swap要先于delete，因此使用release
        if (p->inCnt.fetch_add(increaseCount, std::memory_order_release) == -increaseCount)
        {
          delete p;
        }
        return res;
      }
      else if (p->inCnt.fetch_add(-1, std::memory_order_relaxed) == 1)
      {
        p->inCnt.load(std::memory_order_acquire); // 只是用acquire来同步
        // acquire保证delete在之后执行
        delete p;
      }
    }
  }
  ~A()
  {
    while (pop());
  }
};

lock-free thread-safe queue

    queue与stack不同的是，push和pop访问的是数据结构中不同的部分，因此需要确保对其中一方的修改对另一方来说是可见的，即允许修改被另一方访问。
    以lock-based queue为基础，使用原子的头尾指针，实现如下单生产者单消费者（single-producer single-consumer）lock-free queue

template<typename T>
class A {
  struct node {
    std::shared_ptr<T> val;
    node* next;
    node(): next(nullptr) {}
  };
  std::atomic<node*> head;
  std::atomic<node*> tail;

  node* popHead()
  {
    node* const oldHead = head.load();
    if (oldHead == tail.load()) return nullptr;
    head.store(oldHead->next);
    return oldHead;
  }
 public:
  A(): head(new node), tail(head.load()) {}
  A(const A&) = delete;
  A& operator=(const A&) = delete;
  ~A()
  {
    while (node* const oldHead = head.load())
    {
      head.store(oldHead->next);
      delete oldHead;
    }
  }
  void push(T x)
  {
    std::shared_ptr<T> newVal(std::make_shared<T>(x));
    node* p = new node;
    node* const oldTail = tail.load();
    oldTail->val.swap(newVal);
    oldTail->next = p;
    tail.store(p);
  }
  std::shared_ptr<T> pop()
  {
    node* oldHead = popHead();
    if (!oldHead) return std::shared_ptr<T>();
    const std::shared_ptr<T> res(oldHead->val);
    delete oldHead;
    return res;
  }
};

    这个实现在只有一个线程push，只有一个线程pop，且push先于pop时，可以完美工作。
    但如果两个线程同时push，就会读取同一个尾节点，导致只有一个push的结果被保留，这是明显的race condition。同理，两个线程同时popHead也只会弹出一个节点
    除了确保同时只能有一个线程pop，还要确保其他线程在访问head时能安全地解引用。
    这个问题类似于lock-free stack的pop，需要确保安全地释放，因此之前lock-free stack的方案都可以套用在这里
    假设pop的问题已经解决了，现在考虑push，push的问题就是多个线程获取的是同一个tail节点。
    第一种解决方法是添加一个dummy节点，这样当前的tail只需要更新next指针，如果一个线程将next由nullptr设为新节点就说明添加成功，否则重新读取tail并添加。
    这要求对pop也做一些微小的修改，以便丢弃带空数据指针的节点并再次循环。这种方法的缺点是，每次pop通常要移除两个节点，并且内存分配是原来的两倍
    第二种方法是使用std::atomic<std::shared_ptr>作为节点数据，通过compare_exchange_strong对其设置。
    如果std::shared_ptr为lock-free则一切都解决了，如果不是就需要其他方案，比如让pop返回std::unique_ptr（对象的唯一引用），并使用原始指针原子类型作为节点数据
    下面对push进行修改，为了避免解引用时指针在pop中被释放，使用引用计数的方案，存在外部计数时就不能释放节点

void push(T x)
{
  std::unique_ptr<T> newData(new T(x));
  cntPtr newNext;
  newNext.p = new node;
  newNext.exCnt = 1;
  for (;;)
  {
    node* const oldTail = tail.load(); // 读取尾节点
    T* oldData = nullptr;
    // 解引用尾节点（oldTail->val）
    if (oldTail->val.compare_exchange_strong(oldData, newData.get()))
    {
      oldTail->next = newNext;
      tail.store(newNext.p); // 更新尾节点
      newData.release();
      break;
    }
  }
}

    使用引用计数的push的完整实现

template<typename T>
class A {
  struct node;
  struct cntPtr {
    int exCnt;
    node* p;
  };
  std::atomic<cntPtr> head;
  std::atomic<cntPtr> tail;

  struct nodeCnt {
    unsigned inCnt: 30;
    unsigned exCounter: 2;
  };

  struct node {
    std::atomic<T*> val;
    std::atomic<nodeCnt> cnt;
    cntPtr next;
    node()
    {
      nodeCnt newCnt;
      newCnt.inCnt = 0;
      newCnt.exCounter = 2;
      cnt.store(newCnt);
      next.p = nullptr;
      next.exCnt = 0;
     }
  };
 public:
  void push(T x)
  {
    std::unique_ptr<T> newData(new T(x));
    cntPtr newCnt;
    newCnt.p = new node;
    newCnt.exCnt = 1;
    cntPtr oldTail = tail.load();
    for (;;)
    {
      increaseExCnt(tail, oldTail);
      T* oldData = nullptr;
      if (oldTail.p->val.compare_exchange_strong(oldData, newData.get()))
      {
        oldTail.p->next = newCnt;
        oldTail = tail.exchange(newCnt);
        freeExCnt(oldTail);   // 注意此处可能有问题
        newData.release();
        break;
      }
      oldTail.p->releaseRef();
    }
  }
};

    使用引用计数的pop

template<typename T>
class A {
  struct node {
    void releaseRef()
    {
      nodeCnt oldCnt = cnt.load(std::memory_order_relaxed);
      nodeCnt newCnt;
      do {
        newCnt = oldCnt;
        --newCnt.inCnt;
      } while (!cnt.compare_exchange_strong(oldCnt, newCnt, std::memory_order_acquire, std::memory_order_relaxed));
      if (!newCnt.inCnt && !newCnt.exCounter)
      {
        delete this;
      }
    }
  };
 public:
  std::unique_ptr<T> pop()
  {
    cntPtr oldHead = head.load(std::memory_order_relaxed);
    for (;;)
    {
      increaseExCnt(head, oldHead);
      node* const p = oldHead.p;
      if (p == tail.load().p)
      {
        p->releaseRef();
        return std::unique_ptr<T>();
      }
      if (head.compare_exchange_strong(oldHead, p->next))
      {
        T* const res = p->val.exchange(nullptr);
        freeExCnt(oldHead);   // 注意此处可能有问题
        return std::unique_ptr<T>(res);
      }
      p->releaseRef();
    }
  }
 private:
  static void increaseExCnt(std::atomic<cntPtr>& counter, cntPtr& oldCounter)
  {
    cntPtr newCounter;
    do {
      newCounter = oldCounter;
      ++newCounter.exCnt;
    } while (!counter.compare_exchange_strong(oldCounter, newCounter, std::memory_order_acquire,std::memory_order_relaxed));
    oldCounter.exCnt = newCounter.exCnt;
  }
  static void freeExCnt(cntPtr &oldNodePtr)   // 注意这个函数的实现可能有问题
  {
    node* const p = oldNodePtr.p;
    const int increaseCount = oldNodePtr.exCnt - 2;
    nodeCnt oldCounter = p->cnt.load(std::memory_order_relaxed);
    nodeCnt newCounter;
    do {
      newCounter = oldCounter;
      --newCounter.exCounter;
      newCounter.inCnt += increaseCount;
    } while (!p->cnt.compare_exchange_strong(oldCounter, newCounter, std::memory_order_acquire,std::memory_order_relaxed));
    if (!newCounter.inCnt && !newCounter.exCounter)
    {
      delete p;
    }
  }
};

    现在没有竞争，但还有一个性能问题，一旦一个线程执行push的compare_exchange_strong成功，其他线程看到的都是新值而非nullptr，
    这会导致其他线程compare_exchange_strong失败并重新循环，造成的busy wait就相当于锁，因此代码也就不算lock-free
    为了解决此问题，必须让等待的线程有所进展，即使push线程被挂起。一个方法是替代被挂起线程完成其工作

template<typename T>
class A {
  struct node {
    std::atomic<T*> val;
    std::atomic<nodeCnt> cnt;
    std::atomic<cntPtr> next; // next改用原子类型
  };
  void setNewTail(cntPtr& oldTail, const cntPtr& newTail) // 完成tail的更新
  {
    node* const cur = oldTail.p;
    while (!tail.compare_exchange_weak(oldTail, newTail) && oldTail.p == cur);
    if (oldTail.p == cur)
    {
      freeExCnt(oldTail);
    }
    else
    {
      cur->releaseRef();
    }
  }
 public:
  void push(T x)
  {
    std::unique_ptr<T> newData(new T(x));
    cntPtr newNext;
    newNext.p = new node;
    newNext.exCnt = 1;
    cntPtr oldTail = tail.load();
    for (;;)
    {
      increaseExCnt(tail, oldTail);
      T* oldData = nullptr;
      if (oldTail.p->val.compare_exchange_strong(oldData, newData.get()))
      {
        cntPtr oldNext = { 0 };
        if (!oldTail.p->next.compare_exchange_strong(oldNext, newNext))
        {
          delete newNext.p;
          newNext = oldNext;
        }
        setNewTail(oldTail, newNext);
        newData.release();
        break;
      }
      else // 其他线程帮助完成的工作
      {
        cntPtr oldNext = { 0 };
        if (oldTail.p->next.compare_exchange_strong(oldNext, newNext))
        {
          oldNext = newNext;
        }
        setNewTail(oldTail, oldNext);
      }
    }
  }
  std::unique_ptr<T> pop()
  {
    cntPtr oldHead = head.load(std::memory_order_relaxed);
    for (;;)
    {
      increaseExCnt(head, oldHead);
      node* const p = oldHead.p;
      if (p == tail.load().p) return std::unique_ptr<T>();
      cntPtr next = p->next.load();
      if (head.compare_exchange_strong(oldHead, next))
      {
        T* const res = p->val.exchange(nullptr);
        freeExCnt(oldHead);
        return std::unique_ptr<T>(res);
      } 
      p->releaseRef();
    }
  }
};

07 并发代码的设计

线程间的工作划分

    为了提高线程利用率并最小化开销，必须决定要使用的线程数量，并为每个线程合理分配任务

开始处理之前的线程间数据划分

    简单算法最容易并行化，比如要并行化std::for_each，把元素划分到不同的线程上执行即可。如何划分才能获取最优性能，取决于数据结构的细节，
    这里用一个最简单的划分为例，每N个元素分配给一个线程，每个线程不需要与其他线程通信，直到独立完成各自的处理任务

    如果使用过MPI或OpenMP，会很熟悉这个结构，即把一个任务划分成一系列并行任务，工作线程独立完成任务，最后reduce合并结果。
    不过对for_each来说，最后的reduce实际不需要执行操作，但对其他需要合并结果的并行算法来说，最后一步很重要
    尽管这个技术很强大，但不是万能的，有时数据不能灵活划分，只有在处理数据时划分才明显，最能明显体现这点的就是递归算法，比如快速排序

递归划分数据

    要并行化快速排序，无法直接划分数据，因为只有处理之后才知道某一项应该置于基数的哪一边。
    因此，很容易想到的是使用递归，其中的递归调用完全独立，各自处理不同的元素集，十分适合并发执行

    如果数据集很大，为每个递归生成新线程就会生成大量线程，如果线程过多就会影响性能。因此需要严格控制线程数，不过这个问题可以直接抛给std::async

template<typename T>
std::list<T> parallel_quick_sort(std::list<T> v)
{
  if (v.empty()) return v;
  std::list<T> res;
  res.splice(res.begin(), v, v.begin());
  const T& firstVal = *res.begin();
  auto it = std::partition(v.begin(), v.end(), [&](const T& x) { return x < firstVal; });
  std::list<T> low;
  low.splice(low.end(), v, v.begin(), it);
  std::future<std::list<T>> l(std::async(&parallel_quick_sort<T>, std::move(low)));
  auto r(parallel_quick_sort(std::move(v)));
  res.splice(res.end(), r);
  res.splice(res.begin(), l.get());
  return res;
}

    也可以通过hardware_concurrency得知硬件可支持的线程数，再自己管理线程数。下面是一个使用stack存储已排序数据的并行快速排序

#include "lock_based_stack.hpp"
#include <thread>
#include <future>
#include <atomic>
#include <vector>
#include <list>
#include <algorithm>
#include <memory>

template<typename T>
struct sorter
{
  struct chunk_to_sort
  {
    std::list<T> data;
    std::promise<std::list<T>> promise;
  };

  thread_safe_stack<chunk_to_sort> chunks;
  std::vector<std::thread> threads;
  const unsigned max_thread_count;
  std::atomic<bool> end_of_data;

  sorter() : max_thread_count(std::thread::hardware_concurrency() - 1), end_of_data(false) {}

  ~sorter()
  {
    end_of_data = true;
    for (unsigned i = 0; i < threads.size(); ++i) threads[i].join();
  }

  void try_sort_chunk()
  {
    std::shared_ptr<chunk_to_sort> chunk = chunks.pop();
    if (chunk) sort_chunk(chunk);
  }

  std::list<T> do_sort(std::list<T>& v)
  {
    if (v.empty()) return v;
    std::list<T> res;
    res.splice(res.begin(), v, v.begin());
    const T& firstVal = *res.begin();
    auto it = std::partition(v.begin(), v.end(), [&](const T& val) { return val < firstVal; });
    chunk_to_sort low;
    low.data.splice(low.data.end(), v, v.begin(), it);
    std::future<std::list<T>> l = low.promise.get_future();
    chunks.push(std::move(low));
    if (threads.size() < max_thread_count)
    {
      threads.emplace_back(&sorter<T>::sort_thread, this);
    }
    auto r(do_sort(v));
    res.splice(res.end(), r);
    while (l.wait_for(std::chrono::seconds(0)) != std::future_status::ready)
    {
      try_sort_chunk();
    }
    res.splice(res.begin(), l.get());
    return res;
  }

  void sort_chunk(const std::shared_ptr<chunk_to_sort>& chunk)
  {
    chunk->promise.set_value(do_sort(chunk->data));
  }

  void sort_thread()
  {
    while (!end_of_data)
    {
      try_sort_chunk();
      std::this_thread::yield();
    }
  }
};

template<typename T>
std::list<T> parallel_quick_sort(std::list<T> v)
{
  if (v.empty()) return v;
  sorter<T> s;
  return s.do_sort(v);
}

基于任务划分

    如果数据动态生成或来自外部输入，上述划分方式都不适用，此时应该基于任务而非基于数据来划分。
    一种基于任务的划分方式是让线程针对性处理任务，对同一数据进行不同的操作，而不是都做相同的工作。
    这样线程是独立的，每个线程只需要负责完成总任务的某一部分。这就是SoC（separation of concerns，关注点分离）设计原则
    单线程中，如果有多个任务需要执行，只能依次执行任务，任务需要保存完成状态，并周期性地返回控制流给主循环。
    如果循环中添加了很多任务，就会导致程序变慢，对于一个用户发起的事件可能很久才会响应
    这就是使用线程的原因，如果每个任务分离在线程上，保存状态和返回控制流给主循环这些事都抛给了操作系统，
    此时只需要关注任务本身，并且任务还可以并发运行，这样用户也能及时得到响应
    但现实不一定这么顺利。如果任务都是独立的，线程之间不需要通信，那就很简单了。然而，这些后台运行的任务经常需要处理用户请求，因此就需要在完成时更新用户接口，以通知用户。
    此外，用户还可能想取消任务，这样就需要用户接口发送一条通知后台任务终止的消息。这些情况都要求周全的考虑和设计，以及合适的同步
    虽然如此，但关注点仍然是分离的。用户接口线程线程仍处理用户接口，只是可能在被其他线程请求时要更新接口。同理，后台任务线程仍然关注自己的任务，只是允许被其他线程请求终止
    多线程不是一定要SoC，比如线程间有很多共享数据，或者需要互相等待。对于这样存在过多通信的线程，应该先找出通信的原因，
    如果所有的通信都关联同一个问题，合并成一个单线程来处理可能更好一些
    基于任务划分不要求完全隔离，如果多个输入数据集合适用相同顺序的操作，可以把这个操作序列划分为多个子阶段来分配给每个线程，
    当一个线程完成操作后就把数据放进队列，供下一线程使用，这就是pipeline。
    这也是另一种划分数据的方式，适用于操作开始前输入数据不是完全已知的情况，比如来自网络的数据或者扫描文件系统以识别要处理的文件
    对于序列中耗时的操作，pipeline就能提高响应速度。比如，如果操作包含4步，每步5秒，处理完一个数据就要20秒，
    如果有4个包含整个操作的线程，虽然每20秒能处理4个数据，但每个数据仍要20秒处理。使用pipeline，每个线程只处理一步，对于第一个数据需要20秒处理，之后处理每个数据都只需要5秒

// 非pipeline：每20秒4个数据（每个数据仍要20秒）
线程A：-1- -1- -1- -1- -5- -5- -5- -5-
线程B：-2- -2- -2- -2- -6- -6- -6- -6-
线程C：-3- -3- -3- -3- -7- -7- -7- -7-
线程D：-4- -4- -4- -4- -8- -8- -8- -8-

// pipeline：第一个数据20秒，之后每个5秒
线程A：-1- -2- -3- -4- -5- -6- -7- -8-
线程B：--- -1- -2- -3- -4- -5- -6- -7-
线程C：--- --- -1- -2- -3- -4- -5- -6-
线程D：--- --- --- -1- -2- -3- -4- -5-

    以视频解码为例，每4秒120帧，第一秒达到120帧，卡顿3秒后播放下一个120帧，这样远不如稳定的每秒30帧

影响并发代码性能的因素
处理器数量

    处理器数量是影响多线程程序性能的首要因素，一个并发程序在不同环境下的表现迥异，而开发者的环境和用户很可能不同，
    比如开发环境是双核或四核系统，但用户是任意多核或单核，因此必须谨慎考虑可能的影响并对其测试
    单个16核、4个四核、16个单核是近似的，都能并发运行16个线程，要利用好这点，开发的程序必须至少用上16个线程。
    如果少于16个，就会浪费处理器性能（不考虑系统运行其他程序的情况），另一方面，如果多于16个，就会让处理器浪费时间在切换线程上，这种情况就是oversubscription
    使用hardware_concurrency可以获取硬件支持的线程数，但要注意它不会考虑已运行在系统上的其他线程，如果多个线程都用它给出的线程数，就会导致巨大的oversubscription。
    这个问题可以抛给std::async，它会适度处理并安排所有调用。这个问题也能用线程池解决
    随着处理器数量增加，另一个影响性能的问题也随之而来，即多处理器尝试访问同一数据

乒乓缓存（cache ping-pong）

    如果两个线程在不同处理器上并发执行，读取同一数据一般不会带来问题，数据将拷贝到它们的cache，处理器可以同时处理。
    但如果一个线程修改数据，这个修改传给其他核的cache就需要花费时间，从而可能导致第二个处理器停止以等待改变传到内存硬件（取决于两个线程上的操作和这个操作使用的内存序）。
    从CPU指令的角度来看，这个操作慢到惊人，等价于数百个独立指令（具体取决于硬件的物理结构）

std::atomic<unsigned long> n(0);
void f() // 任何线程都能调用
{
  // 每次n自增，处理器都要确保cache中的拷贝是最新的，修改值后再告知其他处理器
  // fetch_add是读改写操作，每次都要检索最新值，如果另一线程在另一处理器运行此代码，
  // n的数据就要在两个处理器之间来回传递，这样n增加时两个处理器的cache才能有最新值
  while (n.fetch_add(1, std::memory_order_relaxed) < 100000000)
  {
    doSomething(); // 如果doSomething很快或者有很多处理器运行此代码，处理器就要互相等待
    // 一个处理器在更新值，另一个更新值的处理器就要等待，直到第一个更新完成并把改变传过来
    // 这种情况就是high contention，反之处理器很少要互相等待的情况就是low contention
    // 在类似这样的循环中，n的数据在cache之间来回传递，这就是cache ping-pong
  }
}

    如果处理器由于等待cache转移而挂起，就只能干等着而不能做任何工作。上例的情况可能不常见，但有一些和上例没有本质区别的常见情况，比如在循环中获取mutex

std::mutex m;

void f()
{
  while (true)
  {
    std::lock_guard<std::mutex> l(m); // 现在需要来回传递的是m
    if (doneProcessing(data)) break;
  }
}

    要避免乒乓缓存，就要尽量减少多个线程对同一内存位置的竞争。但即使一个特定内存位置只能被一个线程访问，仍然可能存在乒乓缓存，原因就是伪共享

伪共享（false sharing）

    处理器cache不是独立的，而是以cache line作为最小单位，一般为32或64字节，因此小数据可能位于同一cache line。
    有时这是好事，如果一个线程访问的数据都位于同一cache line，性能会比分散在多个cache line好。但如果cache line中的数据项不相关，需要被多个线程访问，就会导致性能问题
    假如有一个int数组，一组线程频繁访问和更新其中的数据。通常int大小不超过一个cache line，因此一个cache line可以存储多个数据项，
    此时即使每个线程只访问自己需要的数据，cache硬件也会造成乒乓缓存。比如访问0号数据的线程要更新数据，cache line的所有权就要被转移到运行这个线程的处理器
    数据可能不共享，但cache line是共享的，这就是伪共享。这个问题的解决方案是，构造数据，让能被同一线程访问的数据项位于内存中的临近位置，
    让能被不同线程访问的数据在内存中相距很远。C++17提供了std::hardware_destructive_interference_size来指定当前编译目标伪共享的最大连续字节数，
    只要数据间隔大于此字节数就可以避免伪共享

data proximity

    造成伪共享的原因是两个线程访问的数据过于接近，相应的，直接影响单线程的性能则是数据布局。
    如果单线程访问的数据分散在内存中，就类似位于不同的cache line，如果在内存中十分靠近，就类似位于同一cache line。
    如果数据是分散的，就需要从内存加载更多的cache line到处理器cache，这就会增加内存访问延迟并降低性能
    如果数据是分散的，一个包含当前线程数据的chache line很可能会包含非当前线程的数据，极端情况下，cache中将包含很多不需要的数据，
    这就会浪费宝贵的cache空间并增加处理器cache miss的概率，导致必须从主存获取数据。而这个数据可能曾在cache中保留过，但为了给其他数据让出空间必须从cache中移除
    这看上去只对单线程很重要，但其实对多线程也很重要，原因在于任务切换（task switching）。
    如果线程数超过核数，就一定会有核要运行多线程，这就增加了cache的压力，因为为了避免伪共享必须确保不同的线程访问不同的cache line，
    当处理器切换线程时，如果数据分散，很可能会重新载入cache line。
    C++17提供了std::hardware_constructive_interference_size来指定保证同一cache line的最大连续字节数，如果数据尺寸小于此字节数就能降低cache miss的几率
    如果线程数超过处理器核数，操作系统可能会调度线程，在某个时间片上给一个核，在下一个时间片上给另一个核，这就要求把第一个核的cache传给第二个，从而增加了时间开销。
    虽然操作系统一般会尽量避免这点，但如果发生了就会对性能造成影响
    当大量线程准备运行而非等待时，就会经常出现任务切换问题，这种处理器在任务切换上花费大量时间的情况就是oversubscription

oversubscription

    线程经常花费时间来等待额外的I/O、mutex阻塞、条件变量，因此使用超过处理器核数的线程以确保没有闲置的处理器是合理的。
    但如果有过多的额外线程，操作系统确保为每个线程公平分配时间片，就会有沉重的任务切换负担。当一个任务重复而无限制地生成新线程，就会导致oversubscription
    如果生成的线程数过多的原因是数据划分，可以限制工作线程的数量。如果oversubscription是因为自然的工作划分，除了选择其他的划分方式，没有什么直接改善的办法。
    但选择合适的划分需要对目标平台有更多的了解，只有性能不可接受，而改变划分方式可以明显提高性能时才值得这样做
    影响多线程代码性能的因素非常多，以上只是一些有明显可见影响的主要因素，比如乒乓缓存的开销在两个单核处理器和一个双核处理器上区别很大，即使两者有相同的CPU类型和时钟速度

适用多线程性能的数据结构

    如果有两个上千行列的矩阵相乘，现在要用多线程来优化计算。一般非稀疏矩阵可以用一个大的一维数组表示，矩阵的每行在数组中连续排列。
    这个计算需要三个数组，其中一个存储计算结果。为了优化性能，就要仔细考虑数据访问模式，尤其是向结果数组的写入
    划分方式有很多，如果行列数超过处理器数，每个线程可以计算结果的某些行或列，或者一个子矩阵
    访问相邻元素可以减少对cache的使用，以及降低伪共享的概率。如果让线程计算结果的某列，就需要依次访问左矩阵的行（最终读取整个左矩阵），并读取右矩阵某列。
    矩阵保存于一维数组，行是相邻的，但列不是，因此写入结果时，其他线程可能访问同一行的其他元素。为了避免伪共享，需要让每行元素所占的空间正好是cache line的数量
    如果让线程计算结果的某行，就需要读取左矩阵的某行，并依次读取右矩阵的列（最终读取整个右矩阵）。
    此时线程按行写入结果，由于一维数组里矩阵行是连续存储的，这个连续内存块不用被其他线程访问，比起上面按列写入结果是一个改进，
    伪共享只可能发生于一个结果块的最后几个元素与下一个块的前几个元素
    如果划分为子矩阵，可以看成先按列划分再按行划分，因此它和按列划分一样存在伪共享的可能。
    如果可以避免这个可能，这个划分就有一个明显的好处，即不需要读取整个源矩阵，因此计算子矩阵比计算行好一些。
    当然，如果性能非常重要，必须针对目标架构profile各种选项并检索相关领域的文献
    对于其他数据结构的数据访问模式进行优化时，需要考虑的本质上与优化对数组的访问类似
        调整线程间的数据分布，让同一线程访问的数据尽量紧密
        尽量减少线程所需的数据量
        依据std::hardware_destructive_interference_size，确保不同线程访问的数据距离足够远，以避免伪共享
    这些用在其他数据结构上并不容易，比如二叉树很难在子树以外的任何单元中再分割，并且二叉树的节点一般是动态分配的，从而会分布在堆的不同位置上。
    数据位于堆的不同位置不是什么特别的问题，但确实意味着处理器需要在cache中保存更多东西。
    不过这是有益的，如果多个线程要遍历树，就都需要访问树节点，如果树节点只包含保存数据的指针，处理器只要在需要时从内存加载数据，
   如果数据被需要它的线程修改了，这能避免节点数据本身和提供树结构的数据之间的伪共享带来的性能问题
    用mutex保护数据也有类似问题。假如有一个类，它包含一个mutex和一些被保护的数据，如果mutex和数据在内存中很接近，这对获取mutex的线程是很理想的，
   为了修改mutex，需要的数据可能已经跟着加载在处理器cache中了。但这也有一个缺点，如果其他线程尝试获取mutex，就会需要访问那块内存
    互斥锁的典型实现为，一个操作在mutex内存位置上以尝试获取mutex的读改写原子操作，如果mutex已锁定，就接着调用操作系统内核。
    这个读改写操作可能会导致，持有该mutex的线程的cache中保存的数据无效。这对于mutex不是问题，在mutex解锁之前线程不会接触mutex，
    但如果mutex与数据共享同一cache line，另一个线程尝试获取mutex时，持有mutex的线程就会受到性能影响
    一个测试这种伪共享是否会带来影响的方法是，在能被并发访问的数据之间添加巨大的填充块。比如用如下方式测试mutex竞争问题

struct protected_data
{
  std::mutex m;
  // 使用超过一个cache line字节数的填充即可
  char padding[std::hardware_destructive_interference_size]; // 如果不支持C++17可以直接padding[65536]
  my_data data_to_protect;
};

    用如下方式测试数组数据伪共享，如果性能提高了就说明伪共享影响了性能，并且可以保留填充或者用其他方式重排数据访问来消除伪共享

struct my_data
{
  data_item1 d1;
  data_item2 d2;
  char padding[std::hardware_destructive_interference_size];
};
my_data some_array[256];

并发设计的其他注意事项

    除了上述问题，设计并发代码时还需要考虑异常安全和可扩展性。如果代码不是异常安全的，就可能导致破坏不变量或race condition，或由于一个操作抛出异常导致程序意外终止。
   可扩展性指的是，性能会随着处理器核数的提升而提升，如果处理器核数是之前的100倍，则最理想的情况下性能也应该之前的100倍

并发算法的异常安全

    并行算法比串行算法更注重异常问题。在串行算法中，如果一个操作抛出异常，只需要保证吞下此异常以避免资源泄漏或破坏不变量，它可以愉快地允许异常传播给调用者处理。
    但在并行算法中，许多操作运行在不同的线程上，异常就不允许传播，因为它在错误的调用栈上。如果新线程上的函数存在异常，程序就会终止
    回顾以前提到的并行版本的std::accumulate，它就是非异常安全的，代码可能抛出异常的位置如下

template<typename Iterator, typename T>
struct accumulate_block {
  void operator()(Iterator first, Iterator last, T& res)
  {
    res = std::accumulate(first, last, res); // 可能抛异常
  }
};

template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init)
{
  const unsigned long len = std::distance(first, last); // 此时没做任何事，抛异常无影响
  if (!len) return init;
  const unsigned long min_per_thread = 25;
  const unsigned long max_threads = (len + min_per_thread - 1) / min_per_thread;
  const unsigned long hardware_threads = std::thread::hardware_concurrency();
  const unsigned long num_threads =
    std::min(hardware_threads != 0 ? hardware_threads : 2, max_threads);
  const unsigned long block_size = len / num_threads;
  std::vector<T> res(num_threads); // 仍未做任何事，抛异常无影响
  std::vector<std::thread> threads(num_threads - 1); // 同上
  Iterator block_start = first; // 同上
  for (unsigned long i = 0; i < num_threads - 1; ++i)
  {
    Iterator block_end = block_start; // 同上
    std::advance(block_end, block_size);
    // 下面创建std::thread，抛异常就导致析构对象，并调用std::terminate终止程序
    threads[i] = std::thread(accumulate_block<Iterator, T>{},
      block_start, block_end, std::ref(res[i]));
    block_start = block_end;
  }
  // accumulate_block::operator()调用的std::accumulate可能抛异常，此时抛异常造成问题同上
  accumulate_block<Iterator, T>()(block_start, last, res[num_threads - 1]);
  std::for_each(threads.begin(), threads.end(), std::mem_fn(&std::thread::join));
  // 最后调用std::accumulate可能抛异常，但不引发大问题，因为所有线程已经join了
  return std::accumulate(res.begin(), res.end(), init);
}

    上面已经分析了所有可能抛出异常的位置，下面来处理这些问题。新线程想做的是返回计算结果，但可能抛出异常导致std::thread析构，而析构没被join的std::thread将导致程序终止。
    解决这个问题很简单，结合使用std::packaged_task和std::future，再把工作线程的异常抛出到主线程，让主线程处理即可

template<typename Iterator, typename T>
struct accumulate_block {
  T operator()(Iterator first, Iterator last)
  {
    return std::accumulate(first, last, T{});
  }
};

template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init)
{
  const unsigned long len = std::distance(first, last);
  if (!len) return init;
  const unsigned long min_per_thread = 25;
  const unsigned long max_threads = (len + min_per_thread - 1) / min_per_thread;
  const unsigned long hardware_threads = std::thread::hardware_concurrency();
  const unsigned long num_threads =
    std::min(hardware_threads != 0 ? hardware_threads : 2, max_threads);
  const unsigned long block_size = len / num_threads;
  std::vector<std::future<T>> fts(num_threads - 1); // 改用std::future获取值
  std::vector<std::thread> threads(num_threads - 1);
  Iterator block_start = first;
  for (unsigned long i = 0; i < num_threads - 1; ++i)
  {
    Iterator block_end = block_start;
    std::advance(block_end, block_size);
    // 用std::packaged_task替代直接创建std::thread
    std::packaged_task<T(Iterator, Iterator)> pt(accumulate_block<Iterator, T>{});
    fts[i] = pt.get_future();
    threads[i] = std::thread(std::move(pt), block_start, block_end);
    block_start = block_end;
  }
  T last_res = accumulate_block<Iterator,T>{}(block_start, last);
  std::for_each(threads.begin(), threads.end(), std::mem_fn(&std::thread::join));
  T res = init;
  try
  {
    for (unsigned long i = 0; i < num_threads - 1; ++i) res += fts[i].get();
    res += last_res;
  }
  catch(...)
  {
    for (unsigned long i = 0; i < num_threads - 1; ++i)
    {
      if (threads[i].joinable()) threads[i].join();
    }
    throw;
  }
  return res;
}

    不过try-catch很难看，并且导致了重复代码（正常控制流和catch块都对线程执行join），因此可以用RAII来处理

class threads_guard {
  std::vector<std::thread>& threads;
 public:
  explicit threads_guard(std::vector<std::thread>& t): threads(t) {}
  ~threads_guard()
  {
    for (auto& x : threads)
    {
      if (x.joinable()) x.join();
    }
  }
};

template<typename Iterator, typename T>
struct accumulate_block {
  T operator()(Iterator first, Iterator last)
  {
    return std::accumulate(first, last, T{});
  }
};

template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init)
{
  const unsigned long len = std::distance(first, last);
  if (!len) return init;
  const unsigned long min_per_thread = 25;
  const unsigned long max_threads = (len + min_per_thread - 1) / min_per_thread;
  const unsigned long hardware_threads = std::thread::hardware_concurrency();
  const unsigned long num_threads =
    std::min(hardware_threads != 0 ? hardware_threads : 2, max_threads);
  const unsigned long block_size = len / num_threads;
  std::vector<std::future<T>> fts(num_threads - 1);
  std::vector<std::thread> threads(num_threads - 1);
  threads_guard g(threads); // threads元素析构时自动join
  Iterator block_start = first;
  for (unsigned long i = 0; i < num_threads - 1; ++i)
  {
    Iterator block_end = block_start;
    std::advance(block_end, block_size);
    std::packaged_task<T(Iterator, Iterator)> pt(accumulate_block<Iterator, T>{});
    fts[i] = pt.get_future();
    threads[i] = std::thread(std::move(pt), block_start, block_end);
    block_start = block_end;
  }
  T last_res = accumulate_block<Iterator,T>{}(block_start, last);
  std::for_each(threads.begin(), threads.end(), std::mem_fn(&std::thread::join));
  T res = init;
  for (unsigned long i = 0; i < num_threads - 1; ++i) res += fts[i].get();
  res += last_res;
  return res;
}

    更优雅的方式是使用std::async

template<typename Iterator, typename T>
T parallel_accumulate(Iterator first, Iterator last, T init)
{
  const unsigned long len = std::distance(first, last);
  const unsigned long max_chunk_size = 25;
  if (len <= max_chunk_size)
  {
    return std::accumulate(first, last, init);
  }
  else
  {
    Iterator mid_point = first;
    std::advance(mid_point, len / 2);
    std::future<T> first_half_res = std::async(parallel_accumulate<Iterator, T>, first, mid_point, init);
    // 递归调用如果抛出异常，由std::async创建的std::future将在异常传播时被析构
    T second_half_res = parallel_accumulate(mid_point, last, T{});
    // 如果异步任务抛出异常，get就会捕获异常并重新抛出
    return first_half_res.get() + second_half_res;
  }
}

可扩展性与阿姆达尔定律（Amdahl’s law）

    可扩展性代表了程序对处理器的利用率。单线程程序就是不可扩展的，因为处理器增加完全不能提高单线程程序的性能。
    对于多线程程序，线程经常需要花费时间等待（等待其他线程、获取mutex、修改条件变量、完成I/O操作......），
    一种简化看待多线程程序的方式是将其分为串行和并行部分，由此可以得到如下公式，即阿姆达尔定律

S = 1 / (a + ( 1 - a ) / N) // a为串行部分占比，N为处理器倍数，S为性能倍数
// 正常情况下S < 1 / a，最理想的情况是a为0，S = N

用多线程隐藏延迟（lantency）

    如果在线程等待期间让系统做一些有用的事，就相当于隐藏了等待。如果只有和处理器单元一样多的线程，阻塞就意味着浪费CPU时间，因此可以利用这个时间去运行额外的线程。
    比如一个用pipeline划分工作的病毒扫描程序，一个线程检索文件系统并将文件放入队列，这是一个费时的I/O操作，因此同时可以让另一线程从队列获取文件名，加载并扫描文件
    利用空闲的CPU时间也可能不需要运行额外的线程。比如，如果一个线程因为等待I/O操作而阻塞，使用异步I/O就是合理的，当I/O操作异步运行在后台时，线程就能做有用的工作。
    又比如，一个线程等待另一线程执行一个操作时，与其阻塞，不如自己执行操作（如lock-free queue）。
    更极端的例子是，如果线程等待一个未被任何线程启动的任务完成，这个线程可能自己执行此任务，或执行另一个未完成的任务

用并发提高响应度（responsiveness）

    添加线程不一定是为了确保使用所有可用的处理器，有时是为了确保及时处理外部事件，以提高系统响应度。
    现代GUI框架大多是事件驱动的，为了确保处理所有事件和消息，GUI程序一般包含一个如下循环

while (true)
{
  event_data event = get_event();
  if (event.type == quit) break;
  process(event);
}

    如果是单线程程序，就很难编写长期运行的任务。为了确保即使响应用户输入，就要以合理频率调用get_event和process，
    这意味着任务要被周期性悬挂（suspend）并把控制流返回给事件循环，或者在代码中的一个适当点调用get_event和process，二者任一都会复杂化任务实现
    通过SoC（separation of concerns）可以把很长的任务放在一个全新的线程上，而让GUI线程来处理事件，线程可以通过简单的机制进行通信，而不需要混入处理事件的代码，
    这样即使任务耗费很长时间，用户线程也总能及时响应事件

std::thread task_thread;
std::atomic<bool> task_cancelled(false);

void gui_thread()
{
  while (true)
  {
    event_data event = get_event();
    if (event.type == quit) break;
    process(event);
  }
}

void task()
{
  while (!task_complete() && !task_cancelled) do_next_operation();
  if (task_cancelled)
  {
    perform_cleanup();
  }
  else
  {
    post_gui_event(task_complete);
  }
}

void process(const event_data& event)
{
  switch(event.type)
  {
    case start_task:
      task_cancelled = false;
      task_thread = std::thread(task);
      break;
    case stop_task:
      task_cancelled = true;
      task_thread.join();
      break;
    case task_complete:
      task_thread.join();
      display_results();
      break;
    default:
      ...
  }
}

实践

    下面为标准库的三个算法实现并行版本，这些实现仅是为了阐述技术的运用，而不是最先进高效的实现。
    更先进的实现可以在学术文献或专业的多线程库（如Intel的Threading Building Blocks）中找到

并行版std::for_each

    std::for_each会按顺序依次作用于每个元素，而并行版不保证顺序，元素最好被并发处理，为此需要把元素划分给每个线程。
    实际上，并行版std::for_each与并行版std::accumulate的实现思路基本一样：使用hardware_concurrency决定线程数，使用连续数据块避免伪共享，
    使用std::packaged_task和std::future在线程间传递异常

class threads_guard {
  std::vector<std::thread>& threads;
 public:
  explicit threads_guard(std::vector<std::thread>& t) : threads(t) {}
  ~threads_guard()
  {
    for (auto& x : threads)
    {
      if (x.joinable()) x.join();
    }
  }
};

template<typename Iterator, typename Func>
void parallel_for_each(Iterator first, Iterator last, Func f)
{
  const unsigned long len = std::distance(first, last);
  if (!len) return;
  const unsigned long min_per_thread = 25;
  const unsigned long max_threads = (len + min_per_thread - 1) / min_per_thread;
  const unsigned long hardware_threads = std::thread::hardware_concurrency();
  const unsigned long num_threads =
    std::min(hardware_threads != 0 ? hardware_threads : 2, max_threads);
  const unsigned long block_size = len / num_threads;
  std::vector<std::future<void>> fts(num_threads - 1);
  std::vector<std::thread> threads(num_threads - 1);
  threads_guard g(threads);
  Iterator block_start = first;
  for (unsigned long i = 0; i < num_threads - 1; ++i)
  {
    Iterator block_end = block_start;
    std::advance(block_end, block_size);
    std::packaged_task<void(void)> pt([=] { std::for_each(block_start, block_end, f); });
    fts[i] = pt.get_future();
    threads[i] = std::thread(std::move(pt));
    block_start = block_end;
  }
  std::for_each(block_start, last, f);
  for (unsigned long i = 0; i < num_threads - 1; ++i) fts[i].get(); // 只是为了传递异常
}

    同并行版std::accumulate一样，也可以使用std::async来简化实现

template<typename Iterator, typename Func>
void parallel_for_each(Iterator first, Iterator last, Func f)
{
  const unsigned long len = std::distance(first, last);
  if (!len) return;
  const unsigned long min_per_thread = 25;
  if (len < 2 * min_per_thread)
  {
    std::for_each(first, last, f);
  }
  else
  {
    const Iterator mid_point = first + len / 2;
    std::future<void> first_half = std::async(&parallel_for_each<Iterator, Func>, first, mid_point, f);
    parallel_for_each(mid_point, last, f);
    first_half.get();
  }
}

并行版std::find

    std::find的不同之处在于，只要找到目标值就应该停止继续查找。
    在并行版本中，一个线程找到了值，不仅自身要停止继续查找，还应该通知其他线程停止，这点可以使用一个原子变量作为标记来实现
    有两种可选方式来返回值和传播异常，一是使用std::future数组和std::packaged_task将返回值和异常交给主线程处理，二是使用std::promise直接设置最终结果。
    如果想在首个异常上终止（即使没有处理完所有元素）则使用std::promise，如果想让其他线程继续搜索则使用std::packaged_task保存所有异常，
    并在没有找到目标值时重新抛出其中一个异常。这里选择使用行为更接近std::find的std::promise

class threads_guard {
  std::vector<std::thread>& threads;
 public:
  explicit threads_guard(std::vector<std::thread>& t) : threads(t) {}
  ~threads_guard()
  {
    for (auto& x : threads)
    {
      if (x.joinable()) x.join();
    }
  }
};

template<typename Iterator, typename T>
Iterator parallel_find(Iterator first, Iterator last, T match)
{
  struct find_element {
    void operator()(Iterator begin, Iterator end, T match,
      std::promise<Iterator>* res, std::atomic<bool>* done_flag)
    {
      try
      {
        for (; begin != end && !done_flag->load(); ++begin)
        {
          if (*begin == match)
          {
            res->set_value(begin);
            done_flag->store(true);
            return;
          }
        }
      }
      catch(...)
      {
        try
        {
          res->set_exception(std::current_exception());
          done_flag->store(true);
        }
        catch(...){}
      }
    }
  };
  const unsigned long len = std::distance(first, last);
  if (!len) return last;
  const unsigned long min_per_thread = 25;
  const unsigned long max_threads = (len + min_per_thread - 1) / min_per_thread;
  const unsigned long hardware_threads = std::thread::hardware_concurrency();
  const unsigned long num_threads =
    std::min(hardware_threads != 0 ? hardware_threads : 2, max_threads);
  const unsigned long block_size = len / num_threads;
  std::promise<Iterator> res;
  std::atomic<bool> done_flag(false);
  std::vector<std::thread> threads(num_threads - 1);
  {
    threads_guard g(threads);
    Iterator block_start = first;
    for (unsigned long i = 0; i < num_threads - 1; ++i)
    {
      Iterator block_end = block_start;
      std::advance(block_end, block_size);
      threads[i] = std::thread(find_element{}, block_start, block_end, match, &res, &done_flag);
      block_start = block_end;
    }
    find_element()(block_start, last, match, &res, &done_flag);
  }
  if (!done_flag.load()) return last;
  return res.get_future().get();
}

    也可以使用std::async实现

template<typename Iterator, typename T>
Iterator parallel_find_impl(Iterator first, Iterator last, T match, std::atomic<bool>& done_flag)
{
  try
  {
    const unsigned long len = std::distance(first, last);
    const unsigned long min_per_thread = 25;
    if (len < (2 * min_per_thread))
    {
      for (; first != last && !done_flag.load(); ++first)
      {
        if (*first == match)
        {
          done_flag = true;
          return first;
        }
      }
      return last; 
    }
    else
    {
      const Iterator mid_point = first + len / 2;
      std::future<Iterator> async_res =
        std::async(&parallel_find_impl<Iterator, T>, mid_point, last, match, std::ref(done_flag));
      const Iterator direct_res = parallel_find_impl(first, mid_point, match, done_flag);
      return direct_res == mid_point ? async_res.get() : direct_res;
    }
  }
  catch(...)
  {
    done_flag = true;
    throw;
  }
}

template<typename Iterator, typename T>
Iterator parallel_find(Iterator first, Iterator last, T match)
{
  std::atomic<bool> done_flag(false);
  return parallel_find_impl(first, last, match, done_flag);
}

并行版std::partial_sum

    std::partial_sum会依次累加元素的和（默认是加，也可以是其他二元操作）

std::vector<int> v { 2, 2, 2, 2 };
std::partial_sum(v.begin(), v.end(), // 输入的迭代器范围
  std::ostream_iterator<int>(std::cout << " "), // 输出到的迭代器起始位置
  std::plus<int>{}); // 使用的二元运算符，不指定则默认累加
// 输出2468

    其实现为

template<class InputIt, class OutputIt, class BinaryOperation>
OutputIt partial_sum(InputIt first, InputIt last, OutputIt d_first, BinaryOperation op)
{
  if (first == last) return d_first;
  typename std::iterator_traits<InputIt>::value_type sum = *first;
  *d_first = sum;
  while (++first != last)
  {
    sum = op(std::move(sum), *first);
    *++d_first = sum;
  }
  return ++d_first;
}

    实现并行版本时，第一种划分方式就是传统的按块划分

1 1 1 1 1 1 1 1 1 // 输入9个1
// 划分为三部分
1 1 1
1 1 1
1 1 1
// 得到三个部分的结果
1 2 3
1 2 3
1 2 3
// 将第一部分的尾元素（即3）加到第二部分
1 2 3
4 5 6
1 2 3
// 再将第二部分的尾元素（即6）加到第三部分
1 2 3
4 5 6
7 8 9

    这种划分方式的实现如下。由于需要线程间同步，这个实现不容易简单地用std::async重写

class threads_guard {
  std::vector<std::thread>& threads;
 public:
  explicit threads_guard(std::vector<std::thread>& t) : threads(t) {}
  ~threads_guard()
  {
    for (auto& x : threads)
    {
      if (x.joinable()) x.join();
    }
  }
};

template<typename Iterator>
void parallel_partial_sum(Iterator first, Iterator last)
{
  using value_type = typename Iterator::value_type;
  struct process_chunk {
    void operator()(Iterator begin, Iterator last,
      std::future<value_type>* previous_end_value,
      std::promise<value_type>* end_value)
    {
      try
      {
        Iterator end = last;
        ++end;
        std::partial_sum(begin, end, begin);
        if (previous_end_value) // 不是第一个块
        {
          value_type addend = previous_end_value->get();
          *last += addend;
          if (end_value)
          {
            end_value->set_value(*last);
          }
          std::for_each(begin, last, [addend](value_type& item) { item += addend; });
        }
        else if (end_value)
        {
          end_value->set_value(*last); // 是第一个块则可以为下个块更新尾元素
        }
      }
      catch(...)
      {
        // 如果抛出异常则存储到std::promise，异常会传播给下一个块（获取这个块的尾元素时）
        if (end_value)
        {
          end_value->set_exception(std::current_exception());
        }
        else
        {
          throw; // 异常最终传给最后一个块，此时再抛出异常
        }
      }
    }
  };
  const unsigned long len = std::distance(first, last);
  if (!len) return;
  const unsigned long min_per_thread = 25;
  const unsigned long max_threads = (len + min_per_thread - 1) / min_per_thread;
  const unsigned long hardware_threads = std::thread::hardware_concurrency();
  const unsigned long num_threads =
    std::min(hardware_threads != 0 ? hardware_threads : 2, max_threads);
  const unsigned long block_size = len / num_threads;
  std::vector<std::thread> threads(num_threads - 1);
  std::vector<std::promise<value_type>> end_values(num_threads - 1); // 存储块内尾元素值
  std::vector<std::future<value_type>> previous_end_values; // 检索前一个块的尾元素
  previous_end_values.reserve(num_threads - 1);
  threads_guard g(threads);
  Iterator block_start = first;
  for (unsigned long i = 0; i < num_threads - 1; ++i)
  {
    Iterator block_last = block_start;
    std::advance(block_last, block_size - 1); // 指向尾元素
    threads[i] = std::thread(process_chunk{},
      block_start, block_last,
      i != 0 ? &previous_end_values[i - 1] : nullptr,
      &end_values[i]);
    block_start = block_last;
    ++block_start;
    previous_end_values.emplace_back(end_values[i].get_future());
  }
  Iterator final_element = block_start;
  std::advance(final_element, std::distance(block_start, last) - 1);
  process_chunk{}(block_start, final_element,
    num_threads > 1 ? &previous_end_values.back() : nullptr,
    nullptr);
}

    如果处理器核数非常多，就没必要使用上面的方式了，因为还有并发度更高的方式。第二种划分方式是隔一定距离计算，每轮计算完成，下一轮计算使用的距离变为之前的两倍

1 1 1 1 1 1 1 1 1 // 输入9个1
// 先让距离为1的元素相加
1 2 2 2 2 2 2 2 2
// 再让距离为2的元素相加
1 2 3 4 4 4 4 4 4
// 再让距离为4的元素相加
1 2 3 4 5 6 7 8 8
// 再让距离为8的元素相加
1 2 3 4 5 6 7 8 9

    这种方式不再需要进一步同步，因为所有中间的结果都直接传给了下一个需要这些结果的处理器。但实际上很少有处理器可以在多条数据上同时执行同一条指令（即SIMD），
    因此必须为通用情况设计代码，在每步操作上显式同步线程。一个方式就是使用barrier的同步机制，它要求线程等待，直到所有线程到达barrier时才能继续执行下一步。
    并发TS提供了std::experimental::barrier，这里自行实现一个barrier

class threads_guard {
  std::vector<std::thread>& threads;
 public:
  explicit threads_guard(std::vector<std::thread>& t) : threads(t) {}
  ~threads_guard()
  {
    for (auto& x : threads)
    {
      if (x.joinable()) x.join();
    }
  }
};

struct barrier {
  std::atomic<unsigned> count; // 需要同步的线程数
  std::atomic<unsigned> spaces; // 剩余未到达barrier的线程数
  std::atomic<unsigned> generation; // 所有线程到达barrier的总次数
  barrier(unsigned n) : count(n), spaces(n), generation(0) {}
  void wait()
  {
    const unsigned gen = generation.load();
    if (!--spaces) // 递减
    {
      spaces = count.load(); // 递减后为0则重置spaces为count
      ++generation;
    }
    else
    {
      while (generation.load() == gen) std::this_thread::yield();
    }
  }
  void done_waiting() // 供最后一个线程调用
  {
    --count;
    if (!--spaces)
    {
      spaces = count.load();
      ++generation;
    }
  }
};

template<typename Iterator>
void parallel_partial_sum(Iterator first, Iterator last)
{
  using value_type = typename Iterator::value_type;
  struct process_element {
    void operator()(Iterator first, Iterator last, std::vector<value_type>& v, unsigned i, barrier& b)
    {
      value_type& ith_element = *(first + i);
      for (unsigned step = 0, stride = 1; stride <= i; ++step, stride *= 2)
      {
        const value_type& source = ith_element;
        value_type& dest = v[i];
        const value_type& addend = *(first + i - stride);
        dest = source + addend;
        b.wait();
        ith_element = dest;
      }
      b.done_waiting();
    }
  };
  const unsigned long len = std::distance(first, last);
  if (len <= 1) return;
  std::vector<value_type> v(len);
  barrier b(len);
  std::vector<std::thread> threads(len - 1);
  threads_guard g(threads);
  Iterator block_start = first;
  for (unsigned long i = 0; i < len - 1; ++i)
  {
    threads[i] = std::thread(process_element{}, first, last, std::ref(v), i, std::ref(b));
  }
  process_element{}(first, last, v, len - 1, b);
}

    这个实现不是异常安全的，如果执行process_element时抛出异常程序就会终止，为此可以使用std::promise存储异常，或者使用被mutex保护的std::exception_ptr

08 高级线程管理

线程池

    线程池一般会用一个表示线程数的参数来初始化，内部需要一个队列来存储任务。下面是一个最简单的线程池实现

class thread_pool {
  std::mutex m;
  std::condition_variable cv;
  bool done = false; // 线程池析构标志
  std::queue<std::function<void()>> q; // 存储任务的队列
 public:
  explicit thread_pool(unsigned n) // n为线程数
  {
    for (unsigned i = 0; i < n; ++i)
    {
      std::thread{
        [this]
        {
          std::unique_lock l(m);
          for (;;)
          {
            if (!q.empty())
            {
              auto task = std::move(q.front());
              q.pop();
              l.unlock();
              task();
              l.lock();
            }
            else if (done) // if (q.empty() && done)
            {
              break;
            }
            else // if (q.empty() && !done)
            {
              cv.wait(l);
            } // 等价于cv.wait(l, [this] { return done || q.empty(); });
          }
        }
      }.detach();
    }
  }

  ~thread_pool()
  {
    {
      std::scoped_lock l(m);
      done = true; // 加锁的原因是cv.wait使用了done作为判断条件
    }
    cv.notify_all();
  }

  template<typename F>
  void submit(F&& f)
  {
    {
      std::scoped_lock l(m);
      q.emplace(std::forward<F>(f));
    }
    cv.notify_one();
  }
};

    如果想让提交的任务带参数会麻烦很多

template<class F, class... Args>
auto submit(F&& f, Args&&... args)
{
  using RT = std::invoke_result_t<F, Args...>;
  auto task = // std::packaged_task不允许拷贝构造，不能直接传入lambda，因此要借助std::shared_ptr
    std::make_shared<std::packaged_task<RT()>>(std::bind(std::forward<F>(f), std::forward<Args>(args)...));
  // 但std::bind会按值拷贝实参，因此这个实现不允许任务的实参是move-only类型
  {
    std::scoped_lock l(m);
    q.emplace([task]() { (*task)(); }); // 捕获指针以传入std::packaged_task
  }
  cv.notify_one();
  return task->get_future();
}

    书上实现的线程池都在死循环中使用了std::this_thread::yield来转让时间片，但这样做的问题是，如果线程池处于空闲状态，就会无限转让时间片，导致CPU使用率达100%，
    下面是对书中的第一个简单线程池的CPU使用率测试结果

    对相同任务用之前实现的线程池的测试结果

    因此书中的线程池实现是不合理的，不过这里还是把书上的内容列出来，以下均为书中内容
    最简单的线程池

class thread_pool {
  // 析构时会倒序析构，为了确保析构不出问题要注意声明顺序
  std::atomic<bool> done;
  thread_safe_queue<std::function<void()>> q;
  std::vector<std::thread> threads; // 标记和任务队列要在此之前声明
  threads_guard g; // threads_guard在最后声明
  void worker_thread()
  {
    while (!done)
    {
      std::function<void()> task;
      if (q.try_pop(task))
      {
        task();
      }
      else
      {
        std::this_thread::yield();
      }
    }
  }
 public:
  thread_pool() : done(false), g(threads)
  {
    const unsigned n = std::thread::hardware_concurrency();
    try
    {
      for (unsigned i = 0; i < n; ++i)
      {
        threads.emplace_back(&thread_pool::worker_thread, this);
      }
    }
    catch(...)
    {
      done = true;
      throw;
    }
  }

  ~thread_pool() { done = true; }

  template<typename F>
  void submit(F f)
  {
    q.push(std::function<void()>(f));
  }
};

    不过这个线程池过于简单，只能执行无参数无返回值的函数，并且可能出现死锁，不足以满足大多数情况的要求，下面希望能执行无参数但有返回值的函数。
    为了得到返回值，就应该把函数传递给std::packaged_task再加入队列，并返回std::packaged_task中的std::future。
    由于std::packaged_task是move-only类型，而std::function要求存储的函数实例可以拷贝构造，
    因此这里需要实现一个支持move-only类型的函数包裹类，即一个带call操作的类型擦除（type-erasure）类

class function_wrapper {
  struct impl_base {
    virtual void call() = 0;
    virtual ~impl_base() {}
  };
  std::unique_ptr<impl_base> impl;

  template<typename F>
  struct impl_type : impl_base
  {
    F f;
    impl_type(F&& f_) noexcept: f(std::move(f_)) {}
    void call() override { f(); }
  };
 public:
  function_wrapper() = default;
  function_wrapper(const function_wrapper&) = delete;
  function_wrapper& operator=(const function_wrapper&) = delete;
  function_wrapper(function_wrapper&& rhs) noexcept : impl(std::move(rhs.impl)) {}
  function_wrapper& operator=(function_wrapper&& rhs) noexcept
  {
    impl = std::move(rhs.impl);
    return *this;
  }
  template<typename F>
  function_wrapper(F&& f) : impl(new impl_type<F>(std::move(f))) {}

  void operator()() const { impl->call(); }
};

    用这个包裹类替代std::function<void()>

class thread_pool {
  std::atomic<bool> done;
  thread_safe_queue<function_wrapper> q;
  std::vector<std::thread> threads;
  threads_guard g;
  void worker_thread()
  {
    while (!done)
    {
      function_wrapper task;
      if (q.try_pop(task))
      {
        task();
      }
      else
      {
        std::this_thread::yield();
      }
    }
  }
 public:
  thread_pool() : done(false), g(threads)
  {
    const unsigned n = std::thread::hardware_concurrency();
    try
    {
      for (unsigned i = 0; i < n; ++i)
      {
        threads.emplace_back(&thread_pool::worker_thread, this);
      }
    }
    catch(...)
    {
      done = true;
      throw;
    }
  }

  ~thread_pool() { done = true; }

  template<typename F>
  std::future<std::invoke_result_t<F>> submit(F f)
  {
    std::packaged_task<std::invoke_result_t<F>()> task(std::move(f));
    std::future<std::invoke_result_t<F>> res(task.get_future());
    q.push(std::move(task));
    return res;
  }
};

    往线程池添加任务会增加任务队列的竞争，lock-free队列可以避免这点但存在乒乓缓存的问题。
    为此需要把任务队列拆分为线程独立的本地队列和全局队列，当线程队列无任务时就去全局队列取任务

class thread_pool {
  std::atomic<bool> done;
  thread_safe_queue<function_wrapper> pool_work_queue;
  inline static thread_local std::unique_ptr<std::queue<function_wrapper>> local_work_queue;
  std::vector<std::thread> threads;
  threads_guard g;
  void worker_thread()
  {
    local_work_queue.reset(new std::queue<function_wrapper>);
    while (!done)
    {
      function_wrapper task;
      if (local_work_queue && !local_work_queue->empty())
      {
        task = std::move(local_work_queue->front());
        local_work_queue->pop();
        task();
      }
      else if (pool_work_queue.try_pop(task))
      {
        task();
      }
      else
      {
        std::this_thread::yield();
      }
    }
  }
 public:
  thread_pool() : done(false), g(threads)
  {
    const unsigned n = std::thread::hardware_concurrency();
    try
    {
      for (unsigned i = 0; i < n; ++i)
      {
        threads.emplace_back(&thread_pool::worker_thread, this);
      }
    }
    catch(...)
    {
      done = true;
      throw;
    }
  }

  ~thread_pool() { done = true; }

  template<typename F>
  std::future<std::invoke_result_t<F>> submit(F f)
  {
    std::packaged_task<std::invoke_result_t<F>()> task(std::move(f));
    std::future<std::invoke_result_t<F>> res(task.get_future());
    if (local_work_queue)
    {
      local_work_queue->push(std::move(task));
    }
    else
    {
      pool_work_queue.push(std::move(task));
    }
    return res;
  }
};

    这可以避免数据竞争，但如果任务分配不均，就会导致某个线程的本地队列中有很多任务，而其他线程无事可做，为此应该让没有工作的线程可以从其他线程获取任务

class work_stealing_queue {
  std::deque<function_wrapper> the_queue;
  mutable std::mutex m;
 public:
  work_stealing_queue() {}
  work_stealing_queue(const work_stealing_queue&) = delete;
  work_stealing_queue& operator=(const work_stealing_queue&) = delete;

  void push(function_wrapper data) 
  {
    std::lock_guard<std::mutex> l(m);
    the_queue.push_front(std::move(data));
  }

  bool empty() const
  {
    std::lock_guard<std::mutex> l(m);
    return the_queue.empty();
  }

  bool try_pop(function_wrapper& res)
  {
    std::lock_guard<std::mutex> l(m);
    if (the_queue.empty()) return false;
    res = std::move(the_queue.front());
    the_queue.pop_front();
    return true;
  }

  bool try_steal(function_wrapper& res) 
  {
    std::lock_guard<std::mutex> l(m);
    if (the_queue.empty()) return false;
    res = std::move(the_queue.back());
    the_queue.pop_back();
    return true;
  }
};

class thread_pool {
  std::atomic<bool> done;
  thread_safe_queue<function_wrapper> pool_work_queue;
  std::vector<std::unique_ptr<work_stealing_queue>> queues;
  std::vector<std::thread> threads;
  threads_guard g;
  static thread_local work_stealing_queue* local_work_queue;
  static thread_local unsigned my_index;

  bool pop_task_from_local_queue(function_wrapper& task)
  {
    return local_work_queue && local_work_queue->try_pop(task);
  }

  bool pop_task_from_pool_queue(function_wrapper& task)
  {
    return pool_work_queue.try_pop(task);
  }

  bool pop_task_from_other_thread_queue(function_wrapper& task)
  {
    for (unsigned i = 0; i < queues.size(); ++i)
    {
      const unsigned index = (my_index + i + 1) % queues.size();
      if (queues[index]->try_steal(task)) return true;
    }
    return false;
  }

  void worker_thread(unsigned my_index_)
  {
    my_index = my_index_;
    local_work_queue = queues[my_index].get();
    while (!done)
    {
      function_wrapper task;
      if (pop_task_from_local_queue(task) ||
        pop_task_from_pool_queue(task) ||
        pop_task_from_other_thread_queue(task))
      {
        task();
      }
      else
      {  
        std::this_thread::yield();
      }
    }
  }
 public:
  thread_pool() : done(false), g(threads)
  {
    const unsigned n = std::thread::hardware_concurrency();
    try
    {
      for (unsigned i = 0; i < n; ++i)
      {
        queues.emplace_back(std::make_unique<work_stealing_queue>());
        threads.emplace_back(&thread_pool::worker_thread, this, i);
      }
    }
    catch(...)
    {
      done = true;
      throw;
    }
  }

  ~thread_pool() { done = true; }

  template<typename F>
  std::future<std::invoke_result_t<F>> submit(F f)
  {
    std::packaged_task<std::invoke_result_t<F>()> task(std::move(f));
    std::future<std::invoke_result_t<F>> res(task.get_future());
    if (local_work_queue)
    {
      local_work_queue->push(std::move(task));
    }
    else
    {
      pool_work_queue.push(std::move(task));
    }
    return res;
  }
};

thread_local work_stealing_queue* thread_pool::local_work_queue;
thread_local unsigned thread_pool::my_index;

中断

    可中断线程的简单实现

class interrupt_flag {
 public:
  void set();
  bool is_set() const;
};

thread_local interrupt_flag this_thread_interrupt_flag;

class interruptible_thread {
  std::thread t;
  interrupt_flag* flag;
 public:
  template<typename F>
  interruptible_thread(F f)
  {
    std::promise<interrupt_flag*> p;
    t = std::thread([f,&p] {
      p.set_value(&this_thread_interrupt_flag);
      f();
    });
    flag = p.get_future().get();
  }
  void interrupt()
  {
    if (flag) flag->set();
  }
};

void interruption_point()
{
  if (this_thread_interrupt_flag.is_set())
  {
    throw thread_interrupted();
  }
}

    在函数中使用

void f()
{
  while (!done)
  {
    interruption_point();
    process_next_item();
  }
}

    尽管这可以工作，但不是理想的，更好的方式是用std::condition_variable来唤醒，而非在循环中持续运行

class interrupt_flag {
  std::atomic<bool> flag;
  std::condition_variable* thread_cond;
  std::mutex m;
 public:
  interrupt_flag(): thread_cond(nullptr) {}

  void set()
  {
    flag.store(true, std::memory_order_relaxed);
    std::lock_guard<std::mutex> l(m);
    if (thread_cond) thread_cond->notify_all();
  }

  bool is_set() const
  {
    return flag.load(std::memory_order_relaxed);
  }

  void set_condition_variable(std::condition_variable& cv)
  {
    std::lock_guard<std::mutex> l(m);
    thread_cond = &cv;
  }

  void clear_condition_variable()
  {
    std::lock_guard<std::mutex> l(m);
    thread_cond = nullptr;
  }

  struct clear_cv_on_destruct
  {
    ~clear_cv_on_destruct()
    {
      this_thread_interrupt_flag.clear_condition_variable();
    }
  };
};

void interruptible_wait(std::condition_variable& cv, std::unique_lock<std::mutex>& l)
{
  interruption_point();
  this_thread_interrupt_flag.set_condition_variable(cv);
  interrupt_flag::clear_cv_on_destruct guard; // 下面的wait_for可能抛异常，所以需要RAII来清除标志
  interruption_point();
  cv.wait_for(l, std::chrono::milliseconds(1)); // 设置线程看到中断前的等待时间上限
  interruption_point();
}

template<typename Predicate>
void interruptible_wait(std::condition_variable& cv, std::unique_lock<std::mutex>& l, Predicate pred)
{
  interruption_point();
  this_thread_interrupt_flag.set_condition_variable(cv);
  interrupt_flag::clear_cv_on_destruct guard;
  while (!this_thread_interrupt_flag.is_set() && !pred())
  {
    cv.wait_for(l, std::chrono::milliseconds(1));
  }
  interruption_point();
}

    和std::condition_variable不同的是，std::condition_variable_any可以使用不限于std::unique_lock的任何类型的锁，这意味着可以使用自定义的锁类型

class interrupt_flag {
  std::atomic<bool> flag;
  std::condition_variable* thread_cond;
  std::condition_variable_any* thread_cond_any;
  std::mutex m;

 public:
  interrupt_flag() : thread_cond(nullptr), thread_cond_any(nullptr) {}

  void set()
  {
    flag.store(true, std::memory_order_relaxed);
    std::lock_guard<std::mutex> l(m);
    if (thread_cond)
    {
      thread_cond->notify_all();
    }
    else if (thread_cond_any)
    {
      thread_cond_any->notify_all();
    }
  }

  template<typename Lockable>
  void wait(std::condition_variable_any& cv, Lockable& l)
  {
    struct custom_lock {
      interrupt_flag* self;
      Lockable& l;

      custom_lock(interrupt_flag* self_, std::condition_variable_any& cond, Lockable& l_)
      : self(self_),l(l_)
      {
        self->m.lock();
        self->thread_cond_any = &cond;
      }

      void unlock()
      {
        l.unlock();
        self->m.unlock();
      }

      void lock()
      {
        std::lock(self->m, l);
      }

      ~custom_lock()
      {
        self->thread_cond_any = nullptr;
        self->m.unlock();
      }
    };
    custom_lock cl(this, cv, l);
    interruption_point();
    cv.wait(cl);
    interruption_point();
  }
  // rest as before
};

template<typename Lockable>
void interruptible_wait(std::condition_variable_any& cv, Lockable& l)
{
  this_thread_interrupt_flag.wait(cv, l);
}

    对于其他阻塞调用（比如mutex、future）的中断一般也可以像对std::condition_variable一样使用timeout，因为不访问内部mutex或future无法在未满足等待的条件时中断等待

template<typename T>
void interruptible_wait(std::future<T>& ft)
{
  while (!this_thread_interrupt_flag.is_set())
  {
    if (ft.wait_for(l, std::chrono::milliseconds(1)) == std::future_status::ready) break;
  }
  interruption_point();
}

    从被中断的线程角度来看，中断就是一个thread_interrupted异常。因此检查出中断后，可以像异常一样对其进行处理

internal_thread = std::thread([f, &p]{
  p.set_value(&this_thread_interrupt_flag);
  try
  {
    f();
  } // 异常传入std::thread的析构函数时将调用std::terminate
  catch(const thread_interrupted&){} // 为了防止程序终止就要捕获异常
});

    假如有一个桌面搜索程序，除了与用户交互，程序还需要监控文件系统的状态，以识别任何更改并更新其索引。
    为了避免影响GUI的响应性，这个处理通常会交给一个后台线程，后台线程需要运行于程序的整个生命周期。
    这样的程序通常只在机器关闭时退出，而在其他情况下关闭程序，就需要井然有序地关闭后台线程，一个关闭方式就是中断

std::mutex config_mutex;
std::vector<interruptible_thread> background_threads;

void background_thread(int disk_id)
{
  while (true)
  {
    interruption_point();
    fs_change fsc = get_fs_changes(disk_id);
    if (fsc.has_changes()) update_index(fsc);
  }
}

void start_background_processing()
{
  background_threads.emplace_back(background_thread, disk_1);
  background_threads.emplace_back(background_thread, disk_2);
}

int main()
{
  start_background_processing();
  process_gui_until_exit();
  std::unique_lock<std::mutex> l(config_mutex);
  for (auto& x : background_threads) x.interrupt();
  // 中断所有线程后再join
  for (auto& x : background_threads) x.join();
  // 不直接在一个循环里中断并join的目的是为了并发
  // 因为中断不会立即完成，它们必须进入下一个中断点，
  // 再在退出前必要地调用析构和异常处理的代码
  // 如果对每个线程都中断后立即join，就会造成中断线程的等待，
  // 即使它还可以做一些有用的工作，比如中断其他线程
}

09 并行算法

执行策略（execution policy）

    C++17对标准库算法重载了并行版本，区别是多了一个指定执行策略的参数

std::vector<int> v;
std::sort(std::execution::par, v.begin(), v.end());

    std::execution::par表示允许多线程并行执行此算法，注意这是一个权限（permission）而非强制要求（requirement），此算法依然可以被单线程执行
    另外，如果指定了执行策略，算法复杂度的要求也更宽松，因为并行算法为了利用好系统的并行性通常要做更多工作。
    比如把工作划分给100个处理器，即使总工作是原来的两倍，也仍然能获得原来的五十倍的性能
    <execution>中指定了如下执行策略类

std::execution::sequenced_policy
std::execution::parallel_policy
std::execution::parallel_unsequenced_policy
std::execution::unsequenced_policy // C++20

    并指定了对应的全局对象

std::execution::seq
std::execution::par
std::execution::par_unseq
std::execution::unseq // C++20

    如果使用执行策略，算法的行为就会受执行策略影响，影响方面包括：算法复杂度、抛异常时的行为、算法步骤的执行位置（where）、方式（how）、时刻（when）
    除了管理并行执行的调度开销，许多并行算法会执行更多的核心操作（交换、比较、使用函数对象等），这样可以减少总的实际消耗时间，从而全面提升性能。
    这就是算法复杂度受影响的原因，其具体改变因算法不同而异
    在不指定执行策略时，如下对算法的调用，抛出的异常会被传播

std::for_each(v.begin(), v.end(), [](auto x){ throw my_exception(); });

    而指定执行策略时，如果算法执行期间抛出异常，则行为结果由执行策略决定。如果有任何未捕获的异常，执行策略将调用std::terminate终止程序，唯一可能抛出异常的情况是，
    内部操作不能获取足够的内存资源时抛出std::bad_alloc。如下操作将调用std::terminate终止程序

std::for_each(std::execution::seq, v.begin(), v.end(), [](auto x){ throw my_exception(); });

    不同的执行策略的执行方式也不相同。执行策略会指定执行算法步骤的代理，可以是常规线程、矢量流、GPU线程或其他任何东西。
    执行策略也会指定算法步骤运行的顺序限制，比如是否要以特定顺序运行、不同算法步骤的一部分是否可以互相交错或并行运行等。下面对不同的执行策略进行详细解释

std::execution::sequenced_policy

    std::execution::sequenced_policy策略要求可以不（may not）并行执行，所有操作将执行在一个线程上。但它也是执行策略，因此与其他执行策略一样会影响算法复杂度和异常行为
    所有执行在一个线程上的操作必须以某个确定顺序执行，因此这些操作是不能互相交错的。但不规定具体顺序，因此对于不同的函数调用可能产生不同的顺序

std::vector<int> v(1000);
int n = 0;
// 下面把1~1000存入容器，但存入顺序是未指定的（可能顺序也可能乱序）
std::for_each(std::execution::seq, v.begin(), v.end(), [&](int& x){ x = ++n; });

    因此std::execution::sequenced_policy策略很少要求算法使用迭代器、值、可调用对象，它们可以自由地使用同步机制，可以依赖于同一线程上调用的操作，尽管不能依赖于这些操作的顺序

std::execution::parallel_policy

    std::execution::parallel_policy策略提供了基本的跨多个线程的并行执行，操作可以执行在调用算法的线程上，或执行在由库创建的线程上，
    在一个给定线程上的操作必须以确定顺序执行，并且不能相互交错。同样这个顺序是未指定的，对于不同的调用可能会有不同的顺序。一个给定的操作将在一个固定的线程上运行完整个周期
    因此std::execution::parallel_policy策略对于迭代器、值、可调用对象的使用就有一定要求，它们在并行调用时不能造成数据竞争，
    并且不能依赖于统一线程上的其他操作，或者说只能依赖于不运行在同一线程上的其他操作
    大多数情况都可以使用std::execution::parallel_policy策略

std::for_each(std::execution::par, v.begin(), v.end(), [](auto& x){ ++x; });

    只有在元素之间有特定顺序或对共享数据的访问不同步时，它才有问题

std::vector<int> v(1000);
int n = 0;
std::for_each(std::execution::par, v.begin(), v.end(),
  [&](int& x){ x = ++n; }); // 如果多个线程执行lambda就会对n产生数据竞争

    因此使用std::execution::parallel_policy策略时，应该事先考虑可能出现的未定义行为。可以用mutex或原子变量来解决竞争问题，但这就影响了并发性。
    不过这个例子只是为了阐述此情况，一般使用std::execution::parallel_policy策略时都是允许同步访问共享数据的

std::execution::parallel_unsequenced_policy

    std::execution::parallel_unsequenced_policy策略提供了最大可能的并行化，代价是对算法使用的迭代器、值和可调用对象有最严格的的要求
    使用std::execution::parallel_unsequenced_policy策略的算法允许以无序的方式在任意未指定的线程中执行，并且在每个线程中彼此不排序。
    也就是说，操作可以在单个线程上互相交错，同一线程上的第二个操作可以开始于第一个操作结束前，
    并且可以在线程间迁移，一个给定的操作可以开始于一个线程，运行于另一线程，而完成于第三个线程
    使用std::execution::parallel_unsequenced_policy策略时，提供给算法的迭代器、值、可调用对象上的操作不能使用任何形式的同步，也不能调用与其他代码同步的任何函数。
    这意味着操作只能作用于相关元素，或任何基于这些元素的可访问数据，并且不能修改任何线程间或元素间的共享数据

标准库并行算法

    <algorithm>和<numberic>中的大部分算法都重载了并行版本。std::accumlate没有并行版本，但C++17提供了std::reduce

auto res = std::accumulate(v.begin(), v.end(), 0);
auto res = std::reduce(std::execution::par, v.begin(), v.end());

    如果常规算法有并行版的重载，则并行版对常规算法原有的所有重载都有一个对应重载版本

template<class RandomIt>
void sort(RandomIt first, RandomIt last);

template<class RandomIt, class Compare>
void sort(RandomIt first, RandomIt last, Compare comp);

// 并行版对应有两个重载
template<class ExecutionPolicy, class RandomIt>
void sort(ExecutionPolicy&& policy, RandomIt first, RandomIt last);

template<class ExecutionPolicy, class RandomIt, class Compare>
void sort(ExecutionPolicy&& policy, RandomIt first, RandomIt last, Compare comp);

    但并行版的重载对部分算法有一些区别，如果常规版本使用的是输入迭代器（input iterator）或输出迭代器（output iterator），则并行版的重载将使用前向迭代器（forward iterator）

template<class InputIt, class OutputIt>
OutputIt copy(InputIt first, InputIt last, OutputIt d_first);

template<class ExecutionPolicy, class ForwardIt1, class ForwardIt2>
ForwardIt2 copy(ExecutionPolicy&& policy, ForwardIt1 first, ForwardIt1 last, ForwardIt2 d_first);

    输入迭代器只能用来读取指向的值，迭代器自增后就再也无法访问之前指向的值，它一般用于从控制台或网络输入，或生成序列，比如std::istream_iterator。
    同理，输出迭代器一般用来输出到文件，或添加值到容器，也是单向的，比如std::ostream_iterator
    前向迭代器返回元素的引用，因此可以用于读写，它同样只能单向传递，std::forward_list的迭代器就是前向迭代器，虽然它不可以回到之前指向的值，
    但可以存储一个指向之前元素的拷贝（比如std::forward_list::begin）来重复利用。对于并行性来说，可以重复利用迭代器很重要。
    此外，前向迭代器的自增不会使其他的迭代器拷贝失效，这样就不用担心其他线程中的迭代器受影响。如果使用输入迭代器，所有线程只能共用一个迭代器，显然无法并行
    std::execution::par是最常用的策略，除非实现提供了更符合需求的非标准策略。一些情况下也可以使用std::execution::par_unseq，
    虽然这不保证更好的并发性，但它给了库通过重排和交错任务来提升性能的可能性，不过代价就是不能使用同步机制，
    要确保线程安全只能让算法本身不会让多个线程访问同一元素，并在调用该算法的外部使用同步机制来避免其他线程对数据的访问

// 内部带同步机制只能使用std::execution::par，使用std::execution::par_unseq则会出现未定义行为
class A {
  mutable std::mutex m;
  int n = 0;
 public:
  int getVal() const
  {
    std::scoped_lock l(m);
    return n;
  }
  void inc()
  {
    std::scoped_lock l(m);
    ++n;
  }
};

void f(std::vector<A>& v)
{
  std::for_each(std::execution::par, v.begin(), v.end(), [](A& x){ x.inc(); });
}

// 如果要使用std::execution::par_unseq，则内部不能使用同步机制，同步机制应该在外部使用
class A {
  int n = 0;
 public:
  int getVal() const { return n; }
  void inc() { ++n; }
};

class B {
  std::mutex m;
  std::vector<A> v;
 public:
  void lock() { m.lock(); }
  void unlock() { m.unlock(); }
  std::vector<A>& getVec() { return v; }
};

void f(B& x)
{
  std::scoped_lock l(x);
  auto& v = x.getVec();
  std::for_each(std::execution::par_unseq, v.begin(), v.end(), [](A& x){ x.inc(); });
}

    下面是一个更实际的例子。假如有一个网站，访问日志有上百万条，为了方便查看数据需要对日志进行处理。对日志每行的处理是独立的工作，很适合使用并行算法

struct log {
  std::string page;
  time_t visit_time;
  // any other fields
};

extern log parse(const std::string& line);

using Map = std::unordered_map<std::string, unsigned long long>;

Map f(const std::vector<std::string>& v)
{
  struct combine {
    // log、Map两个参数有四种组合，所以需要四个重载
    Map operator()(Map lhs, Map rhs) const
    {
      if (lhs.size() < rhs.size()) std::swap(lhs, rhs);
      for (const auto& x : rhs) lhs[x.first] += x.second;
      return lhs;
    }
    Map operator()(log l, Map m) const
    {
      ++m[l.page];
      return m;
    }
    Map operator()(Map m, log l) const
    {
      ++m[l.page];
      return m;
    }
    Map operator()(log lhs, log rhs) const
    {
      Map m;
      ++m[lhs.page];
      ++m[rhs.page];
      return m;
    }
  };
  return std::transform_reduce(std::execution::par, v.begin(), v.end(),
    Map{}, // 初始值，一个空的map
    combine{}, // 结合两个元素的二元操作
    parse); // 对每个元素执行的一元操作
}

10 多线程应用的测试与调试

并发相关的bug类型

    与并发直接相关的bug一般可以分为两大类，一是unwanted blocking，二是race condition
    unwanted blocking包含以下几种情况
        死锁（deadlock）：两个线程互相等待，导致均无法完成工作。最明显的情况是，如果负责用户界面的线程死锁，界面将失去响应。
        也有一些情况是，界面可以保持响应，但一些任务无法完成，比如搜索不返回结果，或者文档不被打印
        活锁（livelock）：类似于死锁，不同的是线程不是阻塞等待，而是在忙碌于一个检查循环中，比如自旋锁。
        严重时，其表现的症状就和死锁一样，比如程序不进行，此外由于线程仍在运行，CPU会处于高使用率状态。
        在不太严重的情况下，活锁最终会被操作系统的随机调度解决，但仍然会造成任务的长时间延迟，并且延迟期间CPU使用率很高
        I/O阻塞或其他外部输入：如果线程阻塞等待外部输入，就无法继续处理工作。因此如果一个线程执行的任务会被其他线程等待，就不要让这个线程等待外部输入
    许多死锁和活锁都是由于race condition造成的，不过很大一部分race condition是良性的，比如要处理任务队列的下一个任务，决定用哪个工作线程去处理是无关紧要的。
    造成问题的race condtion包含以下几种情况
        数据竞争（data race）：数据竞争是一种特定类型的race condtion，由于对共享内存位置的不同步的并发访问，它将导致未定义行为。
        数据竞争通常发生于不正确地使用原子操作来同步线程，或者不加锁访问共享数据
        被破坏的不变量（broken invariant）：它可以表现为空悬指针（其他线程可以删除被访问的数据）、随机内存损坏（由于局部更新导致线程读取的值不一致）、
        双重释放（比如两个线程弹出队列的同一个数据）等。不变量的破坏是暂时的，因为它是基于值的。
        如果不同线程上的操作要求以一个特定顺序执行，不正确的同步就会导致race condition，有时就会违反这个执行顺序
        生命周期问题（lifetime issue）：这个问题可以归入broken invariant，但这里单独提出来。这个问题表现为，线程比其访问的数据活得更长。
        一般这个问题发生于线程引用了超出范围的局部变量，但也不仅限于此，比如调用join，要考虑异常抛出时，调用不被跳过
    通常可以通过调试器来确认死锁和活锁的线程以及它们争用的同步对象。对于数据竞争、不变量的破坏、生命周期问题，可见症状（如随机崩溃或不正确的输出）可以显示在代码的任何位置，
    代码可能重写系统其他部分使用的内存，并且很久以后才被触及，这个错误可能在程序执行的后期出现在与bug代码完全无关的位置。
    这就是共享内存的真正祸端，无论如何限制线程对数据的访问和确保正确的同步，任何线程都可以重写其他线程中的数据

定位bug的方法
code review

    让其他人来审阅代码，因为其他人对代码不熟悉，需要思考代码的工作方式，看待的角度也不一样，更有可能发现潜在的问题。
    也可以过一段时间自己再看这些代码，能起到相同的效果。审阅多线程代码时，一般要思考以下问题
        Which data needs to be protected from concurrent access?
        How do you ensure that the data is protected?
        Where in the code could other threads be at this time?
        Which mutexes does this thread hold?
        Which mutexes might other threads hold?
        Are there any ordering requirements between the operations done in this thread and those done in another? How are those requirements enforced?
        Is the data loaded by this thread still valid? Could it have been modified by other threads?
        If you assume that another thread could be modifying the data, what would that mean and how could you ensure that this never happens?

测试

    测试多线程程序的困难在于，具体的线程调度顺序是不确定的，对于相同的输入，得到的结果却不一定相同，结果可能有时是正确的，有时是错误的。
    因此存在潜在的race condition也不意味着总会得到失败的结果，有时可能也会成功
    由于重现并发相关的bug很困难，所以值得仔细设计测试。最好让每个测试运行最小数量的代码，这样在测试失败时可以最好地隔离出错误代码。
    比如测试一个并发队列，分别测试并发的push和pop的工作，就直接比测试整个队列的功能要好
    为了验证问题是否与并发相关，应该从测试中消除并发性。多线程中的bug并不意味着一定是并发相关的，如果一个问题在单线程中也总是出现，这就是一个普通的bug，而不是并发相关的bug。
    如果一个问题在单核系统中消失，而在多核或多处理器系统中总会出现，一般这就可能是一个race condition，或同步、内存序相关的问题
    如果要测试并发队列，需要考虑以下其情况
        One thread calling push() or pop() on its own to verify that the queue works at a basic level
        One thread calling push() on an empty queue while another thread calls pop()
        Multiple threads calling push() on an empty queue
        Multiple threads calling push() on a full queue
        Multiple threads calling pop() on an empty queue
        Multiple threads calling pop() on a full queue
        Multiple threads calling pop() on a partially full queue with insufficient items for all threads
        Multiple threads calling push() while one thread calls pop() on an empty queue
        Multiple threads calling push() while one thread calls pop() on a full queue
        Multiple threads calling push() while multiple threads call pop() on an empty queue
        Multiple threads calling push() while multiple threads call pop() on a full queue
    考虑完以上情况之外，接着就要考虑测试环境的因素
        What you mean by “multiple threads” in each case (3, 4, 1,024?)
        Whether there are enough processing cores in the system for each thread to run on its own core
        Which processor architectures the tests should be run on
        How you ensure suitable scheduling for the “while” parts of your tests
    一般满足以下条件的代码就是易于测试的，这些条件单线程和多线程中同样适用
        每个函数和类的责任是清晰的
        函数简明扼要（short and to the point）
        测试可以完全控制被测试代码的所在环境
        执行特定操作的被测试代码在系统中是紧密而非分散的
        代码在写下之前已被考虑过如何测试
    为了测试设计并发代码的一个最好方法是消除并发，如果可以把代码分解成负责线程间通信路径的部分，以及在单线程中操作通信数据的部分，就可以极大地简化问题。
    对于操作通信数据的部分就可以用常规的单线程技术测试，对于负责线程间通信的部分，代码小了很多，测试也更容易

多线程测试技术

    第一种测试技术是蛮力测试（也叫压力测试），随着代码运行次数的增加，bug出现的几率也更高，如果代码运行十亿次都通过，代码就很可能是没有问题的。
    如果测试是细粒度的（fine-grained），比如前面对并发队列的测试，蛮力测试就更可靠。如果粒度非常大，可能的组合也非常多，即使十亿次的测试的结果也不算可靠
    蛮力测试的缺点是，如果测试本来就保证了问题不会发生，那么无论测试多少次都不会出现失败的情况，这就会造成误导。
    比如在单核系统上测试多线程程序，race condition和乒乓缓存的问题根本不会出现，但这不表示这个程序在多核系统上是没问题的。
    又比如，不同处理器架构提供了不同的同步和内存序工具，在x86和x86-64架构上，无论使用memory_order_relaxed还是memory_order_seq_cst内存序，原子load操作总是一样的，
    这意味着在x86架构上使用relaxed语义总是可行的，但如果换成细粒度内存序指令的系统（比如SPARC）就会失败
    第二种测试技术是组合仿真测试（combination simulation testing），即使用一个特殊的软件来仿真真实的运行时环境。
    仿真软件将记录数据访问、锁定、原子操作的序列，然后使用C++内存模型的规则来重复运行所有可能的操作组合，以确定race condition和死锁
    虽然这种详尽的组合测试可以保证找到设计所要检测的所有问题，但会花费大量时间，因为组合的数量随线程 数和每个线程执行的操作数呈指数增长，
    它最好用于单个代码片段的细粒度测试，而非用于整个程序。这种技术的另一个明显缺点是，它要求访真软件能处理代码中的操作
    第三种测试技术是使用专门的库。比如共享数据通常会用mutex保护，如果在访问数据时能检查哪些mutex被锁定了，就能验证线程在访问数据时是否锁定了相应的mutex，
    如果没有锁定就报告失败。库实现也能记录上锁的顺序，如果另一个线程对同一个mutex以不同顺序上锁，这就会被记录为潜在的死锁
    另一种类型的库是，同步原语的实现允许测试编写者在多线程等待时，可以控制哪个线程来获得锁，或者哪个线程被notify_one通知。
    这就允许设置特定方案，来验证代码是否在这些方案中按预期运行
    一些测试工具已经作为标准库实现的一部分提供了，其他的则可以基于标准库的部分手动实现

构建多线程测试代码

    多线程测试代码可以分为以下几部分
        必须先执行的总体设置
        必须运行在每个线程上的线程特定的设置
        要并发运行在每个线程上的代码
        并发执行结束后的状态断言
    如下是对一个队列的测试代码

void test_concurrent_push_and_pop_on_empty_queue()
{
  thread_safe_queue<int> q; // 总体设置：先创建一个队列
  std::promise<void> go, push_ready, pop_ready;
  std::shared_future<void> ready(go.get_future());
  std::future<void> push_done;
  std::future<int> pop_done;
  try
  {
    push_done = std::async(std::launch::async, // 指定异步策略保证每个任务运行在自己的线程上
      [&q, ready, &push_ready]()
      {
        push_ready.set_value();
        ready.wait();
        q.push(42); // 线程特定的设置：存入一个int
      }
    );
    pop_done = std::async(std::launch::async,
      [&q, ready, &pop_ready]()
      {
        pop_ready.set_value();
        ready.wait();
        return q.try_pop();
      }
    );
    push_ready.get_future().wait(); // 等待开始测试的通知
    pop_ready.get_future().wait(); // 同上
    go.set_value(); // 通知开始真正的测试
    push_done.get(); // 获取结果
    assert(pop_done.get() == 42); // 获取结果
    assert(q.empty());
  }
  catch(...)
  {
    go.set_value(); // 避免空悬指针
    throw; // 再抛出异常
  }
}

测试多线程代码的性能

    使用并发的一个主要目的就是利用多核处理器来提高程序性能，因此测试代码来确保性能确实提升了是很重要的。
    性能相关的一个主要方面就是可扩展性，性能应该随着核数一起提升。在测试多线程代码性能时，最好在尽可能多的不同配置上进行测试

本书大多未详细解释的基础知识可参考《现代操作系统》：进程与线程、内存管理、文件系统、I/O、死锁

进程

    在进程模型中，计算机上所有可运行的软件，通常也包括操作系统，被组织成若干顺序进程（sequential process），简称进程（process），
    一个进程就是就是一个正在执行程序的实例，包括程序计数器、寄存器和变量的当前值
    概念上来说，每个进程有自己的虚拟CPU，但实际上真正的CPU（假设只有一个CPU）在各进程之间来回切换，同一时刻实际只有一个进程在运行
    实际只有一个物理程序计数器。每个进程运行时，它的逻辑程序计数器被装入实际的程序计数器。当进程结束时，物理程序计数器保存到内存中该进程的逻辑程序计数器中
    进程创建主要有四种形式
        系统初始化：启动系统时会创建若干进程，包括和用户交互的前台进程和停在后台的守护进程，守护进程可以通过UNIX的ps指令或Window的任务管理器查看
        运行中的程序执行创建进程的系统调用：比如启动一个程序，该程序要启动更多进程来分配任务
        用户请求创建一个新进程：比如用户双击图标启动程序
        大型机批处理作业的初始化
    创建进程的系统调用在UNIX中是fork，在Windows中是CreateProcess，进程创建后，父子进程有不同的地址空间
    进程终止通常也有四种形式
        正常退出（自愿的）：比如点击浏览器的关闭图标。进程退出的系统调用在UNIX中是exit，在Windows中是ExitProcess
        出错退出（自愿的）：比如执行cc foo.c编译foo.c而该文件不存在
        严重错误（非自愿）：比如执行非法指令、引用不存在的内存、除数是零，UNIX中会希望自行处理这些错误以通知操作系统，进程会收到信号被中断而非终止
        被其他进程杀死（非自愿）：UNIX中是kill，Windows中是TerminateProcess
    UNIX中，进程和其所有子进程（包括其后裔）组成一个进程组，当用户发出一个键盘信号，该信号会发送给进程组所有成员
    Windows中没有进程层次的概念，所有进程地位相同
    进程阻塞有两种情况，一是正常情况，比如操作系统调度另一个进程占用CPU，二是异常情况，比如没有足够的CPU可调用
    进程有三种状态：运行、就绪、阻塞

运行 <-> 就绪
  K    J
    阻塞

运行：该时刻实际占用CPU
就绪：操作系统调度了其他进程运行而暂时停止
阻塞：逻辑上不能继续运行，比如等待用户输入

    操作系统通过维护一张进程表（一个结构数组）来实现进程模型，每个进程占一个表项（即进程控制块，Processing Control Block）。PCB包含了进程状态的主要信息，如程序计数器、堆栈指针、内存分配状态、所打开的文件状态、账号和调度信息、进程状态切换时必须保存的信息
    所有中断都从保存寄存器开始，通常会保存到当前进程的PCB中。一个进程在执行过程中可能中断几千次，但恢复时，被中断的进程都将返回到与中断发生前完全相同的状态
    发生中断后，操作系统最底层的工作过程
        中断硬件将程序计数器、程序状态字、寄存器压入堆栈
        硬件从中断向量装入新的程序计数器
        通过汇编保存寄存器值（因为这类操作无法用高级语言完成）
        通过汇编设置新的堆栈
        运行C语言（假设操作系统用C编写）中断服务例程
        调用调度程序，决定接下来要运行的进程
        C返回到汇编
        通过汇编运行新进程
    假设一个进程等待I/O操作与其在内存中停留的时间比为p，则n个进程都在等待（此时CPU空转）的概率为p^n，CPU利用率为1-p^n，因此一般（该模型只是粗略情况）I/O时间越短、运行进程越多，CPU利用率越高

假如内存为8G，操作系统和相关表格占2G，用户程序也占2G，内存最多容纳3个用户程序
假设80%时间用于等待I/O操作
CPU利用率 = 1 - 0.8 ^ 3 = 49%
如果增加8G内存，则最多容纳7个用户程序
CPU利用率 = 1 - 0.8 ^ 7 = 79%，吞吐量提高为79% - 49% = 30%
如果再增加8G内存，则最多容纳11个用户程序
CPU利用率 = 1 - 0.8 ^ 11 = 91%，吞吐量只提高了12%，可见第一次增加内存比较划算

线程

    正如进程提供的抽象使得避免了对中断、定时器、上下文切换的考虑，多线程提供了一种新抽象，即并行实例共享同一地址空间和所有可用数据，这正是多进程模型（地址空间不同）无法表达的
    第二个需要多线程的理由是，线程更轻量，创建和撤销都更快（通常创建一个线程比创建一个进程快10-100倍）
    第三个理由是多CPU系统中，多线程为真正的并行提供了可能
    线程包含一个程序计数器（记录接下来要执行哪一条指令）、寄存器（保存线程当前的工作变量）、堆栈指针（记录执行历史，每个线程的堆栈有一帧，每一帧保存一个已调用但还未返回的过程，如局部变量、返回地址）
    各线程可以访问进程地址空间的每一个内存地址，因此一个线程可以读写甚至清除另一个线程的堆栈。线程之间没有保护，因为不可能，也没必要
    除了共享地址空间，线程还共享同一个打开文件集、子进程、定时器及相关信号量
    线程可以处在运行、就绪、阻塞、终止等状态中的任何一个
    thread_yield允许线程自动放弃CPU转让给另一个线程运行，提供这个调用是因为，不同于进程，线程库不能利用时钟中断强制线程让出CPU
    实现线程包主要有两种方式，一是用户级线程（User-Level Thread），二是内核级线程（Kernel-Level Thread），另外也有混合实现
    用户级线程把整个线程包放在用户空间中，内核对其一无所知，不需要内核支持，可以在不支持线程的操作系统上实现。在用户空间管理线程时，每个进程需要有其专用的线程表（thread table），这些表和内核中的进程表类似，只不过记录的是各个线程的属性，如程序计数器、寄存器、堆栈指针和状态等。该线程表由运行时系统管理，当线程转换到就绪或阻塞状态时，在线程表中存放重启该线程所需的信息，与内核在进程表中存放进程的信息完全一样
    用户级线程允许进程有自己定制的调度算法，具有更好的可扩展性（因为内核级线程需要一些固定表格空间和堆栈空间），性能更好。用户级线程的切换需要少量机器指令，而内核级线程需要完整的上下文切换，修改内存映像，使高速缓存失效，这导致了若干数量级的延迟
    用户级线程的问题是如何实现阻塞系统调用，比如线程读取键盘，在没有按下任何按键之前不能让该线程实际进行该系统调用，因为这会停止所有线程。另一个问题是，如果一个线程开始运行，则其所在进程的其他线程就不能运行，除非运行线程自动放弃CPU。而使用内核级线程时，线程阻塞在I/O上时，不需要将整个进程挂起
    内核级线程的线程表（和用户级线程的线程表一样，记录寄存器、状态和其他信息）存在于内核中，当一个线程希望创建一个新线程或撤销一个已有线程时，将进行一个系统调用，这个系统调用通过对线程表的更新完成创建或撤销工作
    当内核级线程阻塞时，内核可以运行同一进程中的另一线程，或者运行另一个进程的线程。而对于用户级线程，运行时系统始终运行其所在进程的线程，直到内核剥夺CPU（或没有可运行的线程存在）为止
    在内核中创建或撤销线程的代价较大，因此内核级线程被撤销时，系统会将其标记为不可运行的，但其内核数据结构未受影响，之后必须创建新线程时就重新启动一个旧线程。用户级线程也可以这样回收，但因为管理代价很小，所以没必要

进程间通信（Inter Process Communication）

    对共享内存进行访问的程序片段称为临界区（critical region、critical section），如果同一时刻临界区只有一个进程，就能避免race condition
    单处理器系统中实现这点的简单做法是，在每个进程刚进入临界区后立即屏蔽所有中断，在即将离开时再打开中断。屏蔽中断后，时钟中断也被屏蔽。CPU只有发生时钟中断或其他中断才会进行进程切换，这样CPU就不会切换到其他进程
    但这个方案并不好，因为把屏蔽中断的权力交给用户进程是不明智的，如果一个进程屏蔽中断后不打开，就可能导致整个系统终止。此外如果系统是多处理器，则屏蔽中断只对执行了disable指令的CPU有效，其他CPU仍将运行
    对于内核来说，更新变量或列表的几条指令期间屏蔽中断很方便，因此屏蔽中断对操作系统本身是一项很有用的技术，但对用户进程则不是一种合适的互斥机制
    第二种方式是一种软件方案，假设有一个共享锁变量，其初始值为0，当进程要进入临界区时，首先测试锁，如果值为0则将锁设为1并进入临界区，如果锁的值已经为1，则进程等待其值为0
    这种方式的问题在于，如果在一个进程检查到锁为0，并要将锁设为1之前，恰好另一个线程被调度运行将锁设为1，而第一个进程恢复运行时也将把锁设为1并进入临界区，此时临界区就有了两个进程
    第三种方式是忙等待（busy waiting），用一个循环不断测试变量值，直到变量值改变才进入临界区，用于忙等待的锁称为自旋锁（spin lock）。这种方式的问题是，在循环中浪费了大量CPU时间，应该避免，除非等待时间非常短才有使用的理由

// 进程A
while (true)
{
    while (x) {}
    critical_region();
    x = true; // 允许进程B进入临界区
    noncritical_region();
}

// 进程B
while (true)
{
    while (!x) {}
    critical_region();
    x = false; // 允许进程A进入临界区
    noncritical_region();
}

    第四种方式是1981年由G. L. Peterson提出的Peterson算法

constexpr int N = 2; // 进程数量为2
int turn = 0; // 轮到的进程
vector<bool> interested(N);

void enter_region(int process)
{
    int other = 1 - process; // 另一进程（进程号为0或1）
    interested[process] = true;
    turn = process; // turn只有一个，因此即使两个进程都调用，只有后一个赋值会保留
    while (turn == process && interested[other]) {}
}

void leave_region(int process) // 当调用上述函数完成后调用此函数
{
    interested[process] = false;
}

// 若进程A调用enter_region则很快返回，此时进程B调用将在while循环挂起，直到进程A调用leave_region
// 若进程AB同时调用enter_region，turn为后赋值者，则先赋值者退出循环并调用leave_region，后赋值者再退出循环

    第五种方式是一种硬件方式，需要借助TSL指令，即测试并加锁（test and set lock），该指令是一个原子操作，执行TSL指令的CPU将锁住内存总线以禁止其他CPU在指令结束前访问该内存

TSL RX, LOCK // 将内存字LOCK读到寄存器RX中，然后在该内存地址写一个非零值，读写是原子操作

    为了使用TSL指令实现互斥，用一个共享变量LOCK来协调对内存的访问，其值为0时任何进程都能用TSL指令将值设为1并读写共享内存，操作结束时再用move指令将值重置为0

enter_region:
    TSL REGISTER, LOCK // 复制锁到寄存器并设置值为1
    CMP REGISTER, #0 // 值为0？
    JNE enter_region // 不是0则循环
    RET // 返回，进入临界区

leave_region:
    MOVE LOCK, #0
    RET

    也可以用可XCHG指令替代TSL指令，它原子交换两个位置的内容

enter_region:
    MOVE REGISTER, #1 // 在寄存器放一个1
    XCHG REGISTER, LOCK // 原子交换寄存器和锁变量的内容
    CMP REGISTER, #0 // 值为0？
    JNE enter_region // 不是0则循环
    RET // 返回，进入临界区

leave_region:
    MOVE LOCK, #0
    RET

    Peterson算法和TSL或XCHG解法同样都有忙等待的问题，它们的本质都是在进程进入临界区时检查是否允许进入，不允许则原地等待直到允许为止

生产者-消费者问题

    两个进程共享一个固定大小的缓冲区，生产者进程将消息放入缓冲区，消费者进程从缓冲区取出消息

constexpr int N = 100; // 缓冲区的槽数
int cnt = 0; // 缓冲区数据数

void producer()
{
    while (true)
    {
        int item = produce_item(); // 生成新数据
        if (cnt == N) sleep();
        insert_item(item); // 将消息放入缓冲区
        ++cnt; // 1
        if (cnt == 1) wakeup(consumer); // 2
    }
}

void consumer()
{
    while (true)
    {
        if (!cnt) sleep(); // 3
        int item = remove_item(); // 从缓冲区取一个数据
        --cnt;
        if (cnt == N - 1) wakeup(producer);
        consume_item(item); // 打印数据
    }
}

// 问题在于cnt的访问存在race condition，如果消费者进程执行到3处，cnt为0，在即将sleep之前
// 生产者进程在此之后才执行到1处，此时cnt为1，执行到2处，调用wakeup
// 但此时消费者进程还未sleep，因此wakeup的信号丢失，没有实际作用
// 接着消费者进程sleep，生产者进程开始下一轮循环
// 生产者进程下一轮循环到1处，cnt为2，到2处，将不再调用wakeup，于是消费者进程仍然保持sleep
// 生产者进程继续之后的循环，并且每一轮都不会唤醒消费者进程
// 最终生产者进程执行到cnt为N时进入sleep，两个进程都将永久sleep

信号量（semaphore）

    信号量是由E. W. Dijkstra于1965年提出的一种方法，它使用一个整型变量作为信号量，值为0表示没有保存下来的唤醒操作，值为正数表示唤醒操作的次数
    信号量有down和up两种操作，Dijkstra在原来的论文中称为P和V操作（荷兰语中的Proberen意为尝试，Verhogen意为增加或升高）
    down操作检查值是否大于0，若大于0则减1并继续，若为0则进程睡眠，并且此时down操作未结束
    up操作对值加1。如果有进程在信号量上睡眠，无法完成一个先前的down操作，则由系统选择其中一个以允许完成其down操作。于是，对一个有睡眠进程的信号量执行一次up操作，信号量值仍为0，但睡眠进程少了一个
    down操作和up操作中的所有操作都是原子的，一般作为系统调用实现。操作系统只要在执行测试信号量、更新信号量、使进程睡眠等操作时暂时屏蔽全部中断，这些动作只需要几条指令，所以屏蔽中断不会带来什么副作用。如果使用多个CPU，则每个信号量应由一个一个锁保护，使用TSL或XCHG指令来确保同一时刻只有一个CPU对信号量进行操作
    注意，这里使用TSL或XCHG指令来防止多CPU同时访问一个信号量，与生产者或消费者用忙等待来等待对方腾出或填充缓冲区是完全不同的。信号量操作只需要几毫秒，而生产者或消费者则可能需要任意长时间
    使用三个信号量解决生产者-消费者问题：full记录已充满的缓冲槽数，初值为0；empty记录空的缓冲槽数，初值为缓冲区中槽的数目；mutex确保生产者和消费者不会同时访问缓冲区，初值为1
    供多个进程使用的信号量初值为1，保证同时只有一个进程可以进入临界区，这种信号量称为二元信号量（binary semaphore）。如果每个进程进入临界区前执行一个down操作，并在刚退出时执行一个up操作，就能实现互斥

constexpr int N = 100; // 缓冲区的槽数
using semaphore = int;
semaphore mutex = 1;
semaphore empty = N; // 缓冲区空槽数
semaphore full = 0; // 缓冲区满槽数

void producer()
{
    while (true)
    {
        int item = produce_item();
        down(&empty);
        down(&mutex);
        insert_item(item);
        up(&mutex);
        up(&full);
    }
}

void consumer()
{
    while (true)
    {
        down(&full);
        down(&mutex);
        int item = remove_item();
        up(&mutex);
        up(&empty);
        consume_item(item);
    }
}

    信号量的另一个作用是实现同步（synchronization），这里full和empty保证缓冲区满时生产者停止运行，缓冲区空时消费者停止运行

互斥量（mutex）

    如果不需要信号量的计数功能，可以使用其称为互斥量的简化版本。互斥量仅适用于管理共享资源或一小段代码。互斥量实现简单且有效，在实现用户空间线程包时十分有用
    互斥量只有加锁和解锁两种状态，只需要一个二进制位表示，不过实际上一般用整型量，0表示解锁，其他值表示加锁
    线程需要访问临界区时调用mutex_lock，如果互斥量是解锁的则临界区可用，调用成功，线程可以进入临界区，否则线程被阻塞，直到临界区中的线程完成并调用mutex_unlock。如果多个线程阻塞在该互斥量上，则随机选择一个线程并允许它获得锁
    用TSL或XCHG指令就可以很容易地在用户空间实现互斥量

mutex_lock:
    TSL REGISTER, MUTEX // 将互斥量复制到寄存器，并将互斥量置为1
    CMP REGISTER, #0
    JZE ok // 如果互斥量为0，它被解锁，所以返回
    CALL thread_yield // 互斥量忙，调度另一个线程
    JMP mutex_lock // 稍后再试
ok: RET

mutex_unlock:
    MOVE MUTEX, #0 // 将互斥量置0
    RET

    thread_yield只是调用用户空间线程调度程序，运行十分快捷，这样mutex_lock和mutex_unlock都不需要任何内核调用。用户级线程通过互斥量的这个过程即可实现同步，而同步过程仅需要少量指令

管程（monitor）

    如果把生产者代码中的两个down操作交换顺序，使得mutex在empty之前减1，就会导致死锁，因此使用信号量要十分小心。为了更易于编写正确的程序，Brinch Hansen和Hoare提出了一种称为管程的高级同步原语
    一个管程是由过程、变量、数据结构等组成的一个集合，它们组成一个特殊的模块或软件包，进程可以在任何需要的时候调用管程中的过程，但不能在管程之外声明的过程中直接访问管程内的数据结构
    任一时刻管程中只能有一个活跃进程，这一特性使得管程能有效地完成互斥。管程是编程语言的组成部分，编译器知道其特殊性，进入管程时的互斥由编译器负责，通常做法是使用互斥量或二元信号量。这样就不需要程序员安排互斥，出错的可能性就小很多
    管程提供了互斥的简便途径，但此外还需要一种方法使得进程在无法继续运行时被阻塞，这个方法就是引入条件变量（condition variable）
    当一个管程过程发现它无法继续运行时（如生产者发现缓冲区满），则会在某个条件变量（如full）上执行wait操作，该操作将阻塞当前进程，并将另一个在管程外的进程调入管程。另一个进程可以通过对同一条件变量执行signal操作唤醒阻塞进程
    为了避免管程中有两个活跃进程，执行signal操作之后有两种规则。Hoare建议让新唤醒的进程运行，挂起另一个进程。Brinch Hansen建议执行signal的进程必须立即退出管程，即signal语句只能作为一个管程过程的最后一条语句。后者在概念上更简单，并且更容易实现。第三种方法是，让发信号者继续运行，直到其退出管程，才允许等待的进程开始运行
    如果一个条件变量上有若干进程正在等待，则对其执行signal操作之后，系统调度程序只能选择其中一个恢复运行
    如果一个条件变量没有等待进程，则对其执行signal会丢失信号，因此wait操作必须在signal之前。这与之前提到的sleep和wakeup的关键区别是，管程的自动互斥保证了在wait完成之前不会先signal

消息传递（message passing）

    管程和信号量通过共享内存解决CPU互斥问题，但没有提供不同机器间（比如局域网中的机器）的信息交换方法
    消息传递使用send和receive原语来实现进程间通信，它们像信号量而不像管程，是系统调用而非语言成分

send(destination, &message);
receive(source, &message);

    send向一个给定目标发送一条消息，receive从一个给定源（或者任意源）接收一条消息，如果没有消息可用则接收者可能被阻塞直至有一条消息到达，或者带着一个错误码立即返回
    消息传递系统面临许多设计难点：比如消息可能被网络丢失，需要三次握手来确认信息到达情况；比如发送方未收到确认，因此重发消息导致接收方收到两条相同消息，接收方需要区分新老消息；比如身份认证（authentication）问题，客户端如何确认通信的是一个文件服务器还是冒充者
    消息传递方式可以有许多变体，一种对消息进行编址的方式是，为每个进程分配一个唯一地址，让消息按进程的地址编址。另一种方式是引入一种称为信箱（mailbox）的数据结构，用来对一定数量的消息进行缓冲。使用信箱时，send和receive调用的地址参数就是信箱而非进程的地址

constexpr int N = 100;

void producer()
{
    message m; // 消息缓冲区

    while (true)
    {
        int item = produce_item();
        receive(consumer, &m); // 等待消费者发送空缓冲区
        build_message(&m, item); // 建立一个待发送的消息
        send(consumer, &m); // 发送数据项给消费者
    }
}

void consumer()
{
    message m;

    for (int i = 0; i < N; ++i)
    {
        send(producer, &m); // 发送N个空缓冲区
    }

    while (true)
    {
        receive(producer, &m); // 接收包含数据项的消息
        int item = extract_item(&m); // 将数据项从消息中提取出来
        send(producer, &m); // 将空缓冲区发送回生产者
        consume_item(item);
    }
}

    使用信箱的另一种极端方法是彻底取消缓冲。采取这种方法时，如果send在receive之前执行则发送进程被阻塞，直到receive发生，反之亦然。执行receive时，消息可以直接从发送者复制到接收者，不用任何中间缓冲。这种方案常被称为会和（rendezvous），实现起来更容易，但降低了灵活性，因为发送者和接收者一定要以步步紧接的方式运行
    通常在并行程序设计系统中使用消息传递，一个著名的消息传递系统是消息传递接口（Message-Passing Interface，MPI），它广泛应用于科学计算

屏障（barrier）

    屏障是一种用于进程组的同步机制，只有所有进程就绪时才能进入下一阶段。每个阶段的结尾设置一个屏障，当一个进程到达屏障时将被阻拦，直到所有进程到达屏障为止

调度

    几乎所有进程的I/O请求和计算都是交替突发的，如果进程花费大量时间在计算上，则称为计算密集型（compute-bound），如果大量时间花费在等待I/O上，则称为I/O密集型（I/O-bound） *随着CPU变得越来越快，更多的进程倾向为I/O密集型。这种现象的原因是CPU的改进比磁盘的改进快得多，所以未来对I/O密集型进程的调度处理更为重要
    调度的基本思想是，如果需要运行I/O密集型进程，就应该让它尽快得到机会，以便发出磁盘请求并保持磁盘始终忙碌
    根据如何处理时钟中断，可以把调度算法分为非抢占式和抢占式两类
    非抢占式调度算法挑选一个进程，然后让该进程运行直至阻塞，或直到该进程自动释放CPU。即使该进程运行了几个小时也不会被强迫挂起，这样导致时钟中断发生时不会进行调度。在处理完时钟中断后，如果没有更高优先级的进程，则被中断的进程将继续运行
    抢占式调度算法挑选一个进程，让该进程运行某个固定时段的最大值，时段结束时将挂起该进程，并挑选另一个进程运行。抢占式调度需要在时间间隔的末端发生时钟中断，以便把CPU控制返回给调度程序，如果没有可用的时钟，就只能选择非抢占式调度
    不同的应用领域有不同的目标，也就需要不同的调度算法。环境可以划分为三种
        批处理：广泛用于商业领域，比如处理薪水清单、账目收入、账目支出、利息计算，批处理系统不会有用户在旁边急切等待响应，因此通常使用非抢占式算法，或对每个进程都有长时间周期的抢占式算法，这样减少了进程切换从而改进了性能
        交互式：必须使用抢占式算法，以避免CPU被一个进程霸占而拒绝为其他进程服务。服务器也归于此类，因为通常要服务多个突发的远程用户
        实时：有时不需要抢占，因为进程了解它们可能会长时间得不到运行，所以通常很快地完成各自工作并阻塞

调度算法的评价指标

    对于批处理系统，调度算法的评价指标主要有三个
        吞吐量（throughout）：系统单位时间内完成的作业数量，比如10道作业花费100秒，则吞吐量为0.1道/秒
        周转时间（turnaround time）：一个批处理作业从提交开始到完成的统计平均时间
        CPU利用率：CPU忙碌时间相对总时间的占比
    对于交互式系统，评价指标最重要的是最小响应时间，即从发出命令到得到响应之间的时间
    实时系统的特点是或多或少必须满足截止时间，多数实时系统中，可预测性十分重要，比如如果多媒体实时系统的音频进程运行错误太多，音质就会明显下降，为此实时系统的调度算法必须是高度可预测和有规律的

批处理系统中的调度
先来先服务（First-Come First-Served，FCFS）

    非抢占式。进程按照请求CPU的先后顺序调度，优点是公平，算法实现简单，不会导致进程饥饿（Starvation，等待时间对进程响应带来明显影响）

进程 到达时间 运行时间
P1   0        7
P2   2        4
P3   4        1
P4   5        4

先到先服务，因此调度顺序为P1 -> P2 -> P3 -> P4
P1      P2   P3 P4
------- ---- -  ----

周转时间 = 完成时间 - 到达时间
P1 = 7 - 0 = 7
P2 = 11 - 2 = 9
P3 = 12 - 4 = 8 // 只运行1，却需要等待8，可见FCFS算法对短作业不利
P4 = 16 - 5 = 11
平均周转时间 = 8.75

带权周转时间 = 周转时间 / 运行时间
P1 = 7 / 7 = 1
P2 = 9 / 4 = 2.25
P3 = 8 / 1 = 8
P4 = 11 / 4 = 2.75
平均带权周转时间 = 3.5

等待时间 = 周转时间 - 运行时间（不考虑等待I/O操作的时间）
P1 = 7 - 7 = 0
P2 = 9 - 4 = 5
P3 = 8 - 1 = 7
P4 = 11 - 4 = 7
平均等待时间 = 4.75

最短作业优先（Shortest Job First，SJF）

    非抢占式。选择已到达的且运行时间最短的进程，运行时间相同则先到达的先运行。目标是追求最短的平均周转时间、平均带权周转时间、平均等待时间，缺点是不公平，对短作业有利，对长作业不利，如果一直有短作业到达可能导致长作业饥饿

进程 到达时间 运行时间
P1   0        7
P2   2        4
P3   4        1
P4   5        4

P1先到达，P1运行结束时P2、P3、P4均到达，P3运行时间最短先运行
P2、P4运行时间相同，P2先到达，因此P2先于P4运行

最终调度顺序为P1 -> P3 -> P2 -> P4
P1      P3 P2    P4
------- -  ----  ----

周转时间 = 完成时间 - 到达时间
P1 = 7 - 0 = 7
P2 = 12 - 2 = 10
P3 = 8 - 4 = 4
P4 = 16 - 5 = 11
平均周转时间 = 8

带权周转时间 = 周转时间 / 运行时间
P1 = 7 / 7 = 1
P2 = 10 / 4 = 2.5
P3 = 4 / 1 = 4
P4 = 11 / 4 = 2.75
平均带权周转时间 = 2.56

等待时间 = 周转时间 - 运行时间（不考虑等待I/O操作的时间）
P1 = 7 - 7 = 0
P2 = 10 - 4 = 6
P3 = 4 - 1 = 3
P4 = 11 - 4 = 7
平均等待时间 = 4

最短剩余时间优先（Shortest Remaining Time Next，SRTN）

    SRTN是SJF的抢占式版本，每当新进程加入时，调度程序总是选择剩余运行时间最短的进程运行，如果当前进程剩余运行时间比新进程长，则挂起当前进程而运行新进程

进程 到达时间 运行时间
P1   0        7
P2   2        4
P3   4        1
P4   5        4

P2到达时P1剩余5，P2为4，，运行P2
P3到达时，P1剩余5，P2剩余2，P3为1，运行P3
P4到达时，P3运行结束，P1剩余5，P2剩余2，P4为4，运行P2
最后依次运行P4和P1

最终调度顺序为P1 -> P2 -> P3 -> P2 -> P4 -> P1
P1 P2 P3 P2 P4    P1
-- -- -  -- ----  -----

周转时间 = 完成时间 - 到达时间
P1 = 16 - 0 = 16
P2 = 7 - 2 = 5
P3 = 5 - 4 = 1
P4 = 11 - 5 = 6
平均周转时间 = 7

带权周转时间 = 周转时间 / 运行时间
P1 = 16 / 7 = 2.29
P2 = 5 / 4 = 1.25
P3 = 1 / 1 = 1
P4 = 6 / 4 = 1.5
平均带权周转时间 = 1.51

等待时间 = 周转时间 - 运行时间（不考虑等待I/O操作的时间）
P1 = 16 - 7 = 9
P2 = 5 - 4 = 1
P3 = 1 - 1 = 0
P4 = 6 - 4 = 2
平均等待时间 = 3

高响应比优先（Highest Response Ratio Next，HRRN）

    非抢占式。在所有已到达进程中选择响应比（等待时间/运行时间 + 1）最高的运行，综合FCFS和SJF的优点，等待时间长、运行时间短的优先，避免长作业饥饿的问题

进程 到达时间 运行时间
P1   0        7
P2   2        4
P3   4        1
P4   5        4

响应比 = （等待时间 + 运行时间） / 运行时间
P1运行至结束，P2、P3、P4均到达，响应比分别为
P2 = (5 + 4) / 4 = 2.25
P3 = (3 + 1) / 1 = 4
P4 = (2 + 4) / 4 = 1.5
运行P3，P3结束时，响应比分别为
P2 = (6 + 4) / 4 = 2.5
P4 = (3 + 4) / 4 = 1.75
运行P2，最后运行P4

最终调度顺序为P1 -> P3 -> P2 -> P4
P1      P3 P2    P4
------- -  ----  ----

交互式系统中的调度
时间片轮转调度（Round-Robin Scheduling，RR）

    RR是一种简单公平的抢占式调度算法，并且可以避免饥饿。每个进程被分配一个时间片（quantum）。时间片结束时，如果进程还在运行，则剥夺CPU并分配给另一个进程。如果进程在时间片结束前阻塞或结束，则CPU立即切换。RR算法实现很容易，只需要维护一张进程队列表

A -> B -> C -> D

若A用完时间片，但仍在运行，则插入到队列尾
B -> C -> D -> A

若B用完时间片，但仍在运行，并到达一个新进程E，则先插入新进程
C -> D -> A -> E -> B

若C用完时间片之前就结束了，则直接切换到下一个进程
D -> A -> E -> B

    需要考虑的是时间片的长度，假设时间片为4ms，上下文切换为1ms，则CPU完成4ms工作后将浪费1ms进行上下文切换（context switch），即浪费了20%的时间。但如果时间片太大，就会退化为FCFS，导致增大响应时间。通常为了提高CPU效率，设置时间片时，切换开销占比应不超过1%

优先级调度

    为每个进程设置优先级，在已到达进程中，选择优先级最高的运行，可以为抢占式或非抢占式
    比如对于操作系统来说，I/O密集型进程的优先级应该更高。I/O密集型继承多数时间用于等待I/O结束，因此需要CPU时应立即分配给它以便启动下一个I/O请求，这样就可以在另一个进程计算的同时执行I/O操作
    一种简单做法是将优先级设置为1/f，f为该进程在上一时间片中的运行时间占比。比如在50ms时间片中，使用1ms的进程优先级为50，使用25ms的进程优先级为2。将进程按优先级分组，再使用RR算法调度高优先级组中的进程

多级反馈队列调度

    CTSS（Compatible Time Sharing System）是最早使用优先级调度的系统之一，但存在进程切换速度太慢的问题，其设计者意识到设置较长的时间片可以减少切换次数，但长时间片又会影响到响应时间。最终的解决方法是多级反馈队列调度，它是对FCFS、SJF、RR、优先级调度的折中权衡
    设置多个优先级队列，每个级别对应不同长度的时间片，比如第一级（最高级）时间片为1，第二级为2，第三级为4，以此类推
    如果一个进程用完当前级别时间片后仍未运行完，则加入下一级队列队尾，如果已经位于最后一级则放回该级队尾
    高优先级队列为空时，才会调度低优先级队列，因此可能导致低优先级进程饥饿
    比如一个进程需要100个时间片，第一次分配1个时间片，第二次分配2个，接下来是4、8、16、32、64，最后一次使用64中的37个即可结束工作，一共进行7次切换。如果使用RR算法，则需要100次切换

最短进程优先

    关键在于如何从可运行进程中找出最短的一个
    一种方法是根据过去的行为进行预测。假设某终端每条命令的估计运行时间为T0，测量到下一次运行时间为T1，则估计时间可以修正为aT0 + (1-a)T1，比如设a为1/2可以得到序列如下

T0
T0/2 + T1/2
T0/4 + T1/4 + T2/2
T0/8 + T1/8 + T2/4 + T3/2 // T0在此时估计时间中的占比下降到1/8

保证调度

    向用户作出明确的性能保证，然后实现它。比如有n个进程运行的单用户系统中，如果所有进程等价，则每个进程获得1/n的CPU时间，为了实现所作的保证，系统跟踪每个进程已使用的CPU时间，并计算应获得的时间，然后转向已用时间最少的进程，直到超过最接近的竞争者

彩票调度（Lottery Scheduling）

    保证调度的想法不错，但很难实现。彩票调度既可以给出类似预测结果，并且实现非常简单。其基本思想是为进程提供各种系统资源（如CPU时间）的彩票，一旦需要做出调度决策时，就随机抽出一张彩票，拥有该彩票的进程获取该资源
    比如系统掌握每秒50次的一种彩票，作为奖励每个获奖者可以获得20ms的CPU时间
    可以给更重要的进程额外的彩票，以增加其获胜的机会，比如出售100张彩票，一个进程持有其中20张，则每次抽奖该进程就有20%的取胜机会，在较长运行时间中该进程就会得到20%的CPU
    彩票调度可以解决其他方法很难解决的问题，比如一个视频服务器上有若干提供视频流的进程，每个流的帧率不同，假设帧率分别为10、20、25，那么给这些进程分别分配10、20、25张彩票，它们就会自动按照接近10:20:25的比例划分CPU的使用

公平分享调度

    之前的调度关注的都是进程本身，而没有关注进程所有者。假设两个用户分别启动9个进程和1个进程，使用RR算法，则两者分别得到90%和10%的CPU时间。为了避免这种情况，在调度处理之前应该考虑进程拥有者

无存储器抽象

    早期计算机没有存储器抽象，每个程序都直接访问物理内存

MOV REGISTER1, 1000 // 将位置1000的物理内存中的内容移到REGISTER1中

    因此那时呈现给程序员的存储器模型就是简单的物理内存：从0到某个上限的地址集合，每个地址对应一个可容纳一定数目（通常是8个）二进制位的存储单元
    这种情况下，在内存中同时运行两个程序是不可能的，如果一个程序在2000的位置写入一个新值，就会擦掉另一个程序在相同位置上的内容，因此无法同时运行两个程序，这两个程序会立刻崩溃
    为了运行多个程序，一个解决方法是，操作系统把当前内存中所有内容保存到磁盘，然后把下一个程序读入到内存中再运行即可。同一时刻，只要内存中只有一个程序，就不会发生冲突
    但这种方法有一个重要的缺陷，即重定位（即逻辑地址到物理地址的转换）问题。假设有两个程序，第一个程序在0处的指令是JMP 24，第二个程序在0处的指令是JMP 28，当第一个程序运行一段时间后再运行第二个程序，第二个程序会跳到第一个程序28处的指令。由于对内存地址的不正确访问，程序立刻崩溃
    一个补救方法是静态重定位，即装入时将逻辑地址转换为物理地址。当一个程序被装载到地址16384时，常数16384被加到每一个程序地址上。虽然这个机制在不出错误的情况下可行，但不是一种通用的解决方法，同时会减慢装载速度，并且它要求所有的可执行程序提供额外的信息，以区分哪些内存字中存有可重定位的地址，哪些没有
    虽然直接引用物理地址对大型计算机、小型计算机、台式计算机、笔记本都已经成为了历史，但在嵌入式系统、智能卡系统中，缺少存储器抽象的情况仍然很常见。像收音机、洗衣机、微波炉都是采用访问绝对内存地址的寻址方式，其中的程序都是事先确定的，用户不能在其上运行自己的软件，因此它们可以正常工作
    总之，把物理地址暴露给进程带来的严重问题有：
        如果用户程序可以寻址内存的每个字节，就可以轻易破坏操作系统
        想要运行多个程序很困难

一种存储器抽象：地址空间

    要使多个程序同时存在于内存中并且互不影响，需要解决保护（进程只能访问自己的内存）和重定位两个问题。对前者的一个原始的解决方法是，给内存标记上一个保护键，并且比较执行进程的键和其访问的每个内存字的保护键，比如进程能访问的空间是0-100，CPU标记此范围，然后在访问内存时检查是否为该进程可访问空间。不过这种方法并没有解决重定位问题
    更好的方法是创造一个新的存储器抽象：地址空间。地址空间是一个进程可用于寻址内存的一套地址集合，每个进程都有一个自己的地址空间，并且这个地址空间独立于其他进程的地址空间（除了一些情况下进程需要共享地址空间）
    地址空间的概念非常通用，比如7位数字的电话号码的地址空间是0 000 000到9 999 999，x86的I/O端口的地址空间是0到16383，IPv4的地址空间是0到2^32-1。地址空间也可以是非数字的，比如以.com结尾的网络域名的集合
    比较难的是给每个程序一个独有的地址空间，使得两个程序的相同地址（如地址28）对应不同的物理地址
    一个简单的方法是使用动态重定位，即运行时将逻辑地址转换为物理地址。把每个进程的地址空间映射到物理内存的不同部分，当一个进程运行时，程序的起始物理地址装载到基址寄存器（又称重定位寄存器），程序的长度装载到界限寄存器（又称限长寄存器）。进程访问内存，CPU在把地址发送到内存总线前会自动把基址加到进程发出的地址值上，同时检查程序提供的地址是否超出了界限寄存器中的值，如果超出了就会产生错误并终止访问。对于之前的例子，比如第二个程序的JMP 28，CPU会将其解释为JMP 16412
    使用基址寄存器和界限寄存器重定位的缺点是，每次访问内存都需要进行加法和比较运算，比较运算可以很快，但加法运算由于进位传递时间的问题，在没有使用特殊电路的情况下会显得很慢
    但物理内存是有限的，把所有进程一直保存在内存中需要巨大的内存，内存不足就无法支持这点。处理内存超载有两种通用方法，最简单的是交换（swapping）技术，即把进程完整调入内存运行一段时间，然后把它存回磁盘，这样空闲进程主要存储在磁盘上，不运行就不会占用内存。另一种方法是虚拟内存（virtual memory），它能使程序只有一部分调入内存的情况下运行
    交换可能在内存中产生多个空闲区（hole）。把进程尽可能靠近，将这些小的间隙合并成一大块，这种技术称为内存紧缩（memory compaction）。通常不进行这个操作，因为它需要耗费大量CPU时间
    如果进程的数据段可以增长（比如从堆中动态分配内存），进程与空闲区相邻，则可以把空闲区分配给进程使其增大。如果进程之间紧紧相邻，就需要把要增长的进程移到内存中一个足够大的区域，或者把一个或多个进程交换出去以生成足够大的空闲区。如果进程在内存中不能增长，并且磁盘上的交换区已满，则这个进程只能挂起直到有空间空闲，或者结束
    如果大部分进程在运行时需要增长，为了减少因内存区不够而引起的进程交换和移动开销，一种方法是在换入或移动进程时额外分配一些预留内存
    动态分配内存时，操作系统必须对其进行管理，一般跟踪内存使用情况有两种方法：位图和空闲区链表
    使用位图法时，把内存划分成分配单元（每个单元小到几个字节或大到几千字节），用位图中的一位来记录每个分配单元的使用情况，比如0表示空闲1表示占用（或者相反）。分配单元越小，位图越大，不过即使4个字节大小的分配单元，32位的内存只需要1位位图，位图只占用了1/32的内存
    位图法的主要问题是，在决定把一个占k个分配单元的进程调入内存时，存储管理器必须搜索位图，在位图中找出有k个连续0的串，这个查找操作很耗时，因为在位图中该串可能跨越字的边界
    另一个记录内存使用情况的方法是，维护一个记录已分配内存段和空闲内存段的链表，链表中的一个节点包含一个进程或者两个进程间的一块空闲区
    使用链表法时，为进程分配内存的最简单的算法是首次适配（first fit）算法，存储管理器沿链表搜索，直到找到一个足够大的空闲区，然后将空闲区分为两部分，一部分为要分配的大小，供进程使用，另一部分形成新的空闲区
    对首次适配算法进行小修改可以得到下次适配（next fit）算法，区别是在每次找到合适的空闲区时记录位置，这样下次就可以从上次结束的地方开始搜索。Bays的仿真程序证明下次适配算法性能略低于首次适配算法
    另一个著名并广泛使用的算法是最佳适配（best fit）算法，搜索整个链表，找到能容纳进程的最小空闲区。因为每次都要搜索整个链表，所以它比首次适配算法慢。有些令人意外的是，它比前两种算法浪费更多的内存，因为它会产生大量无用的小空闲区。为了避免分裂出很多非常小的空闲区，可以考虑最差适配（worst fit）算法，即总是分配最大的可用空闲区，但仿真程序表明这也不是一个好方法
    一个提高算法速度的方式是，为进程和空闲区分别维护链表，代价是增加复杂度和内存释放速度变慢，因为必须将回收的段从进程链表删除并插入到空闲区链表
    如果分别维护进程和空闲区的链表，就可以对空闲区链表按大小排序，以提高最佳适配算法的速度，比如按从小到大排序，第一个合适的空间就是最小的空闲区，就是最佳适配。排序后，首次适配算法与最佳适配算法一样快，下次适配算法无意义
    单独维护空闲区链表时可以做一个小优化，利用空闲区存储信息，每个空闲区的第一个字就是空闲区大小，第二个字指向下一空闲区
    另一种分配算法是快速分配（quick fit）算法，它为常用大小的空闲区维护单独的链表，比如链表第一项是4KB大小空闲区的链表头指针，第二项是8KB大小空闲区的链表头指针，以此类推。像21KB的空闲区，既可以放在20KB的链表中，也可以放在一个专门存放特殊大小的链表中。这种算法查找指定大小的空闲区很快，但同样存在的缺点是，进程终止或换出时，寻找它的相邻块并查找是否可以合并的过程非常费时，如果不合并，内存将很快分裂出大量无法利用的小空闲区

虚拟内存

    当程序大到内存无法容纳时，交换技术就有所缺陷，一个典型SATA磁盘的峰值传输率高达每秒几百兆，交换一个1GB的程序就需要好几秒
    程序大于内存的问题在一些应用领域早就存在了，比如模拟宇宙的创建就要花费大量内存。20世纪60年代的解决方案是，将程序分割为多个覆盖区（overlay）。程序开始运行时，将覆盖管理模块装入内存，该模块立刻装入并运行第一个覆盖区，执行完成后，第一个覆盖区通知管理模块装入下一个覆盖区
    程序员必须把程序分割成多个片段，这个工作非常费时枯燥，并且易出错。不久后有了虚拟内存（virtual memory）的方法，这些工作都可以交给计算机去做
    虚拟内存的基本思想是，程序的地址空间被分割成多个页（page），每一页有连续的地址范围。这些页被映射到物理内存，但并不是所有页必须在内存中才能运行程序。当程序引用到一部分物理内存中的地址空间时，由硬件执行必要的映射。当程序引用到一部分不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令

分页（paging）

    大部分虚拟内存系统都使用了分页技术
    由程序产生的地址称为虚拟地址（virtual address）

MOV REG, 1000 // 将地址为1000的内存单元的内容复制到REG，1000是虚拟地址

    虚拟地址构成了虚拟地址空间（virtual address space）。在没有虚拟内存的计算机上，系统直接将虚拟地址送到内存总线上，读写操作使用相同地址的物理内存字。在使用虚拟内存时，虚拟地址被送到内存管理单元（Memory Management Unit，MMU），MMU把虚拟地址映射为物理内存地址
    页表给出虚拟地址与物理内存地址之间的映射关系
    虚拟地址空间按固定大小划分为页面（page），物理内存中对应的单元称为页框（page frame），页面和页框的大小通常相同，页表说明了每个页面对应的页框。RAM和磁盘之间的交换总是以整个页面为单元进行的

    对应64KB的虚拟地址空间和32KB的物理内存，可以得到16个页面和8个页框
    比如执行指令访问地址0时

MOV REG, 0

    虚拟地址0被送到MMU，MMU发现其位于页面0（0-4095），根据映射结果，页面0对应页框2（8192-12287），于是MMU将地址转换为8192，并把地址8192送到总线上。内存并不需要知道MMU做的事，只看到一个访问地址8192的请求并执行
    当虚拟地址空间比物理内存大时，就会存在未被映射的页面。当程序执行指令访问未映射的页面

MOV REG, 32780 // 位于页面8（从32768开始）

    MMU发现该页面未被映射，于是使CPU陷入（traps）到操作系统，这个陷阱称为缺页中断或缺页错误（page fault）。操作系统找到一个很少使用的页框并把其内容写入磁盘，比如找到页面1对应的页框1。将页面1标记为未映射，再把页面8映射到这个页框1，然后重新启动访问指令，此时虚拟地址32780就可以映射到物理地址4108（4096+32780-32768）
    页面大小一般是2的整数次幂。比如页面大小为4KB，即2^12，对于一个16位的虚拟地址，即可用前4位表示页面的页号，后12位表示偏移量。比如虚拟地址8192，二进制为0010 0000 0000 0100，0010即为页号，0000 0000 0100即为偏移，因此8192位于页号2偏移4的位置
    页表中，查找页号2对应的页框号为6，把页框号110复制到输出寄存器的高3位，后12位保持不变，110 0000 0000 0100即为物理地址
    除了页框号，页表还会有一些其他的位
        有效位，如果该位为1则说明存在映射，如果为0，则访问该页面将引起缺页中断
        保护（protection）位，指出一个页允许的访问方式，比如用一个位表示，0表示读写，1表示只读
        修改（modified）位，记录页面使用情况，写入页面后由硬件自动设置修改位，该位也称为脏位（dirty bit），在重新分配页框时很有用，比如一个页是脏的（已被修改过），则必须把它写回磁盘，是干净的则可以直接丢弃
        访问（referenced）位，在页面被访问时设置，主要用来帮助操作系统在发生缺页中断时选择要淘汰的页面
        禁止高速缓存位，该位对于映射到设备寄存器而非常规内存的页面十分重要，比如操作系统持续等待I/O设备的响应，必须保证硬件读取的数据来自设备而非高速缓存

加速分页过程

    在任何分页系统中都需要考虑两个问题
        虚拟地址到物理地址的映射必须非常快：每次访问内存都要进行映射，所有的指令最终都来自内存，并且很多指令也会访问内存中的操作数，因此每条指令进行一两次或更多页表访问是必要的。如果指令一条指令要1ns，页表查询必须在0.2ns内完成，以避免映射成为主要瓶颈
        如果虚拟地址空间很大，页表也会很大：现代计算机至少使用32位虚拟地址，假设页面大小为4KB，32位的地址空间将有100万页，页表也必然有100万条表项。每个进程都有自己的虚拟地址空间，都需要自己的页表，于是需要为进程分配非常多的连续页框
    大多数程序总是对少量页面多次访问，没有必要让将整个页表保存在内存中，由此得出的一种解决方案是，设置一个转换检测缓冲区（Translation Lookaside Buffer，TLB），也称相联存储器（associate memory）或快表，将虚拟内存直接映射到物理地址，而不必再访问页表
    TLB通常在MMU中，包含少量表项，实际中很少会超过256个。将一个虚拟地址放入MMU中进行转换时，硬件先将页号与TLB中所有表项进行匹配，如果匹配成功且操作不违反保护位，则直接从TLB中取出页框号，而不再访问页表。如果匹配失败，则进行正常的页表查询，并从TLB淘汰一个表项，然后用新找到的页表项代替它
    处理巨大的虚拟地址空间有两种解决方法：多级页表和倒排页表
    比如32位地址空间中，页面大小为4KB，偏移量占12位，则页号占20位。将页号分组，页表项大小为4B，4KB的页面就能放1024个表项，于是每1024个页号分为一组。这样分组得到的页表为二级页表，再用一个顶级页表映射页号到二级页表的物理地址即可
    使用多级页表时，32位的地址划分为10位的PT1域、10位的PT2域、12位的Offset域。比如对于虚拟地址0000 0000 0100 0000 0011 0000 0000 0100，PT1为1，PT2为3，Offset为4，MMU先访问顶级页表1处，得到二级页表的物理地址，由此访问二级页表3处，得到页框号，最后加上Offset即为最终的物理地址
    二级页表可以扩充为更多级。每级页表大小不能超过一个页面，比如4KB页面，偏移为12位，页表项大小为4B，每1024分为一组，则每级最多10位，如果是40位，则除去12位，剩余可以划分为一级8位、二级10位、三级10位的三级页表
    单级页表只要进行两次访存（第一次访问页表得到物理地址，第二次访问物理地址），而每多一级页表就要多一次访存（不考虑TLB）
    另一种方式是倒排页表（inverted page table），让每个页框（而非页面）对应一个表项。比如对于64位虚拟地址，4KB的页，4GB的RAM，一个倒排页表仅需要2^20个表项，表项记录了一个页框对应的页面（进程）
    虽然倒排页表节省了大量空间，但从虚拟地址到物理地址的转换变得很困难，必须搜索整个倒排页表来找到页面，每一次搜索都要执行访问操作。这个问题可以通过TLB解决
    倒排页表在64位机器中很常见，因为64位机器中，即使使用大页面页表项数量也很庞大，比如对于4MB页面和64位虚拟地址，需要的页表项目数为2^42

页面置换算法

    发生缺页中断时，操作系统必须换出内存中的一个页面，以腾出空间。如果换出的页面在内存驻留期间被修改过，就必须把它写回磁盘以更新其在磁盘上的副本，如果未被修改过则不需要写回
    如果一个经常用到的页面被换出内存，短时间内它可能又被调入内存，这会带来不必要的开销。因此发生缺页中断时，如何选择要换出的页面是一个值得考虑的问题

最优页面置换算法（OPTimal replacement，OPT）

    OPT算法的思路很简单，从所有页面中选出下次访问时间距现在最久的淘汰

432143543215 // 页面队列
444444444222 // 页1
 33333333311 // 页2
  2111555555 // 页3
TTTT  T  TT  // 是否发生缺页中断（共发生7次缺页中断，4次页面置换）
   |
   把2替换掉，因为432中，2下一次被访问的时间最靠后

    这个算法的唯一问题在于，它是无法实现的，因为发生缺页中断时，操作系统无法得知各个页面下一次在什么时候被访问
    作为理论最优算法，可以用它衡量其他算法的性能。如果操作系统的页面置换性能只比最优算法差1%，那么花费大量精力来优化算法就不是特别必要的

最近未使用页面置换算法（Not Recently Used，NRU）

    操作系统为每个页面设置了两个状态位，当页面被访问时设置R位，被修改时设置M位。启动进程时，所有页面的RM均设为0，并且R被定期（比如每次时钟中断时）清零
    发生缺页中断时，根据RM位的值，可以将页面分为4类
        第0类：未访问未修改（R位为0，M位为0）
        第1类：未访问已修改（R位为0，M位为1，看起来似乎不可能，实际可以由第3类转换而来）
        第2类：已访问未修改（R位为1，M位为0）
        第3类：已访问已修改（R位为1，M位为1，R在清零后即变为第1类）
    NRU算法随机从第0类中选择一个页面淘汰，如果第0类中没有页面则选择第1类，以此类推，优先选择编号最小的类
    这个算法的隐含思想是，淘汰一个未访问已修改页面（第1类），比淘汰一个频繁使用的干净页面（第2类）好
    NRU的主要优点是易理解且能有效实现，虽然性能不是最好的，但已经够用了

先进先出页面置换算法（First-In First-Out，FIFO）

    顾名思义，淘汰最早进入的页面
    操作系统维护一个内存中所有当前页面的链表，最新进入的页面放在表尾，淘汰页面就是表头页面
    FIFO可能淘汰常用页面，甚至可能出现分配页面数增多但缺页率反而提高的异常现象（Belady异常），因此很少使用纯粹的FIFO算法

第二次机会页面置换算法（Second-Chance）

    对FIFO做一个简单的修改：检查最老页面的R位（访问位），如果R位是0则淘汰，如果是1则把R位清零，并把该页面放到表尾，然后继续搜索
    如果所有页面都被访问过，则该算法就简化为纯粹的FIFO算法

时钟页面置换算法（clock）

    第二次机会算法经常要在链表中移动页面，降低了效率且不是很有必要
    一个更好的办法是将所有页面保存在在一个类似钟面的环形链表中，一个表针指向最老的页面。发生缺页中断时，检查表针指向的页面，如果R位是0则淘汰该页面，并在该位置插入新页面，然后表针后移一步。如果R位是1则把R位清零，然后表针后移一步。如果该页已存在，不发生缺页中断，R位是0则改为1，表针不需要移动

最近最少使用页面置换算法（Least Recently Used，LRU）

    LRU是OPT的一个近似思路，在前几条指令中频繁使用的页面很可能在后几条指令中被使用，反过来说，很久没使用的页面很可能在之后的长时间内仍然不使用
    LRU是可实现的，但代价很高。实现LRU需要维护一个所有页面的链表，最常使用的位于表头，每次访问时必须更新整个链表，在链表中找到页面删除后再添加到表头
    有一些使用特殊硬件实现LRU的方法，比如要求硬件有一个64位计数器，它在每条指令执行完后加1，每个页表项中有一个足够容纳这个计数器值的域。发生缺页中断时，检查所有页表项的计数值，值最小的就是最近最少使用的
    只有非常少的计算机有这种硬件，LRU很优秀但很难实现

最不常用页面置换算法（Not Frequently Used，NFU）

    NFU是LRU的一个软件实现方案
    NFU将 每个页面与一个软件计数器关联，计数器初值为0，每次时钟中断时，操作系统扫描内存中所有页面，将每个页面的R位值加到计数器上，这个计数器大致跟踪了各个页面被访问的频繁程度。发生缺页中断时，则置换计数器值最小的页面
    NFU的问题在于，第一遍扫描中频繁使用的页面，第二遍扫描时，计数器值仍然很高。这就会导致后续扫描中，即使该页面使用次数最少，也会由于计数器值较高而不被置换

老化（aging）算法

    老化算法对NFU做了一些改进，在R位加进之前先将计数器右移一位，然后把R位加到计数器最左端的位

页面
0    10000000    11000000    11100000    11110000    01111000
1    00000000    10000000    11000000    01100000    10110000
2    10000000    01000000    00100000    00100000    10001000
3    00000000    00000000    10000000    01000000    00100000
4    10000000    11000000    01100000    10110000    01011000
5    10000000    01000000    10100000    01010000    00101000
     |           |           |           |           |
     访问页面024 访问014     访问013     访问04      访问12

    发生缺页中断时，置换计数器值最小的页面，因为前面的0越多，说明其最近越不常被访问
    老化算法非常近似LRU，但有两个区别
        比如最后一次访问时，如果发生缺页中断，需要置换一个页面。页面3和页面5开头都是001，即前两次未被访问，前第三次被访问，如果前第三次是页面5先被访问，则LRU会替换页面5，但这里无法区分两者谁先被访问，而只能替换值较小的页面3
        老化算法计数器位数有限，比如这里是8位，只能记录过去8次的访问，超过该次数的记录无法得知。不过实践中，如果时钟滴答是20ms，8位一般是够用的，如果一个页面160ms未被访问，则很可能不重要

工作集页面置换算法

    在单纯的分页系统中，刚开始启动进程时，内存中没有页面，CPU尝试取第一条指令时就会产生一次缺页中断，使操作系统装入含第一条指令的页面。一段时间后，进程需要的大部分页面都在内存了，进程开始在较少缺页中断的情况下运行。这个策略称为请求调页（demand paging），因为页面在需要时被调入，而不是预先装入
    一个进程当前正在使用的页面集合称为它的工作集（Denning），如果整个工作集都被装入内存中，那么进程在运行到下一阶段之前不会产生很多缺页中断。如果内存太小无法容纳整个工作集，进程的运行过程中将产生大量缺页中断，导致运行速度变慢，因为通常执行一条指令只要几纳秒，而从磁盘读入一个页面需要十几毫秒。如果每执行几条指令就发生一次缺页中断，就称这个程序发生了颠簸（Denning）
    请求调页策略中，每次装入一个进程都要产生大量缺页中断，速度太慢，并且CPU花了很多时间处理缺页中断，浪费了许多CPU时间，因此不少分页系统会设法跟踪工作集，以确保在进程运行前，工作集已经在内存中了，这个方法称为工作集模型（Denning），也叫预先调页（prepaging），其目的在于大大减少缺页中断率
    工作集是随着时间变化的，它是最近k次访存所访问过的页面集合。为了实现该算法，需要一种精确的方法来确定哪些页面在工作集中，为此必须预先选定k值。但有了工作集的定义并不意味着就能计算出工作集
    假设有一个长度为k的移位寄存器，每次访存都把寄存器左移一位，然后在最右端插入刚才访问过的页面号，寄存器中k个页面号的集合就是工作集。理论上，发生缺页中断时，只要读出寄存器中的内容并排序，然后删除重复的页面，结果就是工作集。但维护该寄存器并在缺页中断时处理它需要很大的开销，因此该技术从未被使用过
    有几种近似的方法作为替代，一种常见近似方法是，不向后查找最近k次的内存访问，而是查找过去一定时间内，比如过去10ms访存所用到的页面集合
    基于工作集的页面置换算法是，找出一个不在工作集中的页面并淘汰，为此表项中至少需要包含两条信息，一是上次使用该页面的近似时间，二是R位（访问位）
    处理表项时，如果R位是1，则把上次使用时间改为当前实际时间。如果R位是0，则可以作为置换候选者，计算生存时间（当前实际时间与上次使用时间的差），如果生存时间大于定义工作集范围的时间，则该页面在工作集外，将其置换。如果R为0且生存时间不超过定义工作集范围的时间，则该页面仍在工作集中，记录该页面。如果扫描完整个页表都没有可淘汰的，则从记录页面中选一个生存时间最长的淘汰，如果记录页面为空，即所有页面R位均为1，则随机选择一个淘汰

工作集时钟（WSClock）页面置换算法

    工作集算法需要扫描整个页表，比较费时，结合时钟算法的思路稍作改进，即可得到WSClock算法。它实现简单，性能较好，在实际工作中得到了广泛使用

分段（Segmentation）

    一个编译器在编译过程中会建立许多表，其中可能包括
        被保存起来供打印清单用的源程序正文（用于批处理系统）
        包含变量名字和属性的符号表
        包含用到的所有整型量和浮点常量的表
        包含程序语法分析结果的语法分析树
        编译器内部过程调用使用的堆栈
    在一维地址空间中，当有多个动态增加的表时，就可能发生碰撞。一种能令程序员不用管理表扩张和收缩的方法是，在机器上提供多个互相独立的段（segment）的地址空间，段的长度可以不同，在运行时可以改变，比如堆栈段的长度在数据压入时会增长，在数据弹出时会减小
    每个段都构成一个独立的地址空间，在内存中占据连续空间，可以独立地增长或减小，而不会影响其他段
    段是按逻辑功能的划分的实体，程序员使用起来更方便，并且程序的可读性更高。此外，分段有助于共享和保护。分段系统中，可以把共享库放到一个单独的段中由各个进程共享，而不需要在每个进程的地址空间中保存一份。当组成一个程序的所有过程都被编译和链接好以后，如果一个段的过程被修改并重新编译，也不会影响到其他段，因为这个段的起始地址（基址）没有被修改
    要在分段的存储器中表示一个地址，必须提供一个段号（段名）和一个段内地址（段内偏移量）

31 ... 16 15 ... 0 // 可用31-16表示段号，15-0表示段内地址

    每个进程需要一张段表，每个段表项记录一个段的起始位置和段的长度。段表项长度是固定的，因此段号可以是隐含的，不占存储空间。查找时，如果段号越界，则产生越界中断。如果段内地址超出段长，则产生越界中断

K号段的段表存放地址 = 段表起始位置 + K * 段表项长度

段号 基址 段长
0    20K  3K
1    60K  2K
2    40K  5K

如果一个逻辑地址段号为1，段内地址为1024
段号1的段长为2K，大于1024，不产生越界中断
存放地址 = 60K + 1024 = 61K

    分段管理的缺点是，如果段长过大，则不便于分配连续空间，此外会产生外部碎片。分页管理的内存利用率高，不会产生外部碎片，只会有少量页内碎片。因此，两者结合可以互相弥补，实现段页式管理
    段页式系统的地址由段号、页号、页内地址（页内偏移量）组成。分段对用户可见，而分页不可见

31 ... 16 15 ... 12 11 ... 0 // 可用31-16表示段号，15-12表示页号，11-0表示页内地址

    每个段表项记录页表长度、页表起始地址，通过页表起始地址找到页号，通过页号对应的页表项目找到物理地址，一共需要三次访存（如果引入以段号和页号为关键字的TLB且命中，则只需要一次访存）。段表项长度是固定的，段号可以是隐含的。同样，每个页表项长度固定，页号是隐含的



    进程运行时，可以在自己的地址空间存储信息，但这样保存信息的问题是
        对于一些程序，如银行系统，这样的存储空间太小
        进程终止时，保存的信息就丢失了
        经常需要多个进程访问同一信息，这要求信息独立于任何一个进程
    因此，长期存储信息有三个基本要求
        能够存储大量信息
        使用信息的进程终止时，信息仍存在
        允许多个进程并发访问信息
    理论上，磁盘（magnetic disk）就能解决长期存储的问题，但实际上，有许多操作不便于实现
        如何找到信息
        如何防止一个用户读取另一个用户的数据
        如何知道哪些块是空闲的
    为了解决这个问题，引入文件的概念，它是一个建模于磁盘的抽象概念
    文件由操作系统管理，文件的构造、命名、访问、使用、保护、实现、管理方法是操作系统设计的主要内容，操作系统中处理文件的部分称为文件系统（file system）

文件
文件命名

    各个系统中的文件命名规则不同，现代操作系统都允许用1到8个字母组成的字符串作为合法的文件名，通常也允许有数字和一些特殊字符
    一般操作系统支持文件名用圆点分隔为两部分，如main.cpp，圆点后的部分称为文件扩展名（file extension）。UNIX中，文件扩展名只是一种约定，Windows中的扩展名则有特别意义，用户或进程可以在操作系统中注册扩展名，并规定哪个程序拥有该扩展名（即双击该文件则启动此程序并运行该文件）

文件结构

    文件可以有多种构造方式
        常见的一种构造方式是无结构的单字节序列，操作系统见到的就是字节，文件内容的任何含义只在用户程序中解释，UNIX和Windows都采用这种方法。这为操作系统提供了最大的灵活性，用户可以向文件中加入任何内容，以任何形式命名，操作系统不提供帮助也不进行阻碍
        第二种构造方式是固定长度记录的序列，这种方式的中心思想是，读操作返回一个记录，写操作重写或追加一个记录。几十年前，80列的穿孔卡片是主流时，很多大型机的操作系统使用的就是这种方式，文件由80个字符的记录组成，文件系统建立在这种文件基础上
        第三种构造方式是用一棵记录树构成文件，记录的固定位置有一个键，树按键排序，从而可以对键进行快速查找，这种方式被广泛用于处理商业数据的大型计算机

文件类型

    操作系统一般支持多种文件类型，UNIX和Windows都有普通文件（regular file）和目录（directory），此外UNIX还有字符特殊文件（character special file）和块特殊文件（block special file）
    普通文件一般分为ASCII文件和二进制文件
        ASCII文件由多行正文组成，每行用回车符或换行符或两者（如MS-DOS）结束，其最大优势是可以显示、打印、编辑，如果很多程序都用ASCII文件作为输入和输出，就很容易把一个程序的输出作为另一个程序的输入
        二进制文件打印出来是充满乱码的表，通常二进制文件有一定的内部结构，使用该文件的程序才了解这种结构。比如UNIX存档文件，每个文件以模块头开始，其中记录了名称、创建日期、所有者、保护码、文件大小，该模块头与可执行文件一样都是二进制数字，打印输出它们毫无意义

文件访问

    早期操作系统只有顺序访问（sequential access）一种文件访问方式，进程可以从头按顺序读取文件的字节，不能跳过某一些内容。在存储介质是磁带而不是磁盘时，顺序访问文件是很方便的
    用磁盘存储文件时，就能以任何次序读取文件的字节，能被这种方式访问的文件称为随机访问文件（random access file）。对许多程序来说，随机访问文件必不可少，比如数据库系统，查找一条记录时，不需要先读出之前的成千上万条记录

文件属性

    除了文件名和数据，操作系统还会保存文件相关的信息，如创建日期、文件大小等，这些附加信息称为文件属性（attribute）或元数据（metadata）。不同系统中的文件属性差别很大

文件操作

    使用文件是为了存储信息并方便以后检索，不同的操作系统提供了不同的方式，常见的文件相关的系统调用有create、delete、open、close、read、write、append、seek、get attributes、set attributes、rename

目录

    目录系统的最简单形式是单层目录系统，即一个目录中包含所有文件，这个目录通常称为根目录，其优势是简单，且能快速定位文件，常用于简单的嵌入式装置，如电话、数码相机
    现在的用户通常有成千上万的文件，用单层目录寻找文件就很困难了，这就需要层次结构（即一个目录树），几乎所有现代文件系统使用的都是层次目录系统。用目录树组织文件系统时，常用绝对路径名（absolute path name）或相对路径名（relative path name）来指明文件名
    UNIX中常见的目录操作的系统调用有create、delete（只能删除空目录）、opendir、closedir、readdir、rename、link（类似创建快捷方式）、unlink

文件系统的实现
文件系统布局

    文件系统存放在磁盘上。多数磁盘划分为一个或多个分区，每个分区中有一个独立的文件系统
    磁盘的0号扇区称为主引导记录（Master Boot Record，MBR），用来引导计算机
    MBR的结尾是分区表，该表给出了每个分区的起始地址和结束地址。表中的一个分区被标记为活动分区，计算机被引导时，BIOS读入并执行MBR，MBR做的第一件事就是确定活动分区，读入第一个块，即引导块（boot block），并执行
    除了引导块，磁盘分区的布局通常随文件系统的不同而变化，一个可能的文件系统布局如下

|-----------------整个磁盘-----------------|
  分区表               磁盘分区
     ↓       L     L        K        K
 __________________________________________
|MBR||||________|________|________|________|
               /          \
              /            \

|引导块|超级块|空闲空间管理|i节点|根目录|文件和目录|

文件的实现

    文件存储实现的关键是记录文件用到了哪些磁盘块，不同的操作系统的实现方式不同
    最简单的方式是连续分配，每个文件作为一连串连续数据块存储在磁盘上，比如块大小为1KB的磁盘上，50KB的文件要分配50个连续的块。每个文件都要从一个新的块开始，上一个文件末尾块可能会存在部分被浪费的空间
    连续分配的优势是实现简单，只需要为每个文件记录第一块的磁盘地址和使用的块数，另外读操作性能较好，单个操作就可以读出整个文件
    缺点是删除文件会在磁盘中留下断断续续的空闲块。压缩磁盘代价太高，不可行。维护一个空闲块链表，但创建新文件时，为了选择选择合适的空闲区，必须先给出文件的最终大小，如果用户要创建一个文档然后录入，用户是无法给出最终大小的。但这在CD-ROM中是可行的，因为所有文件的大小都事先定好了，并且后续使用也不会被改变
    第二种方式是链式分配，这样不会因为磁盘碎片而浪费存储空间，但随机访问很慢，每次要访问一个块时，都必须从第一个块开始。此外，指向下一个块的指针占用了一些字节，每个磁盘块存储数据的字节数不再是2的整数次幂，虽然这个问题不是非常严重，但也会降低系统的运行效率，因为程序一般以长度为2的整数次幂来读写磁盘块
    第三种方式是把链式分配的指针放到内存的一个表中，这个表称为文件分配表（File Allocation Table，FAT），这样就解决了大小不正常带来的问题，但如果表项过多，比如1TB的磁盘和1KB的块，FAT有10亿项，每项至少占3字节，这就占了3GB内存，因此FAT在大型磁盘中不实用
    最后一种方式是为每个文件赋予一个i节点（index-node）的数据结构，其中列出了文件属性和文件块的磁盘地址。给定i节点就能找到文件的所有块，这种方式相对于FAT的优势是，只有在文件打开时，其i节点才在内存中，最终需要的内存与同时打开的最大文件数成正比

目录的实现

    读文件时必须先打开文件，打开文件时，操作系统利用路径名找到目录项，目录项中提供了查找文件磁盘块所需要的信息。这些信息与系统有关，信息可能是整个文件的磁盘地址（对于连续分配的系统）、第一块的编号（链式分配）、i节点号。文件属性存放的位置可以是目录项或者i节点
    现代操作系统一般都支持长度可变的长文件名。最简单的实现方式是，给文件名一个长度限制，如255个字符，并为每个文件名保留该长度的空间，这种方式简单但浪费了大量目录空间
    第二种方式是，每个目录项中开头有一个记录目录项长度的固定部分，接着是文件属性、任意长度的文件名。缺点和连续分配的磁盘碎片问题一样，移除一个个文件后会留下断断续续的空隙。由于整个目录在内存中，只有对目录进行紧凑操作才能节省空间。另一个问题是一个目录项可能会分布在多个页面上，读取文件名时可能发生缺页中断
    第三种方式是，使目录项有固定长度，将文件名放在目录后面的堆上，并管理这个堆，这样移除一个目录项后，下一个进来的目录项总可以填满这个空隙
    线性查找文件名要从头到尾搜索目录，对于非常长的目录，一个优化方式是在每个目录中使用散列表来映射文件名和对应的目录项

共享文件

    几个用户在同一个项目中工作时常需要共享文件。对于如下文件系统，B与C有一个共享文件，B的目录与该文件的联系称为一个链接（link）。这样，文件系统本身是一个有向无环图（Derected Acyclic Graph，DAG）而不是一棵树，代价是维护变得复杂

    共享文件的问题是，如果目录中包含磁盘地址，链接文件时必须将C目录中的磁盘地址复制到B目录中，如果B（或C）往文件中添加内容，新数据块只会列入B（或C）的用户目录中，C（或B）对此改变是不知道的，这就违背了共享的目的
    解决这个问题的第一个方法是，磁盘块不列入目录，而是列入一个与文件关联的小型数据结构，目录将指向这个小型数据结构。这是UNIX的做法，小型数据结构就是i节点
    这种方法的缺点是，B链接该共享文件时，i节点记录的文件所有者仍是C，只是将i节点的链接计数加1，以让系统知道该文件有多少个指向它的目录项。如果C之后删除了这个文件，B就有一个指向无效的i节点的目录项。如果这个i节点之后分配给另一个文件，B的链接将指向一个错误的文件。系统可以通过i节点的计数知道文件被引用，但无法找到所有目录项并删除，也不可能把目录项指针存储在i节点中，因为可能有无数个这样的目录
    第二个方法是符号链接（symbolic linking），让系统建立一个LINK类型的文件，把该文件放在B目录下，使得B与C的一个文件存在链接。LINK文件中包含了要链接的文件的路径名，B读该链接文件时，操作系统发现是LINK类型，则找到其链接文件的路径并读取
    符号链接在文件被删除后，通过路径名查找文件将失败，因此不会有第一种方法的问题。符号链接的问题在于需要额外开销，必须读取包含路径的文件，然后逐步扫描路径直到找到i节点，这些操作可能需要很多次额外的磁盘访问
    此外，所有方式的链接都存在的一个问题是，文件有多个路径，如果查找文件，将多次定位到被链接的文件，如果一个程序的功能是查找某个文件并复制，就可能导致多次复制同一文件

日志结构文件系统（Log-structured File System，LFS）

    设计LFS的主要原因是，CPU运行速度越来越快，RAM内存变得更大，磁盘高速缓存迅速增加，不需要磁盘访问操作，就可能满足直接来自高速缓存的大部分读请求，由此可以推断，未来的磁盘访问多数是写操作，且写操作往往是零碎的，提前读机制并不能获得更好的性能
    因此LFS的设计者决定重新实现一种UNIX文件系统，即使面对一个由大部分为零碎的随机写操作组成的任务，也能够充分利用磁盘带宽
    基本思路是，将整个磁盘结构化为一个日志，最初所有写操作都缓冲在内存中，每隔一段时间或有特殊需要时，被缓冲在内存中未执行的写操作被放到一个单独的段中，作为日志末尾的一个邻接段被写入磁盘
    但磁盘空间不是无限大的，这种做法最终将导致日志占满整个磁盘，此时就无法再写入新的段。为了解决这个问题，LFS有一个清理线程，该线程周期性扫描日志进行磁盘压缩。整个磁盘成为一个大的环形缓冲区，写线程将新的段写到前面，清理线程将旧的段从后面移走
    LFS在处理大量零碎写操作时的性能比UNIX好一个数量级，在处理读和大块写操作时的性能也不比UNIX差，甚至更好

日志文件系统

    由于LFS和现有的文件系统不相匹配，所以还未被广泛使用，但其内在的一个思想，即面对出错的鲁棒性，可以被其他文件系统借鉴。这个基本想法是，保存一个用于记录系统下一步要做什么的日志。当系统在完成任务前崩溃时，重新启动后，就能通过查看日志获取崩溃前计划完成的任务。这样的文件系统被称为日志文件系统，并已被实际使用，比如微软的NTFS、Linux ext3、RerserFS，OS X将日志文件系统作为可选项提供

虚拟文件系统（Virtual File System，VFS）

    同一台计算机或同一个操作系统中，可以有多个不同的文件系统
    Windows有一个主要的NTFS文件系统，但也有一个包含FAT-32或FAT-16的驱动器或分区，此外还可能有CD-ROM或者DVD（每一个包含特定文件系统），Windows通过指定盘符来处理不同的文件系统，进程打开文件时，盘符是显式或隐式存在的，Windows由此可知向哪个文件系统传递请求，不需要将不同的文件系统整合为统一模式
    所有现代的UNIX尝试将多种文件系统整合到一个统一的结构中。一个Linux系统可以用ext2作为根文件系统，ext3分区装载在/usr下，采用RerserFS的文件系统的硬盘装载在/home下，ISO 9660的CD-ROM临时装载在/mnt下。用户视角中，只有一个文件系统层级，但实际上是对用户和进程不可见的多种不相容的文件系统
    但是多种文件系统的存在在实际应用中是明确可见的，以前大多UNIX操作系统都使用VFS概念尝试将多种文件系统统一成一个有序结构，其核心思想是抽象出所有文件系统共有的部分为单独一层，这一层通过调用底层的实际文件系统来具体管理数据
    UNIX中，所有文件相关的系统调用最初都指向VFS，这些来自用户进程的调用都是标准的POSIX系统调用，VFS对用户进程提供的上层接口就是POSIX接口。VFS也有一个对于实际文件系统的下层接口，即VFS接口，当创造一个新的文件系统和VFS一起工作时，新系统的设计者必须确定它提供VFS所需要的功能调用

--------------------------------
用户进程
--------------------------------
|
|             POSIX接口
↓
--------------------------------
VFS
--------------------------------
|    |    |
|    |    |   VFS接口
↓    ↓    ↓
--------------------------------
FS1  FS2  FS3 实际文件系统
--------------------------------
↑    ↑    ↑
|    |    |
↓    ↓    ↓
--------------------------------
高速缓冲区
--------------------------------

文件系统管理和优化
磁盘空间管理

    几乎所有文件系统都将文件分割成固定大小的块存储，各块之间不一定相邻。块的大小是一个需要考虑的问题，块太小则文件块数越多，需要更多次的寻道与旋转延迟才能读出它们，从而降低了性能。块太大，则文件的最后一个块存在空间浪费。从历史观点上来说，一般设将块大小为1到4KB，但随着现在磁盘超过了1TB，磁盘空间已经不再短缺了，将块的大小提升到64KB并接受一些浪费比较好
    选定块大小后，下一个问题是如何记录空闲块。有两种方法被广泛使用，一是链表，二是位图
    为了防止占用太多磁盘空间，多用户操作系统通常提供了强制性磁盘配额机制，系统管理员为每个用户分配拥有文件和块的最大数量，操作系统确保每个用户不超过得到的配额

文件系统备份

    磁盘转储到磁带上有两种方案
        物理转储：从磁盘的第0块开始，将全部的磁盘块按序输出到磁带上，直到最后一块复制完毕
        逻辑转储：从一个或几个指定的目录开始，递归地转储其自给定日期后有所更改的全部文件和目录

文件系统的一致性

    很多文件系统读取磁盘块，修改后再写回磁盘。如果在写回完成前系统崩溃，文件系统可能处于不一致状态。为此，很多计算机都有一个检查文件系统一致性的实用程序，比如UNIX的fsck、Windows的scandisk，系统启动时，特别是崩溃后的重启，可以运行该程序
    一致性检查分两种
        块的一致性检查：程序构造两张表，每张表为每个块设立一个计数器，第一张表记录块在文件中的出现次数，第二张记录块在空闲区的出现次数。如果文件系统一致，最终每一个块在其中一张表中的计数器为1，如果一个块在两张表中的计数器都为0，则称为块丢失
        文件的一致性检查：原理同上，区别是一个文件（而非一个块）对应一个计数器。注意，由于存在硬链接，一个文件可能出现在多个目录中。而遇到符号链接是不计数的，不会对目标文件的计数器加1

文件系统性能

    访问磁盘比访问内存慢很多，如果只需要一个字，内存访问可以比磁盘访问快百万数量级，因此许多文件系统采用了各种优化措施来改善性能
    最常用的减少磁盘访问次数的技术是块高速缓存（block cache）或缓冲区高速缓存（buffer cache），它们逻辑上属于磁盘，但实际上保存在内存中
    第二个明显提高性能的技术是块提前读，在需要用到块之前先将块提前写入高速缓存，从而提高命中率。块提前读只适用于顺序读取的文件，如果请求文件系统在某个文件中生成一个块，文件系统将潜在地检查高速缓存，如果下一个块不在缓存中，则为下一个块安排一个预读
    另一个重要技术是把可能顺序访问的块放在一起，最好是在同一个柱面上，从而减少磁盘臂的移动次数。这个技术仅当磁盘中装有磁盘臂时才有意义，现在固态硬盘（SSD）越来越流行，而它们不带移动部件。固态硬盘采用了和闪存同样的制造技术，使得随机访问与顺序访问在传输速度上已经较为接近，传统硬盘的诸多问题就消失了，但也有一些新问题，比如每一块只可写入有限次数，使用时要十分小心以达到均匀分散磨损的目的

磁盘碎片整理

    随着不断创建与删除文件，磁盘会逐渐产生许多碎片，创建一个新文件时，其使用的块会散布在整个磁盘上，造成性能降低
    一个恢复方式是，移动文件使其相邻，把空闲区放到一个或多个大的连续区域内。Windows有一个defrag程序，就是用于完成这项工作的，Windows用户应该定期使用它。Linux文件系统由于其选择磁盘块的方式，在磁盘碎片整理上一般不会遇到Windows那样的困难，因此很少需要手动整理磁盘碎片
    固态硬盘不受磁盘碎片的影响，对其做磁盘碎片整理不仅没有提高性能，反而磨损了硬盘，缩短了使用寿命


I/O硬件原理

    I/O设备就是可以将数据输入到计算机（如鼠标、键盘），或者可以接收计算机输出数据的外部设备（如显示器）
    I/O设备按信息交换单位可分为两类
        块设备（block device）：把信息存储在固定大小的块中，每个块都有自己的地址。块设备的基本特征是，传输速率快，可寻址，每个块都能独立于其他块而读写。磁盘就是最常见的块可寻址设备，无论磁盘臂当前处于什么位置，总是能寻址其他柱面并且等待所需要的磁盘块旋转到磁头下面
        字符设备（character device）：以字符为单位发送或接收一个字符流，而不考虑任何块结构，因此传输速率较慢，不可寻址，也没有任何寻道操作，在输入/输出时常采用中断驱动方式。打印机、鼠标就是常见的字符设备
    I/O设备一般由机械部件和电子部件两部分组成
        机械部件主要用于执行具体I/O操作，如鼠标的按钮、键盘的按键、显示器的屏幕、硬盘的磁盘臂
        电子部件也称作设备控制器（device controller）或适配器（adapter），通常是主板上的芯片，或一块插入主板扩充槽的印刷电路板
    CPU无法直接控制机械部件，因此需要通过设备控制器作为中介来控制机械部件。设备控制器的主要功能有
        接收和识别CPU发出的命令：每个控制器有几个寄存器用于与CPU通信，通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执行其他某些操作
        向CPU报告设备的状态：通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收一个新的命令等
        数据交换：除了控制寄存器外，许多设备还有一个操作系统可以读写的数据缓冲区，比如在屏幕上显示像素的常规方法是使用一个视频RAM，这一RAM基本上只是一个数据缓冲区，可供程序或操作系统写入数据
        地址识别：为了区分设备控制器中的寄存器，需要给每个寄存器设置一个地址，控制器通过CPU提供的地址来判断CPU要访问的寄存器
    设备控制器中有多个寄存器，为这些寄存器编址有两种方式
        内存映射I/O（memory-mapped I/O）：所有设备控制器的寄存器映射到内存空间中，每个控制寄存器被分配一个唯一的内存地址，并且不会有内存被分配到这一地址
        寄存器独立编址：每个寄存器被分配一个I/O端口（port）号，所有端口号形成I/O端口空间（I/O port space），并且受到保护使得普通用户程序不能对其进行访问，只有操作系统可以访问。这一方案中，内存地址空间和I/O地址空间是不同且不相关的

I/O软件原理

    I/O软件的设计有以下目标
        设备独立性（device independence）：允许编写出的程序可以访问任意I/O设备而无需事先指定设备，比如读取一个文件作为输入的程序，应该能在硬盘、DVD或USB盘上读取文件，无需为每一种不同的设备修改程序
        统一命名（uniform naming）：一个文件或一个设备的名字应该是一个简单的字符串或一个整数，不应依赖于设备
        错误处理（error handling）：一般来说，错误应该尽可能在接近硬件的层面得到处理。当控制器发现一个读错误时，如果它能够处理，就应该自己设法纠正错误。如果控制器处理不了，设备驱动程序就应当予以处理，可能只需要重读一次这块数据就正确了
        同步（synchronous，即阻塞）和异步（asynchronous，即中断驱动）传输：大多数物理I/O是异步的，比如CPU启动传输后便转去做其他工作，直到中断发生。如果I/O操作是阻塞的，用户程序就更容易编写，比如read系统调用之后程序将自动被挂起，直到缓冲区中的数据准备好，而正是操作系统将实际异步的操作变为了在用户程序看来是阻塞式的操作
        缓冲（buffering）：数据离开一个设备之后通常不能直接存放到最终目的地，比如从网络上进来一个数据包时，直到将该数据包存放到某个地方，并对其进行检查，操作系统才知道要将其置于何处。缓冲涉及大量复制工作，经常对I/O性能有重大影响
        共享设备和独占设备：共享设备能同时让多个用户使用（如磁盘），独占设备则只能由单个用户独占使用（如磁带机）。独占设备的引入带来了各种问题（如死锁），操作系统必须能处理共享设备和独占设备以避免问题发生
    I/O有三种实现方式
        程序控制I/O（programmed I/O）：这是I/O的最简单形式。CPU轮询设备状态，当设备准备好时，CPU向控制器发出读指令，从I/O设备中读取字，再把这些字写入到存储器。这种方式的优点是实现简单，缺点是在完成全部I/O之前，CPU的所有时间都被其占用，如果CPU有其他事情要做，轮询就导致了CPU利用率低
        中断驱动I/O：用中断阻塞等待I/O的进程，CPU在等待I/O设备就绪时，通过调度程序先执行其他进程。当I/O完成后（比如打印机打印完一个字符，准备接收下一个字符），设备控制器将向CPU发送一个中断信号，CPU检测到中断信号后保存当前进程的运行环境信息，然后执行中断驱动程序来处理中断。CPU从设备控制器读一个字的数据传送到CPU寄存器，再写入主存，接着CPU恢复其他进程的运行环境并继续执行（打印下一个字符）。中断的优点是提高了CPU利用率，缺点是每次只能读一个字，每次都要发生一个中断，频繁的中断处理将浪费一定的CPU时间
        使用DMA（Direct Memory Access）的I/O：让DMA控制器来完成CPU要做的工作，使得CPU可以在I/O期间做其他操作。有了DMA控制器，就不用每个字中断一次，而是减少到每个缓冲区一次。DMA控制器通常比CPU慢很多，如果CPU在等待DMA中断时没有其他事情要做，采用中断驱动I/O甚至程序控制I/O也许更好

I/O软件层次

    I/O软件通常组织成四个层次，从上层到底层依次为
        用户级I/O软件：实现了与用户交互的接口，为用户提供I/O操作相关的库函数接口，如printf
        与设备无关的操作系统软件：向用户层提供系统调用，如为printf提供write，另外还要提供设备保护（设置访问权限）、缓冲、错误报告、分配与释放专用设备、建立逻辑设备名到物理设备名的映射关系等功能
        设备驱动程序（device driver）：每个连接到计算机上的I/O设备都需要某些设备特定的代码来对其进行控制，这样的代码称为设备驱动程序
        中断处理程序：进行中断处理

盘

    盘有多种多样的类型，最常用的是磁盘，它具有读写速度同样快的特点，适合作为辅助存储器（用于分页、文件系统等）
    磁盘被组织成柱面，每一个柱面包含若干磁道，磁道数与垂直堆叠的磁头个数相同，磁道又被分为若干扇区，通过(柱面号, 盘面号, 扇区号)即可定位一个磁盘块
    磁盘臂调度算法有
        先来先服务算法（First-Come First-Served，FCFS）：按照请求接收顺序完成请求，优点是公平简单易实现，缺点是平均寻道时间较长
        最短寻道时间优先算法（Shortest Seek Time First，SSTF）：下一次处理，磁头向所有请求中距离最近的位置移动。缺点是可能出现饥饿现象
        扫描算法（SCAN）：也叫电梯算法（elevator algorithm），磁头持续向一个方向移动，直到到达最内侧或最外侧时才改变方向。优点是平均寻道时间较短，不会产生饥饿现象
        LOOK调度算法：对扫描算法稍作优化，如果磁头移动方向上已没有需要处理的请求，则直接改变方向
        循环扫描算法（C-SCAN）：SCAN算法对于各个位置磁道的响应频率不平均，靠近磁盘两侧的可能更快被下一次访问。为了解决这个问题，C-SCAN算法的原理是，只在一个移动方向上处理请求，磁头返回时不处理任何请求
        C-LOOK：只在一个移动方向上处理请求，如果该方向之后没有要处理的请求，则磁头返回，并且只需要返回到第一个有请求的位置


资源死锁（resource deadlock）

    资源分为两类
        可抢占资源（preemptable resource）：可以从拥有它的进程中抢占，而不会产生任何副作用，如存储器
        不可抢占资源（nonpreemptable resource）：在不引起相关的计算失败的情况下，无法把它从占有它的进程处抢占过来，如光盘刻录机
    死锁主要关心不可抢占资源
    如果一个进程集合中，每个进程都在等待集合中的其他进程才能引发的事件，则该进程集合就是死锁的。通常这个事件是其他进程释放自身占有的资源，这种死锁称为资源死锁，这是最常见的死锁类型，但不是唯一的类型
    发生资源死锁的四个必要条件是
        互斥条件：每个资源要么分配给一个进程，要么是可用的
        占有和等待条件：已得到某个资源的进程可以再请求新的资源，并且不会释放已有资源
        不可抢占条件：已分配给一个进程的资源不能被强制抢占，只能被占有它的进程显式释放
        环路等待条件：死锁发生时，系统中必然有多个进程组成一条环路，环路中的每个进程都在等待下一个进程所占有的资源

鸵鸟算法

    最简单的解决方法是，把头埋到沙子里，假装根本没有问题发生。不同人对该方法的看法也不同，数学家认为这种方法完全不可接受，无论代价多大都应该彻底防止死锁发生，工程师认为要根据死锁发生的频率、严重程度、系统崩溃次数来决定，如果死锁每五年发生一次，而系统每个月都会因故障崩溃一次，就没有必要用损失性能和可用性的代价去防止死锁

死锁检测和死锁恢复

    第二种技术是死锁检测和恢复，使用这种技术时，系统不阻止死锁的产生，而是允许死锁发生，在检测到死锁发生后再恢复
    用E表示现有资源向量（exisiting resource vector），A表示可用资源向量（available resource vector），用C表示当前分配矩阵（current allocation matrix），用R表示请求矩阵（request matrix），死锁检测的算法是
        在R中查找是否存在某一行（即一个进程）小于等于A
        如果找到这样一行，就将C中相同行数的行（即该进程的已分配资源）加到A中，然后标记该进程，再转到上一步
        如果不存在这样一行，则算法终止。算法结束时，所有没标记过的进程都是死锁进程
    死锁恢复方法有：抢占、回滚、终止进程

死锁避免

    如果当前状态下没有死锁发生，并且存在某种调度次序能使每个进程都运行完毕，则称该状态是安全的
    对于目前有3个空闲资源的如下状态，先分配2个资源给B，B运行完释放4个资源，此时有5个空闲资源，接着5个资源全分配给C，C运行结束后将有9个空闲资源，最后将9个资源全分配给A即可。按BCA的分配顺序可以使得所有进程都能完成，因此这个状态是安全的

进程 	已分配资源 	最大需求
A 	3 	9
B 	2 	4
C 	2 	7

    空闲资源数为2时的如下状态就是不安全状态。首先只能先运行B，B运行结束后共有4个空闲资源，无法再运行A或C

进程 	已分配资源 	最大需求
A 	4 	9
B 	2 	4
C 	2 	7

    安全状态和不安全状态的区别是：从安全状态出发，系统可以保证所有进程都能完成，而从不安全状态出发就没有这样的保证
    Dijkstra提出了一种避免死锁的调度算法，称为银行家算法（banker's algorithm），方法是对每一个请求进行检查，如果满足这一请求会到达安全状态，则满足该请求，否则推迟对该请求的满足
    之前安全状态的例子考虑的就是单个资源的银行家算法，下面考虑多个资源的银行家算法
    已分配资源

进程 	资源1 	资源2 	资源3 	资源4
A 	3 	0 	1 	1
B 	0 	1 	0 	0
C 	1 	1 	1 	0
D 	1 	1 	0 	1
E 	0 	0 	0 	0

    仍需要的资源

进程 	资源1 	资源2 	资源3 	资源4
A 	1 	1 	0 	0
B 	0 	1 	1 	2
C 	3 	1 	0 	0
D 	0 	0 	1 	0
E 	2 	1 	1 	0

    对应的当前分配矩阵C和请求矩阵R为

// C
3011
0100
1110
1101
0000

// R
1100
0112
3100
0010
2110

    用三个向量表示现有资源E、已分配资源P、可用资源A，计算分配矩阵C的每列和得到P = (5322)，以E = (6342)为例，A = E - P = (1020)
    检测一个状态是否安全的算法是
        查找一个使用可用资源即可运行的进程，如果找不到则系统就会死锁
        如果找到，则假设该进程获取所需资源并运行结束，将该进程标记为终止，再将其资源加到A上
        重复上述两步，如果最后所有进程都被标记为终止，则初始状态是安全的
    对于这个例子
        进程D仍需要的资源为(0010)，均小于(1020)，因此运行D，D最初的已分配资源为(1101)，因此结束后A = (1020) + (1101) = (2121)
        进程A仍需要的资源为(1100)，均小于运行(2121)，运行A（此时E也满足条件，也可以运行E），A最初的已分配资源为(3011)，结束后A = (2121) + (3011) = (5132)
        运行B，结束后A = (5132) + (0100) = (5232)
        运行C，结束后A = (5232) + (1110) = (6342)
        运行E，结束后A = (6342) + (0000) = (6342)
        所有进程都运行结束，因此这个例子的状态是安全的

死锁预防

    死锁避免本质上来说是不可能的，因为它需要获取未来的请求，而这些请求是不可知的
    死锁发生时，四个条件必须同时成立，因此破坏其中条件即可预防发生死锁
        破坏互斥条件：如果资源不被一个进程独占，就一定不会发生死锁。实际情况中，如果允许两个进程同时使用打印机就会造成混乱，解决这个问题的方法是假脱机打印机技术（spooling printer）
        破坏占有并等待条件：禁止已持有资源的进程再等待其他资源即可。一种实现方法是，规定所有进程在开始执行前请求所需的全部资源。这种方法的问题是，很多进程在运行时才知道需要多少资源，实际上如果进程知道需要多少资源就可以使用银行家算法。另一种方法是，当进程请求资源时，先暂时释放其占有的资源，再尝试一次获取所需的全部资源
        破坏不可抢占条件：这种方法是可能的
        破坏环路等待条件：对资源编号，请求必须按编号升序提出，但问题在于，几乎找不出一种使每个人都满意的编号次序

通信死锁（communication deadlock）

    除了最常见的资源死锁，还有通信死锁。通信死锁发生在通信系统（如网络）中，比如进程A向进程B发送请求信息并阻塞至B回复，如果A发送的信息丢失，就会导致A和B均阻塞，从而导致死锁
    通信死锁可以通过超时来解决，发送者在发送信息时启动计时器，如果计时器在回复到达前停止，则发送者可以认为信息已丢失，并重新发送

活锁（livelock）

    活锁不会导致进程阻塞，甚至可以说进程正在活动，因此不是死锁，但实际上进程不会继续往下执行，因此可以称为活锁

void process_A() {
  acquire_lock(&resource_1);
  while (!try_lock(
      &resource_2)) {  // 如果进程A尝试获取资源2失败，则先释放资源1，一段时间后再尝试获取资源2
    release_lock(&resource_1);
    wait_fixed_time();  // 若B此时恰好也在等待，则两者都让出了资源但对方都未获取
    acquire_lock(
        &resource_1);  // 接着两者又拿回自己的资源，则下次获取对方资源仍会失败，若此过程一直重复就是活锁
  }
  use_both_resources();
  release_lock(&resource_2);
  release_lock(&resource_1);
}

void process_B() {
  acquire_lock(&resource_2);
  while (!try_lock(&resource_1)) {
    release_lock(&resource_2);
    wait_fixed_time();
    acquire_lock(&resource_2);
  }
  use_both_resources();
  release_lock(&resource_1);
  release_lock(&resource_2);
}


